{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Neural Networks with Regularization - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Recall from the last lab that you had a training accuracy close to 90% and a test set accuracy close to 76%.\n",
    "\n",
    "As with your previous machine learning work, you should be asking a couple of questions:\n",
    "- Is there a high bias? yes/no\n",
    "- Is there a high variance? yes/no \n",
    "\n",
    "In this lab, you'll use the a train-validate-test partition to get better insights of how to tune neural networks using regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. Then, just before you go on to train the model, you'll see how to include a validation set. From there, you'll define and compile the model like before. However, this time, when you are presented with the `history` dictionary of the model, you will have additional data entries for not only the train and test set but also the validation set.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Construct and run a basic model in Keras\n",
    "* Construct a validation set and explain potential benefits\n",
    "* Apply L1 and L2 regularization\n",
    "* Apply dropout regularization\n",
    "* Observe and comment on the effect of using more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries\n",
    "\n",
    "As usual, start by importing some of the packages and modules that you intend to use. The first thing you'll be doing is importing the data and taking a random sample, so that should clue you in to what tools to import. If you need more tools down the line, you can always import additional packages later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Yyour code here; import some packages/modules you plan to use\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "As with the previous lab, the data is stored in a file **Bank_complaints.csv**. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>In XX/XX/XXXX I filled out the Fedlaon applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am being contacted by a debt collector for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I cosigned XXXX student loans at SallieMae for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>Navient has sytematically and illegally failed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>My wife became eligible for XXXX Loan Forgiven...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Product                       Consumer complaint narrative\n",
       "0  Student loan  In XX/XX/XXXX I filled out the Fedlaon applica...\n",
       "1  Student loan  I am being contacted by a debt collector for p...\n",
       "2  Student loan  I cosigned XXXX student loans at SallieMae for...\n",
       "3  Student loan  Navient has sytematically and illegally failed...\n",
       "4  Student loan  My wife became eligible for XXXX Loan Forgiven..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Yyour code here; load and preview the dataset\n",
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Overview\n",
    "\n",
    "Before you begin to practice some of your new tools regarding regularization and optimization, let's practice munging some data as you did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* One-hot encoding your complaint text\n",
    "* Transforming your category labels\n",
    "* Train - test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since you have quite a bit of data and training networks takes a substantial amount of time and resources, downsample in order to test your initial pipeline. Going forward, these can be interesting areas of investigation: how does your models performance change as you increase (or decrease) the size of your dataset?  \n",
    "\n",
    "Generate the random sample using seed 123 for consistency of results. Make your new sample have 10,000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3431</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>After multiple attempts to resolve this issue ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17542</th>\n",
       "      <td>Credit card</td>\n",
       "      <td>I would like CFPB to continue pursuing this ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5713</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>My loans were recently transferred from XXXX t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50394</th>\n",
       "      <td>Mortgage</td>\n",
       "      <td>A fter Ocwen Loan Servicing ap proved our shor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26688</th>\n",
       "      <td>Bank account or service</td>\n",
       "      <td>XXXX XXXX Dear Consumer Financial Protection, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Product  \\\n",
       "3431              Student loan   \n",
       "17542              Credit card   \n",
       "5713              Student loan   \n",
       "50394                 Mortgage   \n",
       "26688  Bank account or service   \n",
       "\n",
       "                            Consumer complaint narrative  \n",
       "3431   After multiple attempts to resolve this issue ...  \n",
       "17542  I would like CFPB to continue pursuing this ca...  \n",
       "5713   My loans were recently transferred from XXXX t...  \n",
       "50394  A fter Ocwen Loan Servicing ap proved our shor...  \n",
       "26688  XXXX XXXX Dear Consumer Financial Protection, ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Yyour code here\n",
    "import random\n",
    "\n",
    "random.seed(123)\n",
    "\n",
    "train_index = random.sample(set(df.index.values), 10000)\n",
    "train_data = df.iloc[train_index]\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding of the Complaints\n",
    "\n",
    "As before, you need to do some preprocessing and data manipulationg before building the neural network. \n",
    "\n",
    "Keep the 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Yyour code here; use one-hot encoding to reformat the complaints into a matrix of vectors.\n",
    "#Only keep the 2000 most common words.\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "X_raw = df['Consumer complaint narrative']\n",
    "y_raw = df.Product\n",
    "\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(X_raw)\n",
    "\n",
    "X = tokenizer.texts_to_matrix(X_raw, mode='binary')\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder = preprocessing.LabelEncoder()\n",
    "label_encoder.fit(y_raw)\n",
    "\n",
    "y_encoded = label_encoder.transform(y_raw)\n",
    "y = keras.preprocessing.utils.to_categorical(y_encoded)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 2000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "> **Note**: This is similar to your previous work with dummy variables. Each of the various product categories will be its own column, and each observation will be a row. In turn, each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; transform the product labels to numerical values\n",
    "#Then transform these integer values into a matrix of binary flags\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "label_encoder.fit(y_raw)\n",
    "\n",
    "y_encoded = label_encoder.transform(y_raw)\n",
    "y = keras.preprocessing.utils.to_categorical(y_encoded)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test Split\n",
    "\n",
    "Now onto the ever familiar train-test split! \n",
    "Below, perform an appropriate train test split.\n",
    "> Be sure to split both the complaint data (now transformed into word vectors) as well as their associated labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/learn-env/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: using a non-integer array as obj in delete will result in an error in the future\n",
      "  \"\"\"\n",
      "/opt/conda/envs/learn-env/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: using a non-integer array as obj in delete will result in an error in the future\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(59998, 7)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Yyour code here\n",
    "X_train = X[train_index]\n",
    "X_test = np.delete(X, X_train, 0)\n",
    "y_train = y[train_index]\n",
    "y_test = np.delete(y, y_train, 0)\n",
    "\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59998, 2000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, you saw that in deep learning, you generally set aside a validation set, which is then used during hyperparameter tuning. Afterwards, when you have decided upon a final model, the test can then be used to define the final model perforance. \n",
    "\n",
    "In this example, take the first 1000 cases out of the training set to create a validation set. You should do this for both `train` and `label_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just run this block of code \n",
    "random.seed(123)\n",
    "val = X_train[:1000]\n",
    "train_final = X_train[1000:]\n",
    "label_val = y_train[:1000]\n",
    "label_train_final = y_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that you used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because you are dealing with a multiclass problem (classifying the complaints into 7 classes), use a softmax classifyer in order to output 7 class probabilities per case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; build a neural network using Keras as described above.\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,)))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "In the compiler, you'll be passing the optimizer, loss function, and metrics. Train the model for 120 epochs in mini-batches of 256 samples. This time, include the argument `validation_data` and assign it `(val, label_val)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Yyour code here\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Ok, now for the resource intensive part: time to train your model! Note that this is where you also introduce the validation data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "9000/9000 [==============================] - 0s 50us/step - loss: 1.9431 - acc: 0.1621 - val_loss: 1.9259 - val_acc: 0.1880\n",
      "Epoch 2/120\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 1.9045 - acc: 0.2160 - val_loss: 1.8982 - val_acc: 0.2280\n",
      "Epoch 3/120\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 1.8745 - acc: 0.2530 - val_loss: 1.8726 - val_acc: 0.2660\n",
      "Epoch 4/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.8431 - acc: 0.2806 - val_loss: 1.8416 - val_acc: 0.2870\n",
      "Epoch 5/120\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 1.8062 - acc: 0.3068 - val_loss: 1.8028 - val_acc: 0.3180\n",
      "Epoch 6/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.7618 - acc: 0.3322 - val_loss: 1.7575 - val_acc: 0.3310\n",
      "Epoch 7/120\n",
      "9000/9000 [==============================] - 0s 38us/step - loss: 1.7108 - acc: 0.3663 - val_loss: 1.7008 - val_acc: 0.3680\n",
      "Epoch 8/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 1.6530 - acc: 0.3997 - val_loss: 1.6428 - val_acc: 0.4120\n",
      "Epoch 9/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 1.5921 - acc: 0.4354 - val_loss: 1.5857 - val_acc: 0.4300\n",
      "Epoch 10/120\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 1.5298 - acc: 0.4657 - val_loss: 1.5206 - val_acc: 0.4730\n",
      "Epoch 11/120\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 1.4677 - acc: 0.5012 - val_loss: 1.4622 - val_acc: 0.5010\n",
      "Epoch 12/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.4064 - acc: 0.5306 - val_loss: 1.4083 - val_acc: 0.5250\n",
      "Epoch 13/120\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 1.3471 - acc: 0.5658 - val_loss: 1.3478 - val_acc: 0.5500\n",
      "Epoch 14/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.2894 - acc: 0.5868 - val_loss: 1.2919 - val_acc: 0.6010\n",
      "Epoch 15/120\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 1.2338 - acc: 0.6118 - val_loss: 1.2457 - val_acc: 0.6030\n",
      "Epoch 16/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.1820 - acc: 0.6284 - val_loss: 1.1966 - val_acc: 0.6260\n",
      "Epoch 17/120\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 1.1328 - acc: 0.6470 - val_loss: 1.1562 - val_acc: 0.6270\n",
      "Epoch 18/120\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 1.0876 - acc: 0.6574 - val_loss: 1.1081 - val_acc: 0.6600\n",
      "Epoch 19/120\n",
      "9000/9000 [==============================] - 0s 26us/step - loss: 1.0452 - acc: 0.6716 - val_loss: 1.0785 - val_acc: 0.6510\n",
      "Epoch 20/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 1.0064 - acc: 0.6804 - val_loss: 1.0379 - val_acc: 0.6650\n",
      "Epoch 21/120\n",
      "9000/9000 [==============================] - 0s 26us/step - loss: 0.9705 - acc: 0.6901 - val_loss: 1.0065 - val_acc: 0.6730\n",
      "Epoch 22/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.9370 - acc: 0.6987 - val_loss: 0.9807 - val_acc: 0.6720\n",
      "Epoch 23/120\n",
      "9000/9000 [==============================] - 0s 26us/step - loss: 0.9070 - acc: 0.7078 - val_loss: 0.9465 - val_acc: 0.6870\n",
      "Epoch 24/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8791 - acc: 0.7137 - val_loss: 0.9257 - val_acc: 0.6860\n",
      "Epoch 25/120\n",
      "9000/9000 [==============================] - 0s 26us/step - loss: 0.8541 - acc: 0.7196 - val_loss: 0.9095 - val_acc: 0.6830\n",
      "Epoch 26/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8306 - acc: 0.7253 - val_loss: 0.8804 - val_acc: 0.6980\n",
      "Epoch 27/120\n",
      "9000/9000 [==============================] - 0s 26us/step - loss: 0.8089 - acc: 0.7312 - val_loss: 0.8653 - val_acc: 0.7070\n",
      "Epoch 28/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.7889 - acc: 0.7329 - val_loss: 0.8462 - val_acc: 0.7050\n",
      "Epoch 29/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.7706 - acc: 0.7374 - val_loss: 0.8333 - val_acc: 0.7080\n",
      "Epoch 30/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.7538 - acc: 0.7408 - val_loss: 0.8161 - val_acc: 0.7200\n",
      "Epoch 31/120\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.7386 - acc: 0.7457 - val_loss: 0.8103 - val_acc: 0.7220\n",
      "Epoch 32/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.7237 - acc: 0.7471 - val_loss: 0.7909 - val_acc: 0.7290\n",
      "Epoch 33/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.7102 - acc: 0.7542 - val_loss: 0.7854 - val_acc: 0.7280\n",
      "Epoch 34/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.6976 - acc: 0.7563 - val_loss: 0.7763 - val_acc: 0.7230\n",
      "Epoch 35/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.6856 - acc: 0.7620 - val_loss: 0.7664 - val_acc: 0.7310\n",
      "Epoch 36/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.6743 - acc: 0.7648 - val_loss: 0.7554 - val_acc: 0.7320\n",
      "Epoch 37/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.6638 - acc: 0.7658 - val_loss: 0.7446 - val_acc: 0.7290\n",
      "Epoch 38/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.6536 - acc: 0.7693 - val_loss: 0.7369 - val_acc: 0.7360\n",
      "Epoch 39/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.6441 - acc: 0.7741 - val_loss: 0.7302 - val_acc: 0.7320\n",
      "Epoch 40/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.6353 - acc: 0.7769 - val_loss: 0.7310 - val_acc: 0.7290\n",
      "Epoch 41/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.6266 - acc: 0.7800 - val_loss: 0.7183 - val_acc: 0.7300\n",
      "Epoch 42/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.6182 - acc: 0.7822 - val_loss: 0.7107 - val_acc: 0.7460\n",
      "Epoch 43/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.6106 - acc: 0.7849 - val_loss: 0.7076 - val_acc: 0.7370\n",
      "Epoch 44/120\n",
      "9000/9000 [==============================] - 0s 26us/step - loss: 0.6031 - acc: 0.7871 - val_loss: 0.7025 - val_acc: 0.7380\n",
      "Epoch 45/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.5955 - acc: 0.7908 - val_loss: 0.6956 - val_acc: 0.7510\n",
      "Epoch 46/120\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.5889 - acc: 0.7928 - val_loss: 0.7034 - val_acc: 0.7380\n",
      "Epoch 47/120\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.5821 - acc: 0.7961 - val_loss: 0.6955 - val_acc: 0.7390\n",
      "Epoch 48/120\n",
      "9000/9000 [==============================] - 0s 26us/step - loss: 0.5757 - acc: 0.7959 - val_loss: 0.6926 - val_acc: 0.7440\n",
      "Epoch 49/120\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.5696 - acc: 0.7992 - val_loss: 0.6809 - val_acc: 0.7490\n",
      "Epoch 50/120\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.5629 - acc: 0.7998 - val_loss: 0.6773 - val_acc: 0.7390\n",
      "Epoch 51/120\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.5573 - acc: 0.8029 - val_loss: 0.6751 - val_acc: 0.7530\n",
      "Epoch 52/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.5519 - acc: 0.8042 - val_loss: 0.6700 - val_acc: 0.7550\n",
      "Epoch 53/120\n",
      "9000/9000 [==============================] - 0s 26us/step - loss: 0.5456 - acc: 0.8061 - val_loss: 0.6754 - val_acc: 0.7440\n",
      "Epoch 54/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.5409 - acc: 0.8103 - val_loss: 0.6705 - val_acc: 0.7500\n",
      "Epoch 55/120\n",
      "9000/9000 [==============================] - 0s 26us/step - loss: 0.5351 - acc: 0.8114 - val_loss: 0.6623 - val_acc: 0.7520\n",
      "Epoch 56/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.5305 - acc: 0.8141 - val_loss: 0.6589 - val_acc: 0.7550\n",
      "Epoch 57/120\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.5252 - acc: 0.8154 - val_loss: 0.6616 - val_acc: 0.7520\n",
      "Epoch 58/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.5205 - acc: 0.8167 - val_loss: 0.6691 - val_acc: 0.7540\n",
      "Epoch 59/120\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.5161 - acc: 0.8163 - val_loss: 0.6640 - val_acc: 0.7480\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.5118 - acc: 0.8196 - val_loss: 0.6547 - val_acc: 0.7500\n",
      "Epoch 61/120\n",
      "9000/9000 [==============================] - 0s 47us/step - loss: 0.5066 - acc: 0.8194 - val_loss: 0.6485 - val_acc: 0.7590\n",
      "Epoch 62/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.5020 - acc: 0.8236 - val_loss: 0.6482 - val_acc: 0.7510\n",
      "Epoch 63/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.4976 - acc: 0.8250 - val_loss: 0.6463 - val_acc: 0.7600\n",
      "Epoch 64/120\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.4936 - acc: 0.8249 - val_loss: 0.6494 - val_acc: 0.7580\n",
      "Epoch 65/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.4894 - acc: 0.8259 - val_loss: 0.6529 - val_acc: 0.7570\n",
      "Epoch 66/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.4857 - acc: 0.8277 - val_loss: 0.6400 - val_acc: 0.7610\n",
      "Epoch 67/120\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.4809 - acc: 0.8293 - val_loss: 0.6402 - val_acc: 0.7550\n",
      "Epoch 68/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.4772 - acc: 0.8319 - val_loss: 0.6359 - val_acc: 0.7580\n",
      "Epoch 69/120\n",
      "9000/9000 [==============================] - 0s 26us/step - loss: 0.4730 - acc: 0.8312 - val_loss: 0.6346 - val_acc: 0.7670\n",
      "Epoch 70/120\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.4690 - acc: 0.8317 - val_loss: 0.6372 - val_acc: 0.7610\n",
      "Epoch 71/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.4658 - acc: 0.8356 - val_loss: 0.6354 - val_acc: 0.7610\n",
      "Epoch 72/120\n",
      "9000/9000 [==============================] - 0s 26us/step - loss: 0.4622 - acc: 0.8368 - val_loss: 0.6323 - val_acc: 0.7610\n",
      "Epoch 73/120\n",
      "9000/9000 [==============================] - 0s 38us/step - loss: 0.4585 - acc: 0.8393 - val_loss: 0.6307 - val_acc: 0.7620\n",
      "Epoch 74/120\n",
      "9000/9000 [==============================] - 0s 35us/step - loss: 0.4547 - acc: 0.8380 - val_loss: 0.6269 - val_acc: 0.7700\n",
      "Epoch 75/120\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.4509 - acc: 0.8397 - val_loss: 0.6277 - val_acc: 0.7640\n",
      "Epoch 76/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.4478 - acc: 0.8413 - val_loss: 0.6396 - val_acc: 0.7620\n",
      "Epoch 77/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.4445 - acc: 0.8450 - val_loss: 0.6269 - val_acc: 0.7680\n",
      "Epoch 78/120\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 0.4406 - acc: 0.8456 - val_loss: 0.6269 - val_acc: 0.7660\n",
      "Epoch 79/120\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 0.4376 - acc: 0.8494 - val_loss: 0.6342 - val_acc: 0.7600\n",
      "Epoch 80/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.4342 - acc: 0.8492 - val_loss: 0.6212 - val_acc: 0.7690\n",
      "Epoch 81/120\n",
      "9000/9000 [==============================] - 0s 26us/step - loss: 0.4310 - acc: 0.8499 - val_loss: 0.6207 - val_acc: 0.7640\n",
      "Epoch 82/120\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.4279 - acc: 0.8501 - val_loss: 0.6242 - val_acc: 0.7630\n",
      "Epoch 83/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.4245 - acc: 0.8528 - val_loss: 0.6203 - val_acc: 0.7730\n",
      "Epoch 84/120\n",
      "9000/9000 [==============================] - 0s 42us/step - loss: 0.4212 - acc: 0.8528 - val_loss: 0.6203 - val_acc: 0.7680\n",
      "Epoch 85/120\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.4182 - acc: 0.8548 - val_loss: 0.6196 - val_acc: 0.7690\n",
      "Epoch 86/120\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.4146 - acc: 0.8563 - val_loss: 0.6240 - val_acc: 0.7620\n",
      "Epoch 87/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.4124 - acc: 0.8582 - val_loss: 0.6180 - val_acc: 0.7710\n",
      "Epoch 88/120\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.4090 - acc: 0.8586 - val_loss: 0.6187 - val_acc: 0.7700\n",
      "Epoch 89/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.4060 - acc: 0.8598 - val_loss: 0.6272 - val_acc: 0.7680\n",
      "Epoch 90/120\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.4032 - acc: 0.8618 - val_loss: 0.6166 - val_acc: 0.7700\n",
      "Epoch 91/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.4001 - acc: 0.8627 - val_loss: 0.6154 - val_acc: 0.7770\n",
      "Epoch 92/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.3972 - acc: 0.8627 - val_loss: 0.6226 - val_acc: 0.7720\n",
      "Epoch 93/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.3946 - acc: 0.8652 - val_loss: 0.6170 - val_acc: 0.7750\n",
      "Epoch 94/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.3915 - acc: 0.8667 - val_loss: 0.6144 - val_acc: 0.7840\n",
      "Epoch 95/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.3892 - acc: 0.8668 - val_loss: 0.6148 - val_acc: 0.7770\n",
      "Epoch 96/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.3864 - acc: 0.8678 - val_loss: 0.6226 - val_acc: 0.7770\n",
      "Epoch 97/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.3831 - acc: 0.8704 - val_loss: 0.6136 - val_acc: 0.7750\n",
      "Epoch 98/120\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.3807 - acc: 0.8692 - val_loss: 0.6149 - val_acc: 0.7780\n",
      "Epoch 99/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.3780 - acc: 0.8701 - val_loss: 0.6139 - val_acc: 0.7770\n",
      "Epoch 100/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.3750 - acc: 0.8708 - val_loss: 0.6142 - val_acc: 0.7820\n",
      "Epoch 101/120\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.3729 - acc: 0.8723 - val_loss: 0.6142 - val_acc: 0.7850\n",
      "Epoch 102/120\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.3704 - acc: 0.8720 - val_loss: 0.6162 - val_acc: 0.7750\n",
      "Epoch 103/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.3673 - acc: 0.8756 - val_loss: 0.6220 - val_acc: 0.7750\n",
      "Epoch 104/120\n",
      "9000/9000 [==============================] - 0s 26us/step - loss: 0.3652 - acc: 0.8771 - val_loss: 0.6168 - val_acc: 0.7760\n",
      "Epoch 105/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.3624 - acc: 0.8762 - val_loss: 0.6183 - val_acc: 0.7800\n",
      "Epoch 106/120\n",
      "9000/9000 [==============================] - 0s 26us/step - loss: 0.3601 - acc: 0.8762 - val_loss: 0.6180 - val_acc: 0.7760\n",
      "Epoch 107/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.3574 - acc: 0.8790 - val_loss: 0.6142 - val_acc: 0.7780\n",
      "Epoch 108/120\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.3550 - acc: 0.8783 - val_loss: 0.6157 - val_acc: 0.7760\n",
      "Epoch 109/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.3524 - acc: 0.8797 - val_loss: 0.6145 - val_acc: 0.7840\n",
      "Epoch 110/120\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.3504 - acc: 0.8802 - val_loss: 0.6198 - val_acc: 0.7790\n",
      "Epoch 111/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.3480 - acc: 0.8820 - val_loss: 0.6182 - val_acc: 0.7780\n",
      "Epoch 112/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.3456 - acc: 0.8836 - val_loss: 0.6224 - val_acc: 0.7740\n",
      "Epoch 113/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.3433 - acc: 0.8838 - val_loss: 0.6148 - val_acc: 0.7840\n",
      "Epoch 114/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.3405 - acc: 0.8859 - val_loss: 0.6273 - val_acc: 0.7770\n",
      "Epoch 115/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.3386 - acc: 0.8870 - val_loss: 0.6200 - val_acc: 0.7780\n",
      "Epoch 116/120\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.3365 - acc: 0.8873 - val_loss: 0.6261 - val_acc: 0.7750\n",
      "Epoch 117/120\n",
      "9000/9000 [==============================] - 0s 26us/step - loss: 0.3340 - acc: 0.8891 - val_loss: 0.6170 - val_acc: 0.7850\n",
      "Epoch 118/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.3317 - acc: 0.8888 - val_loss: 0.6186 - val_acc: 0.7790\n",
      "Epoch 119/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.3294 - acc: 0.8901 - val_loss: 0.6198 - val_acc: 0.7790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.3274 - acc: 0.8914 - val_loss: 0.6185 - val_acc: 0.7840\n"
     ]
    }
   ],
   "source": [
    "#Code provided; note the extra validation parameter passed.\n",
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Performance Results: the `history` dictionary\n",
    "\n",
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 0s 46us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59998/59998 [==============================] - 3s 43us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.32442573176489936, 0.8938888888888888]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5831170548355227, 0.7906263542177677]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result isn't exactly the same as before. Note that this because the training set is slightly different! you remove 1000 instances for validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss function versus the number of epochs. Be sure to include the training and the validation loss in the same plot. Then, create a second plot comparing training and validation accuracy to the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'blue', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'blue', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a limit around the 60th epoch. This means that you're probably **overfitting** the model to the training data when you train for many epochs past this dropoff point of around 40 epochs. Luckily, you learned how to tackle overfitting in the previous lecture! Since it seems clear that you are training too long, include early stopping at the 60th epoch first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping\n",
    "\n",
    "Below, observe how to update the model to include an earlier cutoff point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/60\n",
      "9000/9000 [==============================] - 0s 43us/step - loss: 1.9469 - acc: 0.1591 - val_loss: 1.9371 - val_acc: 0.1400\n",
      "Epoch 2/60\n",
      "9000/9000 [==============================] - 0s 26us/step - loss: 1.9207 - acc: 0.1892 - val_loss: 1.9171 - val_acc: 0.1910\n",
      "Epoch 3/60\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.8981 - acc: 0.2207 - val_loss: 1.8966 - val_acc: 0.2230\n",
      "Epoch 4/60\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 1.8740 - acc: 0.2426 - val_loss: 1.8735 - val_acc: 0.2470\n",
      "Epoch 5/60\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 1.8469 - acc: 0.2687 - val_loss: 1.8461 - val_acc: 0.2540\n",
      "Epoch 6/60\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 1.8156 - acc: 0.2932 - val_loss: 1.8145 - val_acc: 0.2800\n",
      "Epoch 7/60\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 1.7795 - acc: 0.3124 - val_loss: 1.7769 - val_acc: 0.3160\n",
      "Epoch 8/60\n",
      "9000/9000 [==============================] - 0s 26us/step - loss: 1.7359 - acc: 0.3458 - val_loss: 1.7321 - val_acc: 0.3460\n",
      "Epoch 9/60\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.6833 - acc: 0.3850 - val_loss: 1.6777 - val_acc: 0.4040\n",
      "Epoch 10/60\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 1.6218 - acc: 0.4224 - val_loss: 1.6167 - val_acc: 0.4380\n",
      "Epoch 11/60\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 1.5542 - acc: 0.4598 - val_loss: 1.5524 - val_acc: 0.4620\n",
      "Epoch 12/60\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 1.4842 - acc: 0.4896 - val_loss: 1.4872 - val_acc: 0.4900\n",
      "Epoch 13/60\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.4132 - acc: 0.5250 - val_loss: 1.4180 - val_acc: 0.5140\n",
      "Epoch 14/60\n",
      "9000/9000 [==============================] - 0s 26us/step - loss: 1.3452 - acc: 0.5504 - val_loss: 1.3551 - val_acc: 0.5340\n",
      "Epoch 15/60\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.2810 - acc: 0.5808 - val_loss: 1.2947 - val_acc: 0.5660\n",
      "Epoch 16/60\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 1.2207 - acc: 0.6127 - val_loss: 1.2433 - val_acc: 0.5810\n",
      "Epoch 17/60\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.1653 - acc: 0.6329 - val_loss: 1.1882 - val_acc: 0.6220\n",
      "Epoch 18/60\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.1136 - acc: 0.6589 - val_loss: 1.1457 - val_acc: 0.6300\n",
      "Epoch 19/60\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 1.0668 - acc: 0.6749 - val_loss: 1.0987 - val_acc: 0.6560\n",
      "Epoch 20/60\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.0229 - acc: 0.6859 - val_loss: 1.0606 - val_acc: 0.6540\n",
      "Epoch 21/60\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.9838 - acc: 0.6998 - val_loss: 1.0245 - val_acc: 0.6740\n",
      "Epoch 22/60\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.9477 - acc: 0.7086 - val_loss: 0.9987 - val_acc: 0.6780\n",
      "Epoch 23/60\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.9150 - acc: 0.7173 - val_loss: 0.9712 - val_acc: 0.6830\n",
      "Epoch 24/60\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8853 - acc: 0.7250 - val_loss: 0.9434 - val_acc: 0.6910\n",
      "Epoch 25/60\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.8577 - acc: 0.7286 - val_loss: 0.9233 - val_acc: 0.6840\n",
      "Epoch 26/60\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8333 - acc: 0.7358 - val_loss: 0.9005 - val_acc: 0.6960\n",
      "Epoch 27/60\n",
      "9000/9000 [==============================] - 0s 25us/step - loss: 0.8102 - acc: 0.7427 - val_loss: 0.8837 - val_acc: 0.6970\n",
      "Epoch 28/60\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.7891 - acc: 0.7470 - val_loss: 0.8669 - val_acc: 0.6940\n",
      "Epoch 29/60\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.7700 - acc: 0.7501 - val_loss: 0.8462 - val_acc: 0.7010\n",
      "Epoch 30/60\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.7518 - acc: 0.7529 - val_loss: 0.8286 - val_acc: 0.7080\n",
      "Epoch 31/60\n",
      "9000/9000 [==============================] - 0s 26us/step - loss: 0.7353 - acc: 0.7583 - val_loss: 0.8167 - val_acc: 0.7100\n",
      "Epoch 32/60\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.7200 - acc: 0.7614 - val_loss: 0.8026 - val_acc: 0.7200\n",
      "Epoch 33/60\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.7059 - acc: 0.7661 - val_loss: 0.7958 - val_acc: 0.7120\n",
      "Epoch 34/60\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.6926 - acc: 0.7723 - val_loss: 0.7834 - val_acc: 0.7190\n",
      "Epoch 35/60\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.6799 - acc: 0.7736 - val_loss: 0.7767 - val_acc: 0.7180\n",
      "Epoch 36/60\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.6685 - acc: 0.7768 - val_loss: 0.7683 - val_acc: 0.7240\n",
      "Epoch 37/60\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.6581 - acc: 0.7801 - val_loss: 0.7564 - val_acc: 0.7240\n",
      "Epoch 38/60\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.6479 - acc: 0.7818 - val_loss: 0.7545 - val_acc: 0.7330\n",
      "Epoch 39/60\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.6384 - acc: 0.7848 - val_loss: 0.7449 - val_acc: 0.7330\n",
      "Epoch 40/60\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.6286 - acc: 0.7883 - val_loss: 0.7412 - val_acc: 0.7250\n",
      "Epoch 41/60\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.6197 - acc: 0.7893 - val_loss: 0.7346 - val_acc: 0.7230\n",
      "Epoch 42/60\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.6115 - acc: 0.7924 - val_loss: 0.7255 - val_acc: 0.7450\n",
      "Epoch 43/60\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.6038 - acc: 0.7953 - val_loss: 0.7205 - val_acc: 0.7410\n",
      "Epoch 44/60\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.5956 - acc: 0.7988 - val_loss: 0.7129 - val_acc: 0.7450\n",
      "Epoch 45/60\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.5885 - acc: 0.7994 - val_loss: 0.7108 - val_acc: 0.7410\n",
      "Epoch 46/60\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.5814 - acc: 0.8022 - val_loss: 0.7086 - val_acc: 0.7440\n",
      "Epoch 47/60\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.5745 - acc: 0.8056 - val_loss: 0.7032 - val_acc: 0.7430\n",
      "Epoch 48/60\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.5679 - acc: 0.8077 - val_loss: 0.6982 - val_acc: 0.7450\n",
      "Epoch 49/60\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.5617 - acc: 0.8079 - val_loss: 0.6926 - val_acc: 0.7520\n",
      "Epoch 50/60\n",
      "9000/9000 [==============================] - 0s 26us/step - loss: 0.5556 - acc: 0.8111 - val_loss: 0.6952 - val_acc: 0.7490\n",
      "Epoch 51/60\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.5496 - acc: 0.8122 - val_loss: 0.6924 - val_acc: 0.7460\n",
      "Epoch 52/60\n",
      "9000/9000 [==============================] - 0s 26us/step - loss: 0.5432 - acc: 0.8163 - val_loss: 0.6911 - val_acc: 0.7470\n",
      "Epoch 53/60\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.5381 - acc: 0.8188 - val_loss: 0.6804 - val_acc: 0.7550\n",
      "Epoch 54/60\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.5326 - acc: 0.8191 - val_loss: 0.6805 - val_acc: 0.7580\n",
      "Epoch 55/60\n",
      "9000/9000 [==============================] - 0s 26us/step - loss: 0.5268 - acc: 0.8232 - val_loss: 0.6775 - val_acc: 0.7590\n",
      "Epoch 56/60\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.5216 - acc: 0.8247 - val_loss: 0.6768 - val_acc: 0.7550\n",
      "Epoch 57/60\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.5165 - acc: 0.8262 - val_loss: 0.6800 - val_acc: 0.7490\n",
      "Epoch 58/60\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.5119 - acc: 0.8262 - val_loss: 0.6729 - val_acc: 0.7500\n",
      "Epoch 59/60\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.5070 - acc: 0.8301 - val_loss: 0.6664 - val_acc: 0.7610\n",
      "Epoch 60/60\n",
      "9000/9000 [==============================] - 0s 26us/step - loss: 0.5021 - acc: 0.8323 - val_loss: 0.6701 - val_acc: 0.7560\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=60,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the test set to make label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 0s 43us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59998/59998 [==============================] - 3s 42us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5689828497727712, 0.8097333333651224]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6309139318313435, 0.7691089703029838]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! your test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs model you originally fit.\n",
    "\n",
    "Now, take a look at how regularization techniques can further improve your model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, take a look at L2 regularization. Keras makes L2 regularization easy. Simply add the `kernel_regularizer=kernel_regulizers.l2(lamda_coeff)` parameter to any model layer. The lambda_coeff parameter determines the strength of the regularization you wish to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "9000/9000 [==============================] - 0s 45us/step - loss: 2.5965 - acc: 0.1831 - val_loss: 2.5788 - val_acc: 0.1950\n",
      "Epoch 2/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 2.5671 - acc: 0.1971 - val_loss: 2.5589 - val_acc: 0.2080\n",
      "Epoch 3/120\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 2.5434 - acc: 0.2089 - val_loss: 2.5390 - val_acc: 0.2190\n",
      "Epoch 4/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 2.5194 - acc: 0.2211 - val_loss: 2.5176 - val_acc: 0.2290\n",
      "Epoch 5/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 2.4931 - acc: 0.2374 - val_loss: 2.4930 - val_acc: 0.2440\n",
      "Epoch 6/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 2.4634 - acc: 0.2596 - val_loss: 2.4651 - val_acc: 0.2690\n",
      "Epoch 7/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 2.4298 - acc: 0.2957 - val_loss: 2.4342 - val_acc: 0.3060\n",
      "Epoch 8/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 2.3926 - acc: 0.3330 - val_loss: 2.3958 - val_acc: 0.3330\n",
      "Epoch 9/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 2.3507 - acc: 0.3687 - val_loss: 2.3561 - val_acc: 0.3760\n",
      "Epoch 10/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 2.3039 - acc: 0.4087 - val_loss: 2.3104 - val_acc: 0.4140\n",
      "Epoch 11/120\n",
      "9000/9000 [==============================] - 0s 44us/step - loss: 2.2528 - acc: 0.4464 - val_loss: 2.2618 - val_acc: 0.4430\n",
      "Epoch 12/120\n",
      "9000/9000 [==============================] - 0s 39us/step - loss: 2.1971 - acc: 0.4857 - val_loss: 2.2085 - val_acc: 0.4640\n",
      "Epoch 13/120\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 2.1376 - acc: 0.5137 - val_loss: 2.1517 - val_acc: 0.4860\n",
      "Epoch 14/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 2.0760 - acc: 0.5449 - val_loss: 2.0923 - val_acc: 0.5160\n",
      "Epoch 15/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 2.0142 - acc: 0.5666 - val_loss: 2.0338 - val_acc: 0.5490\n",
      "Epoch 16/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.9533 - acc: 0.5933 - val_loss: 1.9790 - val_acc: 0.5680\n",
      "Epoch 17/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.8944 - acc: 0.6164 - val_loss: 1.9242 - val_acc: 0.5800\n",
      "Epoch 18/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.8382 - acc: 0.6338 - val_loss: 1.8720 - val_acc: 0.6130\n",
      "Epoch 19/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.7855 - acc: 0.6542 - val_loss: 1.8262 - val_acc: 0.6060\n",
      "Epoch 20/120\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 1.7359 - acc: 0.6661 - val_loss: 1.7772 - val_acc: 0.6330\n",
      "Epoch 21/120\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 1.6903 - acc: 0.6778 - val_loss: 1.7385 - val_acc: 0.6320\n",
      "Epoch 22/120\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 1.6468 - acc: 0.6906 - val_loss: 1.7018 - val_acc: 0.6420\n",
      "Epoch 23/120\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 1.6078 - acc: 0.6977 - val_loss: 1.6624 - val_acc: 0.6690\n",
      "Epoch 24/120\n",
      "9000/9000 [==============================] - 0s 42us/step - loss: 1.5711 - acc: 0.7077 - val_loss: 1.6308 - val_acc: 0.6660\n",
      "Epoch 25/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.5372 - acc: 0.7132 - val_loss: 1.5982 - val_acc: 0.6840\n",
      "Epoch 26/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.5068 - acc: 0.7222 - val_loss: 1.5754 - val_acc: 0.6790\n",
      "Epoch 27/120\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.4780 - acc: 0.7286 - val_loss: 1.5446 - val_acc: 0.6900\n",
      "Epoch 28/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.4519 - acc: 0.7292 - val_loss: 1.5259 - val_acc: 0.6820\n",
      "Epoch 29/120\n",
      "9000/9000 [==============================] - 0s 42us/step - loss: 1.4276 - acc: 0.7340 - val_loss: 1.5055 - val_acc: 0.7000\n",
      "Epoch 30/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.4049 - acc: 0.7387 - val_loss: 1.4832 - val_acc: 0.6950\n",
      "Epoch 31/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.3844 - acc: 0.7412 - val_loss: 1.4649 - val_acc: 0.7010\n",
      "Epoch 32/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.3653 - acc: 0.7458 - val_loss: 1.4435 - val_acc: 0.7180\n",
      "Epoch 33/120\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 1.3468 - acc: 0.7501 - val_loss: 1.4307 - val_acc: 0.7080\n",
      "Epoch 34/120\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 1.3296 - acc: 0.7510 - val_loss: 1.4151 - val_acc: 0.7160\n",
      "Epoch 35/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.3137 - acc: 0.7541 - val_loss: 1.4015 - val_acc: 0.7180\n",
      "Epoch 36/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 1.2987 - acc: 0.7586 - val_loss: 1.3883 - val_acc: 0.7210\n",
      "Epoch 37/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.2845 - acc: 0.7612 - val_loss: 1.3804 - val_acc: 0.7150\n",
      "Epoch 38/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.2711 - acc: 0.7608 - val_loss: 1.3656 - val_acc: 0.7210\n",
      "Epoch 39/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 1.2581 - acc: 0.7667 - val_loss: 1.3512 - val_acc: 0.7340\n",
      "Epoch 40/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.2460 - acc: 0.7704 - val_loss: 1.3424 - val_acc: 0.7300\n",
      "Epoch 41/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.2342 - acc: 0.7718 - val_loss: 1.3344 - val_acc: 0.7280\n",
      "Epoch 42/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.2232 - acc: 0.7730 - val_loss: 1.3208 - val_acc: 0.7390\n",
      "Epoch 43/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.2127 - acc: 0.7753 - val_loss: 1.3161 - val_acc: 0.7370\n",
      "Epoch 44/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.2025 - acc: 0.7774 - val_loss: 1.3056 - val_acc: 0.7400\n",
      "Epoch 45/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.1924 - acc: 0.7804 - val_loss: 1.2982 - val_acc: 0.7360\n",
      "Epoch 46/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.1831 - acc: 0.7836 - val_loss: 1.2956 - val_acc: 0.7270\n",
      "Epoch 47/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 1.1735 - acc: 0.7863 - val_loss: 1.2824 - val_acc: 0.7390\n",
      "Epoch 48/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.1649 - acc: 0.7887 - val_loss: 1.2784 - val_acc: 0.7360\n",
      "Epoch 49/120\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 1.1557 - acc: 0.7896 - val_loss: 1.2710 - val_acc: 0.7400\n",
      "Epoch 50/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.1473 - acc: 0.7924 - val_loss: 1.2682 - val_acc: 0.7380\n",
      "Epoch 51/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 1.1395 - acc: 0.7952 - val_loss: 1.2546 - val_acc: 0.7420\n",
      "Epoch 52/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.1314 - acc: 0.7972 - val_loss: 1.2491 - val_acc: 0.7420\n",
      "Epoch 53/120\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 1.1234 - acc: 0.8000 - val_loss: 1.2437 - val_acc: 0.7460\n",
      "Epoch 54/120\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.1155 - acc: 0.8000 - val_loss: 1.2396 - val_acc: 0.7430\n",
      "Epoch 55/120\n",
      "9000/9000 [==============================] - 0s 43us/step - loss: 1.1086 - acc: 0.8038 - val_loss: 1.2329 - val_acc: 0.7550\n",
      "Epoch 56/120\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 1.1013 - acc: 0.8033 - val_loss: 1.2294 - val_acc: 0.7470\n",
      "Epoch 57/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.0936 - acc: 0.8059 - val_loss: 1.2185 - val_acc: 0.7530\n",
      "Epoch 58/120\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.0869 - acc: 0.8077 - val_loss: 1.2151 - val_acc: 0.7500\n",
      "Epoch 59/120\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 1.0802 - acc: 0.8098 - val_loss: 1.2113 - val_acc: 0.7540\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 0s 33us/step - loss: 1.0736 - acc: 0.8110 - val_loss: 1.2070 - val_acc: 0.7530\n",
      "Epoch 61/120\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.0671 - acc: 0.8116 - val_loss: 1.1994 - val_acc: 0.7540\n",
      "Epoch 62/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.0607 - acc: 0.8157 - val_loss: 1.1992 - val_acc: 0.7510\n",
      "Epoch 63/120\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.0539 - acc: 0.8158 - val_loss: 1.1911 - val_acc: 0.7560\n",
      "Epoch 64/120\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.0481 - acc: 0.8168 - val_loss: 1.1867 - val_acc: 0.7600\n",
      "Epoch 65/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.0418 - acc: 0.8179 - val_loss: 1.1809 - val_acc: 0.7600\n",
      "Epoch 66/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.0358 - acc: 0.8192 - val_loss: 1.1840 - val_acc: 0.7490\n",
      "Epoch 67/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.0302 - acc: 0.8196 - val_loss: 1.1756 - val_acc: 0.7590\n",
      "Epoch 68/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 1.0238 - acc: 0.8219 - val_loss: 1.1689 - val_acc: 0.7600\n",
      "Epoch 69/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.0180 - acc: 0.8223 - val_loss: 1.1672 - val_acc: 0.7550\n",
      "Epoch 70/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.0130 - acc: 0.8249 - val_loss: 1.1618 - val_acc: 0.7660\n",
      "Epoch 71/120\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 1.0074 - acc: 0.8258 - val_loss: 1.1578 - val_acc: 0.7580\n",
      "Epoch 72/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.0015 - acc: 0.8291 - val_loss: 1.1596 - val_acc: 0.7600\n",
      "Epoch 73/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.9963 - acc: 0.8290 - val_loss: 1.1507 - val_acc: 0.7630\n",
      "Epoch 74/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.9908 - acc: 0.8296 - val_loss: 1.1469 - val_acc: 0.7650\n",
      "Epoch 75/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.9856 - acc: 0.8308 - val_loss: 1.1443 - val_acc: 0.7710\n",
      "Epoch 76/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.9805 - acc: 0.8336 - val_loss: 1.1462 - val_acc: 0.7600\n",
      "Epoch 77/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.9748 - acc: 0.8333 - val_loss: 1.1351 - val_acc: 0.7690\n",
      "Epoch 78/120\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.9702 - acc: 0.8347 - val_loss: 1.1358 - val_acc: 0.7680\n",
      "Epoch 79/120\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.9650 - acc: 0.8344 - val_loss: 1.1311 - val_acc: 0.7650\n",
      "Epoch 80/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.9599 - acc: 0.8363 - val_loss: 1.1275 - val_acc: 0.7700\n",
      "Epoch 81/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.9551 - acc: 0.8388 - val_loss: 1.1235 - val_acc: 0.7670\n",
      "Epoch 82/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.9503 - acc: 0.8399 - val_loss: 1.1273 - val_acc: 0.7610\n",
      "Epoch 83/120\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 0.9459 - acc: 0.8406 - val_loss: 1.1154 - val_acc: 0.7750\n",
      "Epoch 84/120\n",
      "9000/9000 [==============================] - 0s 47us/step - loss: 0.9412 - acc: 0.8420 - val_loss: 1.1142 - val_acc: 0.7730\n",
      "Epoch 85/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.9363 - acc: 0.8422 - val_loss: 1.1095 - val_acc: 0.7740\n",
      "Epoch 86/120\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.9316 - acc: 0.8439 - val_loss: 1.1057 - val_acc: 0.7780\n",
      "Epoch 87/120\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.9272 - acc: 0.8453 - val_loss: 1.1046 - val_acc: 0.7810\n",
      "Epoch 88/120\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.9228 - acc: 0.8444 - val_loss: 1.1042 - val_acc: 0.7760\n",
      "Epoch 89/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.9181 - acc: 0.8468 - val_loss: 1.1004 - val_acc: 0.7760\n",
      "Epoch 90/120\n",
      "9000/9000 [==============================] - 0s 26us/step - loss: 0.9137 - acc: 0.8483 - val_loss: 1.0989 - val_acc: 0.7650\n",
      "Epoch 91/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.9092 - acc: 0.8502 - val_loss: 1.0935 - val_acc: 0.7740\n",
      "Epoch 92/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.9051 - acc: 0.8506 - val_loss: 1.0893 - val_acc: 0.7790\n",
      "Epoch 93/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.9006 - acc: 0.8511 - val_loss: 1.0898 - val_acc: 0.7690\n",
      "Epoch 94/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8966 - acc: 0.8509 - val_loss: 1.0907 - val_acc: 0.7670\n",
      "Epoch 95/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8918 - acc: 0.8538 - val_loss: 1.0826 - val_acc: 0.7820\n",
      "Epoch 96/120\n",
      "9000/9000 [==============================] - 0s 41us/step - loss: 0.8883 - acc: 0.8560 - val_loss: 1.0806 - val_acc: 0.7800\n",
      "Epoch 97/120\n",
      "9000/9000 [==============================] - 0s 35us/step - loss: 0.8835 - acc: 0.8550 - val_loss: 1.0790 - val_acc: 0.7720\n",
      "Epoch 98/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8800 - acc: 0.8561 - val_loss: 1.0774 - val_acc: 0.7770\n",
      "Epoch 99/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8754 - acc: 0.8580 - val_loss: 1.0733 - val_acc: 0.7810\n",
      "Epoch 100/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8719 - acc: 0.8581 - val_loss: 1.0707 - val_acc: 0.7770\n",
      "Epoch 101/120\n",
      "9000/9000 [==============================] - 0s 36us/step - loss: 0.8676 - acc: 0.8573 - val_loss: 1.0718 - val_acc: 0.7710\n",
      "Epoch 102/120\n",
      "9000/9000 [==============================] - 0s 40us/step - loss: 0.8641 - acc: 0.8608 - val_loss: 1.0689 - val_acc: 0.7790\n",
      "Epoch 103/120\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.8601 - acc: 0.8610 - val_loss: 1.0643 - val_acc: 0.7840\n",
      "Epoch 104/120\n",
      "9000/9000 [==============================] - 0s 41us/step - loss: 0.8562 - acc: 0.8630 - val_loss: 1.0606 - val_acc: 0.7810\n",
      "Epoch 105/120\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.8526 - acc: 0.8627 - val_loss: 1.0585 - val_acc: 0.7800\n",
      "Epoch 106/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8488 - acc: 0.8639 - val_loss: 1.0556 - val_acc: 0.7800\n",
      "Epoch 107/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8446 - acc: 0.8639 - val_loss: 1.0543 - val_acc: 0.7790\n",
      "Epoch 108/120\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.8413 - acc: 0.8660 - val_loss: 1.0521 - val_acc: 0.7730\n",
      "Epoch 109/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8375 - acc: 0.8668 - val_loss: 1.0516 - val_acc: 0.7850\n",
      "Epoch 110/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8340 - acc: 0.8670 - val_loss: 1.0458 - val_acc: 0.7880\n",
      "Epoch 111/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8301 - acc: 0.8683 - val_loss: 1.0449 - val_acc: 0.7820\n",
      "Epoch 112/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8266 - acc: 0.8694 - val_loss: 1.0451 - val_acc: 0.7780\n",
      "Epoch 113/120\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.8230 - acc: 0.8702 - val_loss: 1.0444 - val_acc: 0.7860\n",
      "Epoch 114/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8198 - acc: 0.8706 - val_loss: 1.0394 - val_acc: 0.7810\n",
      "Epoch 115/120\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8161 - acc: 0.8697 - val_loss: 1.0399 - val_acc: 0.7830\n",
      "Epoch 116/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8123 - acc: 0.8718 - val_loss: 1.0393 - val_acc: 0.7800\n",
      "Epoch 117/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8093 - acc: 0.8734 - val_loss: 1.0356 - val_acc: 0.7810\n",
      "Epoch 118/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8059 - acc: 0.8738 - val_loss: 1.0311 - val_acc: 0.7850\n",
      "Epoch 119/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8027 - acc: 0.8733 - val_loss: 1.0296 - val_acc: 0.7800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.7992 - acc: 0.8753 - val_loss: 1.0295 - val_acc: 0.7800\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'g', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. Notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at L1 regularization. Will this work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "9000/9000 [==============================] - 0s 52us/step - loss: 15.9468 - acc: 0.1253 - val_loss: 15.4633 - val_acc: 0.1380\n",
      "Epoch 2/120\n",
      "9000/9000 [==============================] - 0s 26us/step - loss: 15.0435 - acc: 0.1591 - val_loss: 14.5845 - val_acc: 0.1690\n",
      "Epoch 3/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 14.1788 - acc: 0.1799 - val_loss: 13.7386 - val_acc: 0.1960\n",
      "Epoch 4/120\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 13.3455 - acc: 0.2000 - val_loss: 12.9217 - val_acc: 0.2270\n",
      "Epoch 5/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 12.5394 - acc: 0.2140 - val_loss: 12.1290 - val_acc: 0.2330\n",
      "Epoch 6/120\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 11.7589 - acc: 0.2137 - val_loss: 11.3646 - val_acc: 0.2320\n",
      "Epoch 7/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 11.0075 - acc: 0.2190 - val_loss: 10.6292 - val_acc: 0.2320\n",
      "Epoch 8/120\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 10.2851 - acc: 0.2271 - val_loss: 9.9245 - val_acc: 0.2540\n",
      "Epoch 9/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 9.5905 - acc: 0.2606 - val_loss: 9.2477 - val_acc: 0.2830\n",
      "Epoch 10/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 8.9233 - acc: 0.3010 - val_loss: 8.5984 - val_acc: 0.3180\n",
      "Epoch 11/120\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 8.2853 - acc: 0.3366 - val_loss: 7.9803 - val_acc: 0.3860\n",
      "Epoch 12/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 7.6767 - acc: 0.3933 - val_loss: 7.3883 - val_acc: 0.4050\n",
      "Epoch 13/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 7.0986 - acc: 0.4331 - val_loss: 6.8286 - val_acc: 0.4150\n",
      "Epoch 14/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 6.5515 - acc: 0.4623 - val_loss: 6.3050 - val_acc: 0.4700\n",
      "Epoch 15/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 6.0363 - acc: 0.5003 - val_loss: 5.8089 - val_acc: 0.4880\n",
      "Epoch 16/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 5.5537 - acc: 0.5256 - val_loss: 5.3477 - val_acc: 0.5120\n",
      "Epoch 17/120\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 5.1035 - acc: 0.5463 - val_loss: 4.9149 - val_acc: 0.5360\n",
      "Epoch 18/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 4.6857 - acc: 0.5624 - val_loss: 4.5185 - val_acc: 0.5390\n",
      "Epoch 19/120\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 4.3002 - acc: 0.5768 - val_loss: 4.1529 - val_acc: 0.5580\n",
      "Epoch 20/120\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 3.9470 - acc: 0.5923 - val_loss: 3.8145 - val_acc: 0.5520\n",
      "Epoch 21/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 3.6257 - acc: 0.5992 - val_loss: 3.5140 - val_acc: 0.5710\n",
      "Epoch 22/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 3.3360 - acc: 0.6076 - val_loss: 3.2414 - val_acc: 0.5810\n",
      "Epoch 23/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 3.0771 - acc: 0.6184 - val_loss: 3.0000 - val_acc: 0.5800\n",
      "Epoch 24/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 2.8491 - acc: 0.6222 - val_loss: 2.7906 - val_acc: 0.5820\n",
      "Epoch 25/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 2.6518 - acc: 0.6276 - val_loss: 2.6062 - val_acc: 0.5850\n",
      "Epoch 26/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 2.4844 - acc: 0.6294 - val_loss: 2.4552 - val_acc: 0.5930\n",
      "Epoch 27/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 2.3472 - acc: 0.6318 - val_loss: 2.3338 - val_acc: 0.5920\n",
      "Epoch 28/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 2.2383 - acc: 0.6351 - val_loss: 2.2404 - val_acc: 0.5950\n",
      "Epoch 29/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 2.1563 - acc: 0.6370 - val_loss: 2.1678 - val_acc: 0.6010\n",
      "Epoch 30/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 2.0975 - acc: 0.6384 - val_loss: 2.1208 - val_acc: 0.6150\n",
      "Epoch 31/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 2.0560 - acc: 0.6374 - val_loss: 2.0872 - val_acc: 0.6060\n",
      "Epoch 32/120\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 2.0234 - acc: 0.6376 - val_loss: 2.0552 - val_acc: 0.6220\n",
      "Epoch 33/120\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.9950 - acc: 0.6393 - val_loss: 2.0287 - val_acc: 0.6160\n",
      "Epoch 34/120\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.9690 - acc: 0.6370 - val_loss: 2.0011 - val_acc: 0.6170\n",
      "Epoch 35/120\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.9445 - acc: 0.6380 - val_loss: 1.9787 - val_acc: 0.6160\n",
      "Epoch 36/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.9219 - acc: 0.6398 - val_loss: 1.9612 - val_acc: 0.6180\n",
      "Epoch 37/120\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 1.9004 - acc: 0.6402 - val_loss: 1.9398 - val_acc: 0.6150\n",
      "Epoch 38/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.8800 - acc: 0.6407 - val_loss: 1.9174 - val_acc: 0.6180\n",
      "Epoch 39/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.8608 - acc: 0.6408 - val_loss: 1.9036 - val_acc: 0.6170\n",
      "Epoch 40/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 1.8423 - acc: 0.6410 - val_loss: 1.8802 - val_acc: 0.6200\n",
      "Epoch 41/120\n",
      "9000/9000 [==============================] - 0s 38us/step - loss: 1.8246 - acc: 0.6420 - val_loss: 1.8655 - val_acc: 0.6130\n",
      "Epoch 42/120\n",
      "9000/9000 [==============================] - 0s 46us/step - loss: 1.8071 - acc: 0.6414 - val_loss: 1.8432 - val_acc: 0.6290\n",
      "Epoch 43/120\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 1.7911 - acc: 0.6446 - val_loss: 1.8340 - val_acc: 0.6190\n",
      "Epoch 44/120\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 1.7751 - acc: 0.6453 - val_loss: 1.8161 - val_acc: 0.6190\n",
      "Epoch 45/120\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 1.7599 - acc: 0.6486 - val_loss: 1.8027 - val_acc: 0.6320\n",
      "Epoch 46/120\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.7451 - acc: 0.6508 - val_loss: 1.7826 - val_acc: 0.6420\n",
      "Epoch 47/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.7307 - acc: 0.6527 - val_loss: 1.7820 - val_acc: 0.6230\n",
      "Epoch 48/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.7174 - acc: 0.6536 - val_loss: 1.7584 - val_acc: 0.6420\n",
      "Epoch 49/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.7038 - acc: 0.6570 - val_loss: 1.7468 - val_acc: 0.6370\n",
      "Epoch 50/120\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.6905 - acc: 0.6599 - val_loss: 1.7286 - val_acc: 0.6490\n",
      "Epoch 51/120\n",
      "9000/9000 [==============================] - 0s 43us/step - loss: 1.6776 - acc: 0.6639 - val_loss: 1.7225 - val_acc: 0.6470\n",
      "Epoch 52/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.6655 - acc: 0.6701 - val_loss: 1.7069 - val_acc: 0.6510\n",
      "Epoch 53/120\n",
      "9000/9000 [==============================] - 0s 38us/step - loss: 1.6530 - acc: 0.6726 - val_loss: 1.6946 - val_acc: 0.6580\n",
      "Epoch 54/120\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 1.6407 - acc: 0.6807 - val_loss: 1.6836 - val_acc: 0.6550\n",
      "Epoch 55/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.6288 - acc: 0.6807 - val_loss: 1.6727 - val_acc: 0.6600\n",
      "Epoch 56/120\n",
      "9000/9000 [==============================] - 0s 44us/step - loss: 1.6175 - acc: 0.6843 - val_loss: 1.6585 - val_acc: 0.6620\n",
      "Epoch 57/120\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 1.6062 - acc: 0.6868 - val_loss: 1.6493 - val_acc: 0.6580\n",
      "Epoch 58/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.5953 - acc: 0.6854 - val_loss: 1.6422 - val_acc: 0.6670\n",
      "Epoch 59/120\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.5846 - acc: 0.6904 - val_loss: 1.6261 - val_acc: 0.6820\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.5739 - acc: 0.6931 - val_loss: 1.6168 - val_acc: 0.6710\n",
      "Epoch 61/120\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 1.5634 - acc: 0.6956 - val_loss: 1.6076 - val_acc: 0.6770\n",
      "Epoch 62/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.5540 - acc: 0.6981 - val_loss: 1.5999 - val_acc: 0.6790\n",
      "Epoch 63/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.5436 - acc: 0.7020 - val_loss: 1.5838 - val_acc: 0.6770\n",
      "Epoch 64/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.5336 - acc: 0.7009 - val_loss: 1.5815 - val_acc: 0.6790\n",
      "Epoch 65/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.5241 - acc: 0.7011 - val_loss: 1.5677 - val_acc: 0.6830\n",
      "Epoch 66/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 1.5143 - acc: 0.7028 - val_loss: 1.5559 - val_acc: 0.6910\n",
      "Epoch 67/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.5052 - acc: 0.7043 - val_loss: 1.5531 - val_acc: 0.6900\n",
      "Epoch 68/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 1.4969 - acc: 0.7076 - val_loss: 1.5391 - val_acc: 0.6940\n",
      "Epoch 69/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.4869 - acc: 0.7083 - val_loss: 1.5342 - val_acc: 0.7080\n",
      "Epoch 70/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.4788 - acc: 0.7081 - val_loss: 1.5237 - val_acc: 0.6890\n",
      "Epoch 71/120\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 1.4695 - acc: 0.7098 - val_loss: 1.5107 - val_acc: 0.6950\n",
      "Epoch 72/120\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.4608 - acc: 0.7109 - val_loss: 1.5064 - val_acc: 0.6990\n",
      "Epoch 73/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.4527 - acc: 0.7120 - val_loss: 1.4951 - val_acc: 0.6980\n",
      "Epoch 74/120\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.4437 - acc: 0.7117 - val_loss: 1.4881 - val_acc: 0.6990\n",
      "Epoch 75/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.4353 - acc: 0.7130 - val_loss: 1.4842 - val_acc: 0.6970\n",
      "Epoch 76/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.4271 - acc: 0.7137 - val_loss: 1.4832 - val_acc: 0.6910\n",
      "Epoch 77/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.4196 - acc: 0.7143 - val_loss: 1.4621 - val_acc: 0.7010\n",
      "Epoch 78/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.4112 - acc: 0.7149 - val_loss: 1.4602 - val_acc: 0.6990\n",
      "Epoch 79/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 1.4037 - acc: 0.7149 - val_loss: 1.4558 - val_acc: 0.7030\n",
      "Epoch 80/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 1.3961 - acc: 0.7160 - val_loss: 1.4374 - val_acc: 0.7020\n",
      "Epoch 81/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.3876 - acc: 0.7174 - val_loss: 1.4318 - val_acc: 0.7100\n",
      "Epoch 82/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.3804 - acc: 0.7178 - val_loss: 1.4254 - val_acc: 0.7050\n",
      "Epoch 83/120\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.3730 - acc: 0.7196 - val_loss: 1.4177 - val_acc: 0.7080\n",
      "Epoch 84/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.3655 - acc: 0.7194 - val_loss: 1.4105 - val_acc: 0.7070\n",
      "Epoch 85/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.3579 - acc: 0.7186 - val_loss: 1.4110 - val_acc: 0.7000\n",
      "Epoch 86/120\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.3507 - acc: 0.7190 - val_loss: 1.3924 - val_acc: 0.7110\n",
      "Epoch 87/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 1.3434 - acc: 0.7213 - val_loss: 1.3949 - val_acc: 0.7010\n",
      "Epoch 88/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.3367 - acc: 0.7206 - val_loss: 1.3815 - val_acc: 0.7090\n",
      "Epoch 89/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.3296 - acc: 0.7220 - val_loss: 1.3778 - val_acc: 0.7110\n",
      "Epoch 90/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 1.3226 - acc: 0.7203 - val_loss: 1.3637 - val_acc: 0.7120\n",
      "Epoch 91/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.3156 - acc: 0.7222 - val_loss: 1.3721 - val_acc: 0.7060\n",
      "Epoch 92/120\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 1.3090 - acc: 0.7212 - val_loss: 1.3559 - val_acc: 0.7090\n",
      "Epoch 93/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.3022 - acc: 0.7226 - val_loss: 1.3479 - val_acc: 0.7130\n",
      "Epoch 94/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.2958 - acc: 0.7228 - val_loss: 1.3453 - val_acc: 0.7090\n",
      "Epoch 95/120\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 1.2893 - acc: 0.7231 - val_loss: 1.3432 - val_acc: 0.7140\n",
      "Epoch 96/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.2834 - acc: 0.7247 - val_loss: 1.3310 - val_acc: 0.7070\n",
      "Epoch 97/120\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.2769 - acc: 0.7243 - val_loss: 1.3279 - val_acc: 0.7120\n",
      "Epoch 98/120\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 1.2707 - acc: 0.7246 - val_loss: 1.3139 - val_acc: 0.7140\n",
      "Epoch 99/120\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.2645 - acc: 0.7242 - val_loss: 1.3214 - val_acc: 0.7090\n",
      "Epoch 100/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.2588 - acc: 0.7269 - val_loss: 1.3149 - val_acc: 0.7100\n",
      "Epoch 101/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.2531 - acc: 0.7251 - val_loss: 1.3003 - val_acc: 0.7160\n",
      "Epoch 102/120\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.2467 - acc: 0.7271 - val_loss: 1.2934 - val_acc: 0.7180\n",
      "Epoch 103/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.2414 - acc: 0.7270 - val_loss: 1.2849 - val_acc: 0.7160\n",
      "Epoch 104/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.2352 - acc: 0.7278 - val_loss: 1.2819 - val_acc: 0.7210\n",
      "Epoch 105/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.2299 - acc: 0.7277 - val_loss: 1.2753 - val_acc: 0.7140\n",
      "Epoch 106/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.2240 - acc: 0.7271 - val_loss: 1.2766 - val_acc: 0.7140\n",
      "Epoch 107/120\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.2191 - acc: 0.7292 - val_loss: 1.2638 - val_acc: 0.7150\n",
      "Epoch 108/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.2136 - acc: 0.7280 - val_loss: 1.2642 - val_acc: 0.7210\n",
      "Epoch 109/120\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.2090 - acc: 0.7280 - val_loss: 1.2607 - val_acc: 0.7140\n",
      "Epoch 110/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.2035 - acc: 0.7298 - val_loss: 1.2537 - val_acc: 0.7170\n",
      "Epoch 111/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 1.1994 - acc: 0.7288 - val_loss: 1.2556 - val_acc: 0.7120\n",
      "Epoch 112/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.1944 - acc: 0.7300 - val_loss: 1.2407 - val_acc: 0.7140\n",
      "Epoch 113/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.1887 - acc: 0.7300 - val_loss: 1.2366 - val_acc: 0.7140\n",
      "Epoch 114/120\n",
      "9000/9000 [==============================] - 0s 45us/step - loss: 1.1838 - acc: 0.7319 - val_loss: 1.2314 - val_acc: 0.7120\n",
      "Epoch 115/120\n",
      "9000/9000 [==============================] - 0s 39us/step - loss: 1.1798 - acc: 0.7306 - val_loss: 1.2352 - val_acc: 0.7210\n",
      "Epoch 116/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.1753 - acc: 0.7309 - val_loss: 1.2210 - val_acc: 0.7190\n",
      "Epoch 117/120\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 1.1703 - acc: 0.7312 - val_loss: 1.2212 - val_acc: 0.7100\n",
      "Epoch 118/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.1665 - acc: 0.7321 - val_loss: 1.2141 - val_acc: 0.7180\n",
      "Epoch 119/120\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.1617 - acc: 0.7328 - val_loss: 1.2113 - val_acc: 0.7170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.1576 - acc: 0.7322 - val_loss: 1.2055 - val_acc: 0.7140\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the training and validation accuracy don't diverge as much as before. Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like you can still improve the model by training much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      "9000/9000 [==============================] - 0s 52us/step - loss: 15.9177 - acc: 0.1383 - val_loss: 15.4239 - val_acc: 0.1800\n",
      "Epoch 2/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 15.0046 - acc: 0.1740 - val_loss: 14.5414 - val_acc: 0.2010\n",
      "Epoch 3/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 14.1330 - acc: 0.2078 - val_loss: 13.6895 - val_acc: 0.2190\n",
      "Epoch 4/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 13.2938 - acc: 0.2299 - val_loss: 12.8705 - val_acc: 0.2380\n",
      "Epoch 5/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 12.4869 - acc: 0.2394 - val_loss: 12.0814 - val_acc: 0.2490\n",
      "Epoch 6/1000\n",
      "9000/9000 [==============================] - 0s 42us/step - loss: 11.7102 - acc: 0.2506 - val_loss: 11.3230 - val_acc: 0.2640\n",
      "Epoch 7/1000\n",
      "9000/9000 [==============================] - 0s 38us/step - loss: 10.9621 - acc: 0.2698 - val_loss: 10.5905 - val_acc: 0.2750\n",
      "Epoch 8/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 10.2426 - acc: 0.2882 - val_loss: 9.8861 - val_acc: 0.2960\n",
      "Epoch 9/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 9.5508 - acc: 0.3194 - val_loss: 9.2114 - val_acc: 0.3350\n",
      "Epoch 10/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 8.8876 - acc: 0.3677 - val_loss: 8.5653 - val_acc: 0.3920\n",
      "Epoch 11/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 8.2521 - acc: 0.4232 - val_loss: 7.9461 - val_acc: 0.4430\n",
      "Epoch 12/1000\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 7.6467 - acc: 0.4697 - val_loss: 7.3595 - val_acc: 0.4840\n",
      "Epoch 13/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 7.0736 - acc: 0.5112 - val_loss: 6.8063 - val_acc: 0.5070\n",
      "Epoch 14/1000\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 6.5329 - acc: 0.5364 - val_loss: 6.2858 - val_acc: 0.5310\n",
      "Epoch 15/1000\n",
      "9000/9000 [==============================] - 0s 35us/step - loss: 6.0242 - acc: 0.5588 - val_loss: 5.7910 - val_acc: 0.5450\n",
      "Epoch 16/1000\n",
      "9000/9000 [==============================] - 0s 38us/step - loss: 5.5458 - acc: 0.5787 - val_loss: 5.3292 - val_acc: 0.5800\n",
      "Epoch 17/1000\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 5.0987 - acc: 0.6019 - val_loss: 4.9032 - val_acc: 0.5620\n",
      "Epoch 18/1000\n",
      "9000/9000 [==============================] - 0s 41us/step - loss: 4.6843 - acc: 0.6023 - val_loss: 4.5062 - val_acc: 0.6030\n",
      "Epoch 19/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 4.3012 - acc: 0.6196 - val_loss: 4.1401 - val_acc: 0.6130\n",
      "Epoch 20/1000\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 3.9508 - acc: 0.6297 - val_loss: 3.8075 - val_acc: 0.6280\n",
      "Epoch 21/1000\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 3.6329 - acc: 0.6427 - val_loss: 3.5085 - val_acc: 0.6170\n",
      "Epoch 22/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 3.3471 - acc: 0.6389 - val_loss: 3.2408 - val_acc: 0.6350\n",
      "Epoch 23/1000\n",
      "9000/9000 [==============================] - 0s 38us/step - loss: 3.0930 - acc: 0.6453 - val_loss: 3.0057 - val_acc: 0.6130\n",
      "Epoch 24/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 2.8703 - acc: 0.6456 - val_loss: 2.7983 - val_acc: 0.6100\n",
      "Epoch 25/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 2.6787 - acc: 0.6414 - val_loss: 2.6215 - val_acc: 0.6280\n",
      "Epoch 26/1000\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 2.5170 - acc: 0.6473 - val_loss: 2.4776 - val_acc: 0.6410\n",
      "Epoch 27/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 2.3841 - acc: 0.6494 - val_loss: 2.3585 - val_acc: 0.6370\n",
      "Epoch 28/1000\n",
      "9000/9000 [==============================] - 0s 45us/step - loss: 2.2792 - acc: 0.6499 - val_loss: 2.2680 - val_acc: 0.6390\n",
      "Epoch 29/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 2.2000 - acc: 0.6488 - val_loss: 2.1985 - val_acc: 0.6390\n",
      "Epoch 30/1000\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 2.1438 - acc: 0.6491 - val_loss: 2.1538 - val_acc: 0.6460\n",
      "Epoch 31/1000\n",
      "9000/9000 [==============================] - 0s 36us/step - loss: 2.1045 - acc: 0.6526 - val_loss: 2.1260 - val_acc: 0.6220\n",
      "Epoch 32/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 2.0740 - acc: 0.6510 - val_loss: 2.0943 - val_acc: 0.6270\n",
      "Epoch 33/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 2.0470 - acc: 0.6536 - val_loss: 2.0703 - val_acc: 0.6440\n",
      "Epoch 34/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 2.0235 - acc: 0.6594 - val_loss: 2.0562 - val_acc: 0.6380\n",
      "Epoch 35/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 2.0016 - acc: 0.6617 - val_loss: 2.0268 - val_acc: 0.6400\n",
      "Epoch 36/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.9805 - acc: 0.6616 - val_loss: 2.0031 - val_acc: 0.6550\n",
      "Epoch 37/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 1.9603 - acc: 0.6676 - val_loss: 1.9848 - val_acc: 0.6500\n",
      "Epoch 38/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.9414 - acc: 0.6706 - val_loss: 1.9670 - val_acc: 0.6610\n",
      "Epoch 39/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.9238 - acc: 0.6749 - val_loss: 1.9492 - val_acc: 0.6590\n",
      "Epoch 40/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 1.9062 - acc: 0.6749 - val_loss: 1.9332 - val_acc: 0.6740\n",
      "Epoch 41/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.8895 - acc: 0.6793 - val_loss: 1.9214 - val_acc: 0.6640\n",
      "Epoch 42/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 1.8732 - acc: 0.6791 - val_loss: 1.9009 - val_acc: 0.6660\n",
      "Epoch 43/1000\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 1.8575 - acc: 0.6788 - val_loss: 1.8953 - val_acc: 0.6580\n",
      "Epoch 44/1000\n",
      "9000/9000 [==============================] - 0s 41us/step - loss: 1.8422 - acc: 0.6800 - val_loss: 1.8685 - val_acc: 0.6830\n",
      "Epoch 45/1000\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 1.8268 - acc: 0.6842 - val_loss: 1.8550 - val_acc: 0.6790\n",
      "Epoch 46/1000\n",
      "9000/9000 [==============================] - 0s 46us/step - loss: 1.8123 - acc: 0.6870 - val_loss: 1.8472 - val_acc: 0.6770\n",
      "Epoch 47/1000\n",
      "9000/9000 [==============================] - 0s 42us/step - loss: 1.7984 - acc: 0.6877 - val_loss: 1.8352 - val_acc: 0.6750\n",
      "Epoch 48/1000\n",
      "9000/9000 [==============================] - 0s 45us/step - loss: 1.7846 - acc: 0.6880 - val_loss: 1.8147 - val_acc: 0.6860\n",
      "Epoch 49/1000\n",
      "9000/9000 [==============================] - 0s 45us/step - loss: 1.7715 - acc: 0.6893 - val_loss: 1.8032 - val_acc: 0.6840\n",
      "Epoch 50/1000\n",
      "9000/9000 [==============================] - 0s 43us/step - loss: 1.7577 - acc: 0.6910 - val_loss: 1.7917 - val_acc: 0.6910\n",
      "Epoch 51/1000\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 1.7452 - acc: 0.6941 - val_loss: 1.7773 - val_acc: 0.6880\n",
      "Epoch 52/1000\n",
      "9000/9000 [==============================] - 0s 39us/step - loss: 1.7326 - acc: 0.6928 - val_loss: 1.7627 - val_acc: 0.6920\n",
      "Epoch 53/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 1.7204 - acc: 0.6932 - val_loss: 1.7472 - val_acc: 0.6960\n",
      "Epoch 54/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 1.7080 - acc: 0.6942 - val_loss: 1.7437 - val_acc: 0.6930\n",
      "Epoch 55/1000\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 1.6963 - acc: 0.6956 - val_loss: 1.7295 - val_acc: 0.6910\n",
      "Epoch 56/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.6851 - acc: 0.6951 - val_loss: 1.7157 - val_acc: 0.6910\n",
      "Epoch 57/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.6740 - acc: 0.6979 - val_loss: 1.7115 - val_acc: 0.6900\n",
      "Epoch 58/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.6633 - acc: 0.6962 - val_loss: 1.6969 - val_acc: 0.6960\n",
      "Epoch 59/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 1.6526 - acc: 0.6957 - val_loss: 1.6882 - val_acc: 0.6940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 1.6421 - acc: 0.6980 - val_loss: 1.6738 - val_acc: 0.6940\n",
      "Epoch 61/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.6314 - acc: 0.6974 - val_loss: 1.6641 - val_acc: 0.6970\n",
      "Epoch 62/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.6213 - acc: 0.6979 - val_loss: 1.6533 - val_acc: 0.7020\n",
      "Epoch 63/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.6112 - acc: 0.6983 - val_loss: 1.6505 - val_acc: 0.7030\n",
      "Epoch 64/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.6016 - acc: 0.6996 - val_loss: 1.6395 - val_acc: 0.6990\n",
      "Epoch 65/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.5920 - acc: 0.6988 - val_loss: 1.6309 - val_acc: 0.7010\n",
      "Epoch 66/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.5831 - acc: 0.6997 - val_loss: 1.6188 - val_acc: 0.6930\n",
      "Epoch 67/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.5731 - acc: 0.6994 - val_loss: 1.6186 - val_acc: 0.6980\n",
      "Epoch 68/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.5645 - acc: 0.6993 - val_loss: 1.5965 - val_acc: 0.6970\n",
      "Epoch 69/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.5554 - acc: 0.7023 - val_loss: 1.5874 - val_acc: 0.7020\n",
      "Epoch 70/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.5460 - acc: 0.7019 - val_loss: 1.5800 - val_acc: 0.6990\n",
      "Epoch 71/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.5374 - acc: 0.7024 - val_loss: 1.5670 - val_acc: 0.7020\n",
      "Epoch 72/1000\n",
      "9000/9000 [==============================] - 0s 38us/step - loss: 1.5290 - acc: 0.7037 - val_loss: 1.5663 - val_acc: 0.7040\n",
      "Epoch 73/1000\n",
      "9000/9000 [==============================] - 0s 45us/step - loss: 1.5204 - acc: 0.7036 - val_loss: 1.5593 - val_acc: 0.7010\n",
      "Epoch 74/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.5122 - acc: 0.7034 - val_loss: 1.5465 - val_acc: 0.6970\n",
      "Epoch 75/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.5040 - acc: 0.7041 - val_loss: 1.5419 - val_acc: 0.7010\n",
      "Epoch 76/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.4961 - acc: 0.7050 - val_loss: 1.5367 - val_acc: 0.6990\n",
      "Epoch 77/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.4881 - acc: 0.7049 - val_loss: 1.5259 - val_acc: 0.7010\n",
      "Epoch 78/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.4802 - acc: 0.7072 - val_loss: 1.5134 - val_acc: 0.6980\n",
      "Epoch 79/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.4720 - acc: 0.7071 - val_loss: 1.5111 - val_acc: 0.7050\n",
      "Epoch 80/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 1.4646 - acc: 0.7072 - val_loss: 1.5012 - val_acc: 0.7020\n",
      "Epoch 81/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 1.4570 - acc: 0.7082 - val_loss: 1.4939 - val_acc: 0.7000\n",
      "Epoch 82/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.4498 - acc: 0.7059 - val_loss: 1.4889 - val_acc: 0.7050\n",
      "Epoch 83/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 1.4422 - acc: 0.7094 - val_loss: 1.4755 - val_acc: 0.6990\n",
      "Epoch 84/1000\n",
      "9000/9000 [==============================] - 0s 38us/step - loss: 1.4350 - acc: 0.7099 - val_loss: 1.4753 - val_acc: 0.7060\n",
      "Epoch 85/1000\n",
      "9000/9000 [==============================] - 0s 43us/step - loss: 1.4277 - acc: 0.7102 - val_loss: 1.4615 - val_acc: 0.7060\n",
      "Epoch 86/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 1.4206 - acc: 0.7097 - val_loss: 1.4538 - val_acc: 0.7030\n",
      "Epoch 87/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.4133 - acc: 0.7102 - val_loss: 1.4565 - val_acc: 0.7060\n",
      "Epoch 88/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.4069 - acc: 0.7130 - val_loss: 1.4505 - val_acc: 0.7020\n",
      "Epoch 89/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 1.4005 - acc: 0.7131 - val_loss: 1.4369 - val_acc: 0.7040\n",
      "Epoch 90/1000\n",
      "9000/9000 [==============================] - 0s 43us/step - loss: 1.3932 - acc: 0.7127 - val_loss: 1.4319 - val_acc: 0.7080\n",
      "Epoch 91/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.3869 - acc: 0.7139 - val_loss: 1.4299 - val_acc: 0.7100\n",
      "Epoch 92/1000\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 1.3809 - acc: 0.7139 - val_loss: 1.4169 - val_acc: 0.7030\n",
      "Epoch 93/1000\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 1.3731 - acc: 0.7143 - val_loss: 1.4142 - val_acc: 0.7100\n",
      "Epoch 94/1000\n",
      "9000/9000 [==============================] - 0s 35us/step - loss: 1.3673 - acc: 0.7150 - val_loss: 1.4012 - val_acc: 0.7130\n",
      "Epoch 95/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 1.3610 - acc: 0.7162 - val_loss: 1.4024 - val_acc: 0.7080\n",
      "Epoch 96/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 1.3546 - acc: 0.7166 - val_loss: 1.3936 - val_acc: 0.7110\n",
      "Epoch 97/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.3488 - acc: 0.7164 - val_loss: 1.3881 - val_acc: 0.7140\n",
      "Epoch 98/1000\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 1.3426 - acc: 0.7172 - val_loss: 1.3785 - val_acc: 0.7110\n",
      "Epoch 99/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.3367 - acc: 0.7177 - val_loss: 1.3742 - val_acc: 0.7130\n",
      "Epoch 100/1000\n",
      "9000/9000 [==============================] - 0s 36us/step - loss: 1.3307 - acc: 0.7188 - val_loss: 1.3689 - val_acc: 0.7170\n",
      "Epoch 101/1000\n",
      "9000/9000 [==============================] - 0s 35us/step - loss: 1.3252 - acc: 0.7186 - val_loss: 1.3639 - val_acc: 0.7100\n",
      "Epoch 102/1000\n",
      "9000/9000 [==============================] - 0s 48us/step - loss: 1.3200 - acc: 0.7201 - val_loss: 1.3553 - val_acc: 0.7140\n",
      "Epoch 103/1000\n",
      "9000/9000 [==============================] - 0s 36us/step - loss: 1.3134 - acc: 0.7201 - val_loss: 1.3511 - val_acc: 0.7120\n",
      "Epoch 104/1000\n",
      "9000/9000 [==============================] - 0s 36us/step - loss: 1.3081 - acc: 0.7198 - val_loss: 1.3467 - val_acc: 0.7130\n",
      "Epoch 105/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.3029 - acc: 0.7227 - val_loss: 1.3480 - val_acc: 0.7140\n",
      "Epoch 106/1000\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 1.2977 - acc: 0.7197 - val_loss: 1.3381 - val_acc: 0.7170\n",
      "Epoch 107/1000\n",
      "9000/9000 [==============================] - 0s 35us/step - loss: 1.2922 - acc: 0.7232 - val_loss: 1.3313 - val_acc: 0.7130\n",
      "Epoch 108/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.2864 - acc: 0.7218 - val_loss: 1.3319 - val_acc: 0.7120\n",
      "Epoch 109/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.2813 - acc: 0.7229 - val_loss: 1.3233 - val_acc: 0.7190\n",
      "Epoch 110/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.2769 - acc: 0.7234 - val_loss: 1.3165 - val_acc: 0.7170\n",
      "Epoch 111/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.2712 - acc: 0.7246 - val_loss: 1.3107 - val_acc: 0.7190\n",
      "Epoch 112/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.2664 - acc: 0.7254 - val_loss: 1.3056 - val_acc: 0.7170\n",
      "Epoch 113/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 1.2614 - acc: 0.7252 - val_loss: 1.3057 - val_acc: 0.7160\n",
      "Epoch 114/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.2567 - acc: 0.7249 - val_loss: 1.2968 - val_acc: 0.7210\n",
      "Epoch 115/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.2516 - acc: 0.7256 - val_loss: 1.2925 - val_acc: 0.7190\n",
      "Epoch 116/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.2470 - acc: 0.7263 - val_loss: 1.2915 - val_acc: 0.7170\n",
      "Epoch 117/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.2428 - acc: 0.7261 - val_loss: 1.2837 - val_acc: 0.7170\n",
      "Epoch 118/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.2381 - acc: 0.7267 - val_loss: 1.2747 - val_acc: 0.7210\n",
      "Epoch 119/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.2329 - acc: 0.7272 - val_loss: 1.2759 - val_acc: 0.7190\n",
      "Epoch 120/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.2292 - acc: 0.7278 - val_loss: 1.2714 - val_acc: 0.7180\n",
      "Epoch 121/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.2252 - acc: 0.7294 - val_loss: 1.2692 - val_acc: 0.7190\n",
      "Epoch 122/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.2212 - acc: 0.7284 - val_loss: 1.2614 - val_acc: 0.7160\n",
      "Epoch 123/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.2165 - acc: 0.7292 - val_loss: 1.2593 - val_acc: 0.7180\n",
      "Epoch 124/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.2129 - acc: 0.7297 - val_loss: 1.2526 - val_acc: 0.7220\n",
      "Epoch 125/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.2074 - acc: 0.7310 - val_loss: 1.2509 - val_acc: 0.7210\n",
      "Epoch 126/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.2044 - acc: 0.7312 - val_loss: 1.2456 - val_acc: 0.7250\n",
      "Epoch 127/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 1.2001 - acc: 0.7306 - val_loss: 1.2378 - val_acc: 0.7220\n",
      "Epoch 128/1000\n",
      "9000/9000 [==============================] - 0s 26us/step - loss: 1.1960 - acc: 0.7318 - val_loss: 1.2364 - val_acc: 0.7230\n",
      "Epoch 129/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.1925 - acc: 0.7324 - val_loss: 1.2417 - val_acc: 0.7230\n",
      "Epoch 130/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.1885 - acc: 0.7321 - val_loss: 1.2281 - val_acc: 0.7260\n",
      "Epoch 131/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 1.1851 - acc: 0.7343 - val_loss: 1.2280 - val_acc: 0.7260\n",
      "Epoch 132/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.1817 - acc: 0.7318 - val_loss: 1.2237 - val_acc: 0.7200\n",
      "Epoch 133/1000\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 1.1771 - acc: 0.7346 - val_loss: 1.2261 - val_acc: 0.7270\n",
      "Epoch 134/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.1745 - acc: 0.7333 - val_loss: 1.2116 - val_acc: 0.7220\n",
      "Epoch 135/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.1698 - acc: 0.7357 - val_loss: 1.2217 - val_acc: 0.7220\n",
      "Epoch 136/1000\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 1.1671 - acc: 0.7346 - val_loss: 1.2093 - val_acc: 0.7260\n",
      "Epoch 137/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.1631 - acc: 0.7349 - val_loss: 1.2013 - val_acc: 0.7260\n",
      "Epoch 138/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 1.1589 - acc: 0.7357 - val_loss: 1.2082 - val_acc: 0.7240\n",
      "Epoch 139/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 1.1567 - acc: 0.7358 - val_loss: 1.2010 - val_acc: 0.7280\n",
      "Epoch 140/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.1531 - acc: 0.7379 - val_loss: 1.1952 - val_acc: 0.7260\n",
      "Epoch 141/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.1501 - acc: 0.7366 - val_loss: 1.1942 - val_acc: 0.7250\n",
      "Epoch 142/1000\n",
      "9000/9000 [==============================] - 0s 43us/step - loss: 1.1465 - acc: 0.7374 - val_loss: 1.1932 - val_acc: 0.7270\n",
      "Epoch 143/1000\n",
      "9000/9000 [==============================] - 0s 40us/step - loss: 1.1433 - acc: 0.7386 - val_loss: 1.1886 - val_acc: 0.7310\n",
      "Epoch 144/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.1397 - acc: 0.7389 - val_loss: 1.1852 - val_acc: 0.7330\n",
      "Epoch 145/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.1371 - acc: 0.7406 - val_loss: 1.1814 - val_acc: 0.7280\n",
      "Epoch 146/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 1.1342 - acc: 0.7394 - val_loss: 1.1821 - val_acc: 0.7270\n",
      "Epoch 147/1000\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 1.1319 - acc: 0.7410 - val_loss: 1.1768 - val_acc: 0.7260\n",
      "Epoch 148/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.1279 - acc: 0.7412 - val_loss: 1.1744 - val_acc: 0.7260\n",
      "Epoch 149/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 1.1246 - acc: 0.7423 - val_loss: 1.1715 - val_acc: 0.7250\n",
      "Epoch 150/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.1221 - acc: 0.7412 - val_loss: 1.1653 - val_acc: 0.7300\n",
      "Epoch 151/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 1.1192 - acc: 0.7436 - val_loss: 1.1618 - val_acc: 0.7360\n",
      "Epoch 152/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.1164 - acc: 0.7427 - val_loss: 1.1639 - val_acc: 0.7280\n",
      "Epoch 153/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.1136 - acc: 0.7432 - val_loss: 1.1632 - val_acc: 0.7320\n",
      "Epoch 154/1000\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 1.1116 - acc: 0.7429 - val_loss: 1.1620 - val_acc: 0.7320\n",
      "Epoch 155/1000\n",
      "9000/9000 [==============================] - 0s 35us/step - loss: 1.1088 - acc: 0.7446 - val_loss: 1.1532 - val_acc: 0.7320\n",
      "Epoch 156/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.1061 - acc: 0.7443 - val_loss: 1.1514 - val_acc: 0.7280\n",
      "Epoch 157/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.1037 - acc: 0.7438 - val_loss: 1.1464 - val_acc: 0.7360\n",
      "Epoch 158/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.1011 - acc: 0.7434 - val_loss: 1.1482 - val_acc: 0.7310\n",
      "Epoch 159/1000\n",
      "9000/9000 [==============================] - 0s 40us/step - loss: 1.0979 - acc: 0.7448 - val_loss: 1.1491 - val_acc: 0.7350\n",
      "Epoch 160/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.0970 - acc: 0.7452 - val_loss: 1.1421 - val_acc: 0.7290\n",
      "Epoch 161/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 1.0940 - acc: 0.7460 - val_loss: 1.1607 - val_acc: 0.7360\n",
      "Epoch 162/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.0928 - acc: 0.7463 - val_loss: 1.1400 - val_acc: 0.7370\n",
      "Epoch 163/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.0898 - acc: 0.7482 - val_loss: 1.1417 - val_acc: 0.7420\n",
      "Epoch 164/1000\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 1.0878 - acc: 0.7472 - val_loss: 1.1322 - val_acc: 0.7360\n",
      "Epoch 165/1000\n",
      "9000/9000 [==============================] - 0s 35us/step - loss: 1.0849 - acc: 0.7464 - val_loss: 1.1367 - val_acc: 0.7400\n",
      "Epoch 166/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.0832 - acc: 0.7477 - val_loss: 1.1283 - val_acc: 0.7370\n",
      "Epoch 167/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.0815 - acc: 0.7479 - val_loss: 1.1337 - val_acc: 0.7390\n",
      "Epoch 168/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.0807 - acc: 0.7469 - val_loss: 1.1285 - val_acc: 0.7410\n",
      "Epoch 169/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.0769 - acc: 0.7473 - val_loss: 1.1235 - val_acc: 0.7350\n",
      "Epoch 170/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.0745 - acc: 0.7498 - val_loss: 1.1200 - val_acc: 0.7380\n",
      "Epoch 171/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 1.0730 - acc: 0.7482 - val_loss: 1.1204 - val_acc: 0.7400\n",
      "Epoch 172/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.0708 - acc: 0.7470 - val_loss: 1.1193 - val_acc: 0.7400\n",
      "Epoch 173/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.0688 - acc: 0.7498 - val_loss: 1.1167 - val_acc: 0.7410\n",
      "Epoch 174/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.0668 - acc: 0.7480 - val_loss: 1.1189 - val_acc: 0.7400\n",
      "Epoch 175/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.0652 - acc: 0.7494 - val_loss: 1.1131 - val_acc: 0.7370\n",
      "Epoch 176/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.0636 - acc: 0.7477 - val_loss: 1.1137 - val_acc: 0.7360\n",
      "Epoch 177/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 1.0614 - acc: 0.7492 - val_loss: 1.1063 - val_acc: 0.7360\n",
      "Epoch 178/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.0596 - acc: 0.7494 - val_loss: 1.1070 - val_acc: 0.7380\n",
      "Epoch 179/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.0580 - acc: 0.7501 - val_loss: 1.1063 - val_acc: 0.7440\n",
      "Epoch 180/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 1.0555 - acc: 0.7496 - val_loss: 1.1077 - val_acc: 0.7340\n",
      "Epoch 181/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.0552 - acc: 0.7503 - val_loss: 1.1089 - val_acc: 0.7330\n",
      "Epoch 182/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.0534 - acc: 0.7490 - val_loss: 1.1003 - val_acc: 0.7430\n",
      "Epoch 183/1000\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 1.0511 - acc: 0.7504 - val_loss: 1.1117 - val_acc: 0.7300\n",
      "Epoch 184/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.0505 - acc: 0.7500 - val_loss: 1.1145 - val_acc: 0.7320\n",
      "Epoch 185/1000\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 1.0488 - acc: 0.7499 - val_loss: 1.0926 - val_acc: 0.7370\n",
      "Epoch 186/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.0466 - acc: 0.7517 - val_loss: 1.0981 - val_acc: 0.7410\n",
      "Epoch 187/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.0452 - acc: 0.7510 - val_loss: 1.0946 - val_acc: 0.7380\n",
      "Epoch 188/1000\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 1.0435 - acc: 0.7513 - val_loss: 1.0948 - val_acc: 0.7460\n",
      "Epoch 189/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 1.0423 - acc: 0.7517 - val_loss: 1.0974 - val_acc: 0.7370\n",
      "Epoch 190/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 1.0401 - acc: 0.7519 - val_loss: 1.0905 - val_acc: 0.7380\n",
      "Epoch 191/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.0389 - acc: 0.7516 - val_loss: 1.0999 - val_acc: 0.7400\n",
      "Epoch 192/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.0378 - acc: 0.7544 - val_loss: 1.1083 - val_acc: 0.7420\n",
      "Epoch 193/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 1.0373 - acc: 0.7504 - val_loss: 1.0931 - val_acc: 0.7420\n",
      "Epoch 194/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.0342 - acc: 0.7526 - val_loss: 1.0937 - val_acc: 0.7450\n",
      "Epoch 195/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 1.0333 - acc: 0.7529 - val_loss: 1.0883 - val_acc: 0.7410\n",
      "Epoch 196/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.0319 - acc: 0.7540 - val_loss: 1.0807 - val_acc: 0.7410\n",
      "Epoch 197/1000\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 1.0302 - acc: 0.7527 - val_loss: 1.0931 - val_acc: 0.7370\n",
      "Epoch 198/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.0293 - acc: 0.7541 - val_loss: 1.0792 - val_acc: 0.7400\n",
      "Epoch 199/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.0273 - acc: 0.7547 - val_loss: 1.0764 - val_acc: 0.7460\n",
      "Epoch 200/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.0258 - acc: 0.7538 - val_loss: 1.0836 - val_acc: 0.7410\n",
      "Epoch 201/1000\n",
      "9000/9000 [==============================] - 0s 35us/step - loss: 1.0246 - acc: 0.7548 - val_loss: 1.0761 - val_acc: 0.7390\n",
      "Epoch 202/1000\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 1.0232 - acc: 0.7556 - val_loss: 1.0768 - val_acc: 0.7330\n",
      "Epoch 203/1000\n",
      "9000/9000 [==============================] - 0s 36us/step - loss: 1.0217 - acc: 0.7558 - val_loss: 1.0698 - val_acc: 0.7380\n",
      "Epoch 204/1000\n",
      "9000/9000 [==============================] - 0s 36us/step - loss: 1.0200 - acc: 0.7560 - val_loss: 1.0897 - val_acc: 0.7360\n",
      "Epoch 205/1000\n",
      "9000/9000 [==============================] - 0s 40us/step - loss: 1.0202 - acc: 0.7561 - val_loss: 1.0692 - val_acc: 0.7450\n",
      "Epoch 206/1000\n",
      "9000/9000 [==============================] - 0s 39us/step - loss: 1.0183 - acc: 0.7550 - val_loss: 1.0700 - val_acc: 0.7380\n",
      "Epoch 207/1000\n",
      "9000/9000 [==============================] - 0s 36us/step - loss: 1.0164 - acc: 0.7572 - val_loss: 1.0732 - val_acc: 0.7450\n",
      "Epoch 208/1000\n",
      "9000/9000 [==============================] - 0s 42us/step - loss: 1.0158 - acc: 0.7563 - val_loss: 1.0729 - val_acc: 0.7410\n",
      "Epoch 209/1000\n",
      "9000/9000 [==============================] - 0s 36us/step - loss: 1.0153 - acc: 0.7553 - val_loss: 1.0692 - val_acc: 0.7420\n",
      "Epoch 210/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.0134 - acc: 0.7566 - val_loss: 1.0752 - val_acc: 0.7410\n",
      "Epoch 211/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.0129 - acc: 0.7563 - val_loss: 1.0634 - val_acc: 0.7480\n",
      "Epoch 212/1000\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 1.0109 - acc: 0.7566 - val_loss: 1.0605 - val_acc: 0.7440\n",
      "Epoch 213/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.0091 - acc: 0.7561 - val_loss: 1.0603 - val_acc: 0.7420\n",
      "Epoch 214/1000\n",
      "9000/9000 [==============================] - 0s 43us/step - loss: 1.0086 - acc: 0.7569 - val_loss: 1.0637 - val_acc: 0.7350\n",
      "Epoch 215/1000\n",
      "9000/9000 [==============================] - 0s 36us/step - loss: 1.0068 - acc: 0.7571 - val_loss: 1.0586 - val_acc: 0.7450\n",
      "Epoch 216/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.0061 - acc: 0.7563 - val_loss: 1.0653 - val_acc: 0.7340\n",
      "Epoch 217/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 1.0050 - acc: 0.7572 - val_loss: 1.0625 - val_acc: 0.7400\n",
      "Epoch 218/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 1.0040 - acc: 0.7576 - val_loss: 1.0715 - val_acc: 0.7400\n",
      "Epoch 219/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.0032 - acc: 0.7581 - val_loss: 1.0522 - val_acc: 0.7480\n",
      "Epoch 220/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.0015 - acc: 0.7573 - val_loss: 1.0553 - val_acc: 0.7410\n",
      "Epoch 221/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 1.0010 - acc: 0.7592 - val_loss: 1.0632 - val_acc: 0.7450\n",
      "Epoch 222/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.9995 - acc: 0.7581 - val_loss: 1.0525 - val_acc: 0.7420\n",
      "Epoch 223/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.9982 - acc: 0.7578 - val_loss: 1.0508 - val_acc: 0.7490\n",
      "Epoch 224/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.9977 - acc: 0.7590 - val_loss: 1.0529 - val_acc: 0.7410\n",
      "Epoch 225/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.9962 - acc: 0.7577 - val_loss: 1.0529 - val_acc: 0.7400\n",
      "Epoch 226/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.9955 - acc: 0.7584 - val_loss: 1.0507 - val_acc: 0.7430\n",
      "Epoch 227/1000\n",
      "9000/9000 [==============================] - 0s 42us/step - loss: 0.9945 - acc: 0.7579 - val_loss: 1.0523 - val_acc: 0.7450\n",
      "Epoch 228/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.9931 - acc: 0.7594 - val_loss: 1.0526 - val_acc: 0.7460\n",
      "Epoch 229/1000\n",
      "9000/9000 [==============================] - 0s 35us/step - loss: 0.9932 - acc: 0.7591 - val_loss: 1.0666 - val_acc: 0.7390\n",
      "Epoch 230/1000\n",
      "9000/9000 [==============================] - 0s 36us/step - loss: 0.9922 - acc: 0.7591 - val_loss: 1.0539 - val_acc: 0.7420\n",
      "Epoch 231/1000\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.9917 - acc: 0.7603 - val_loss: 1.0531 - val_acc: 0.7460\n",
      "Epoch 232/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.9900 - acc: 0.7588 - val_loss: 1.0488 - val_acc: 0.7420\n",
      "Epoch 233/1000\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.9891 - acc: 0.7592 - val_loss: 1.0610 - val_acc: 0.7460\n",
      "Epoch 234/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.9881 - acc: 0.7589 - val_loss: 1.0403 - val_acc: 0.7500\n",
      "Epoch 235/1000\n",
      "9000/9000 [==============================] - 0s 43us/step - loss: 0.9864 - acc: 0.7584 - val_loss: 1.0464 - val_acc: 0.7440\n",
      "Epoch 236/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.9858 - acc: 0.7601 - val_loss: 1.0446 - val_acc: 0.7430\n",
      "Epoch 237/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.9843 - acc: 0.7594 - val_loss: 1.0511 - val_acc: 0.7360\n",
      "Epoch 238/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.9845 - acc: 0.7597 - val_loss: 1.0379 - val_acc: 0.7450\n",
      "Epoch 239/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.9828 - acc: 0.7587 - val_loss: 1.0465 - val_acc: 0.7430\n",
      "Epoch 240/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.9819 - acc: 0.7604 - val_loss: 1.0401 - val_acc: 0.7440\n",
      "Epoch 241/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.9811 - acc: 0.7594 - val_loss: 1.0358 - val_acc: 0.7460\n",
      "Epoch 242/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.9793 - acc: 0.7594 - val_loss: 1.0351 - val_acc: 0.7440\n",
      "Epoch 243/1000\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 0.9789 - acc: 0.7606 - val_loss: 1.0395 - val_acc: 0.7430\n",
      "Epoch 244/1000\n",
      "9000/9000 [==============================] - 0s 36us/step - loss: 0.9778 - acc: 0.7600 - val_loss: 1.0472 - val_acc: 0.7450\n",
      "Epoch 245/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.9780 - acc: 0.7601 - val_loss: 1.0397 - val_acc: 0.7410\n",
      "Epoch 246/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.9764 - acc: 0.7620 - val_loss: 1.0321 - val_acc: 0.7520\n",
      "Epoch 247/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.9755 - acc: 0.7609 - val_loss: 1.0572 - val_acc: 0.7290\n",
      "Epoch 248/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.9765 - acc: 0.7612 - val_loss: 1.0386 - val_acc: 0.7470\n",
      "Epoch 249/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.9755 - acc: 0.7606 - val_loss: 1.0385 - val_acc: 0.7460\n",
      "Epoch 250/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.9740 - acc: 0.7619 - val_loss: 1.0342 - val_acc: 0.7510\n",
      "Epoch 251/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.9728 - acc: 0.7608 - val_loss: 1.0289 - val_acc: 0.7470\n",
      "Epoch 252/1000\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.9709 - acc: 0.7611 - val_loss: 1.0281 - val_acc: 0.7490\n",
      "Epoch 253/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.9709 - acc: 0.7623 - val_loss: 1.0313 - val_acc: 0.7410\n",
      "Epoch 254/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.9699 - acc: 0.7631 - val_loss: 1.0367 - val_acc: 0.7410\n",
      "Epoch 255/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.9703 - acc: 0.7628 - val_loss: 1.0330 - val_acc: 0.7400\n",
      "Epoch 256/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.9684 - acc: 0.7634 - val_loss: 1.0352 - val_acc: 0.7470\n",
      "Epoch 257/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.9672 - acc: 0.7629 - val_loss: 1.0228 - val_acc: 0.7470\n",
      "Epoch 258/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.9664 - acc: 0.7610 - val_loss: 1.0302 - val_acc: 0.7450\n",
      "Epoch 259/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.9654 - acc: 0.7621 - val_loss: 1.0311 - val_acc: 0.7380\n",
      "Epoch 260/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.9651 - acc: 0.7643 - val_loss: 1.0236 - val_acc: 0.7510\n",
      "Epoch 261/1000\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.9642 - acc: 0.7619 - val_loss: 1.0225 - val_acc: 0.7510\n",
      "Epoch 262/1000\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.9627 - acc: 0.7639 - val_loss: 1.0222 - val_acc: 0.7490\n",
      "Epoch 263/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.9631 - acc: 0.7637 - val_loss: 1.0288 - val_acc: 0.7450\n",
      "Epoch 264/1000\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 0.9628 - acc: 0.7620 - val_loss: 1.0211 - val_acc: 0.7480\n",
      "Epoch 265/1000\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.9609 - acc: 0.7638 - val_loss: 1.0221 - val_acc: 0.7460\n",
      "Epoch 266/1000\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 0.9604 - acc: 0.7649 - val_loss: 1.0143 - val_acc: 0.7560\n",
      "Epoch 267/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.9602 - acc: 0.7640 - val_loss: 1.0170 - val_acc: 0.7500\n",
      "Epoch 268/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.9590 - acc: 0.7636 - val_loss: 1.0216 - val_acc: 0.7400\n",
      "Epoch 269/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.9585 - acc: 0.7612 - val_loss: 1.0187 - val_acc: 0.7420\n",
      "Epoch 270/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.9572 - acc: 0.7634 - val_loss: 1.0127 - val_acc: 0.7560\n",
      "Epoch 271/1000\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.9569 - acc: 0.7640 - val_loss: 1.0307 - val_acc: 0.7400\n",
      "Epoch 272/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.9568 - acc: 0.7623 - val_loss: 1.0125 - val_acc: 0.7450\n",
      "Epoch 273/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.9550 - acc: 0.7656 - val_loss: 1.0192 - val_acc: 0.7540\n",
      "Epoch 274/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.9551 - acc: 0.7644 - val_loss: 1.0146 - val_acc: 0.7420\n",
      "Epoch 275/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.9540 - acc: 0.7650 - val_loss: 1.0142 - val_acc: 0.7450\n",
      "Epoch 276/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.9531 - acc: 0.7634 - val_loss: 1.0162 - val_acc: 0.7470\n",
      "Epoch 277/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.9520 - acc: 0.7627 - val_loss: 1.0098 - val_acc: 0.7550\n",
      "Epoch 278/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.9517 - acc: 0.7646 - val_loss: 1.0173 - val_acc: 0.7510\n",
      "Epoch 279/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.9518 - acc: 0.7641 - val_loss: 1.0159 - val_acc: 0.7440\n",
      "Epoch 280/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.9514 - acc: 0.7639 - val_loss: 1.0213 - val_acc: 0.7410\n",
      "Epoch 281/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.9494 - acc: 0.7666 - val_loss: 1.0122 - val_acc: 0.7420\n",
      "Epoch 282/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.9495 - acc: 0.7647 - val_loss: 1.0198 - val_acc: 0.7580\n",
      "Epoch 283/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.9489 - acc: 0.7661 - val_loss: 1.0117 - val_acc: 0.7500\n",
      "Epoch 284/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.9483 - acc: 0.7653 - val_loss: 1.0099 - val_acc: 0.7490\n",
      "Epoch 285/1000\n",
      "9000/9000 [==============================] - 0s 51us/step - loss: 0.9470 - acc: 0.7654 - val_loss: 1.0135 - val_acc: 0.7480\n",
      "Epoch 286/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.9472 - acc: 0.7657 - val_loss: 1.0183 - val_acc: 0.7460\n",
      "Epoch 287/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.9467 - acc: 0.7664 - val_loss: 1.0043 - val_acc: 0.7500\n",
      "Epoch 288/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.9448 - acc: 0.7668 - val_loss: 1.0114 - val_acc: 0.7490\n",
      "Epoch 289/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.9450 - acc: 0.7682 - val_loss: 1.0095 - val_acc: 0.7550\n",
      "Epoch 290/1000\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 0.9455 - acc: 0.7656 - val_loss: 1.0021 - val_acc: 0.7480\n",
      "Epoch 291/1000\n",
      "9000/9000 [==============================] - 0s 39us/step - loss: 0.9432 - acc: 0.7648 - val_loss: 1.0072 - val_acc: 0.7550\n",
      "Epoch 292/1000\n",
      "9000/9000 [==============================] - 0s 39us/step - loss: 0.9432 - acc: 0.7658 - val_loss: 1.0085 - val_acc: 0.7540\n",
      "Epoch 293/1000\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.9420 - acc: 0.7661 - val_loss: 1.0112 - val_acc: 0.7470\n",
      "Epoch 294/1000\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 0.9415 - acc: 0.7667 - val_loss: 1.0046 - val_acc: 0.7590\n",
      "Epoch 295/1000\n",
      "9000/9000 [==============================] - 0s 41us/step - loss: 0.9411 - acc: 0.7666 - val_loss: 1.0060 - val_acc: 0.7500\n",
      "Epoch 296/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 0s 37us/step - loss: 0.9400 - acc: 0.7688 - val_loss: 1.0037 - val_acc: 0.7550\n",
      "Epoch 297/1000\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 0.9398 - acc: 0.7677 - val_loss: 0.9972 - val_acc: 0.7550\n",
      "Epoch 298/1000\n",
      "9000/9000 [==============================] - 0s 39us/step - loss: 0.9383 - acc: 0.7681 - val_loss: 0.9965 - val_acc: 0.7530\n",
      "Epoch 299/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.9385 - acc: 0.7664 - val_loss: 0.9964 - val_acc: 0.7540\n",
      "Epoch 300/1000\n",
      "9000/9000 [==============================] - 0s 39us/step - loss: 0.9380 - acc: 0.7687 - val_loss: 1.0013 - val_acc: 0.7530\n",
      "Epoch 301/1000\n",
      "9000/9000 [==============================] - 0s 41us/step - loss: 0.9369 - acc: 0.7677 - val_loss: 1.0240 - val_acc: 0.7410\n",
      "Epoch 302/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.9375 - acc: 0.7678 - val_loss: 1.0015 - val_acc: 0.7480\n",
      "Epoch 303/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.9357 - acc: 0.7690 - val_loss: 1.0092 - val_acc: 0.7450\n",
      "Epoch 304/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.9346 - acc: 0.7672 - val_loss: 1.0191 - val_acc: 0.7360\n",
      "Epoch 305/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.9353 - acc: 0.7678 - val_loss: 0.9964 - val_acc: 0.7450\n",
      "Epoch 306/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.9338 - acc: 0.7680 - val_loss: 0.9995 - val_acc: 0.7440\n",
      "Epoch 307/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.9334 - acc: 0.7672 - val_loss: 1.0176 - val_acc: 0.7420\n",
      "Epoch 308/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.9352 - acc: 0.7669 - val_loss: 0.9930 - val_acc: 0.7470\n",
      "Epoch 309/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.9316 - acc: 0.7708 - val_loss: 0.9964 - val_acc: 0.7430\n",
      "Epoch 310/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.9316 - acc: 0.7701 - val_loss: 0.9961 - val_acc: 0.7560\n",
      "Epoch 311/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.9306 - acc: 0.7677 - val_loss: 0.9904 - val_acc: 0.7540\n",
      "Epoch 312/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.9306 - acc: 0.7688 - val_loss: 0.9915 - val_acc: 0.7590\n",
      "Epoch 313/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.9303 - acc: 0.7673 - val_loss: 0.9914 - val_acc: 0.7510\n",
      "Epoch 314/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.9297 - acc: 0.7679 - val_loss: 0.9993 - val_acc: 0.7510\n",
      "Epoch 315/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.9295 - acc: 0.7689 - val_loss: 0.9997 - val_acc: 0.7600\n",
      "Epoch 316/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.9289 - acc: 0.7706 - val_loss: 0.9928 - val_acc: 0.7460\n",
      "Epoch 317/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.9275 - acc: 0.7692 - val_loss: 0.9949 - val_acc: 0.7540\n",
      "Epoch 318/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.9276 - acc: 0.7691 - val_loss: 1.0051 - val_acc: 0.7450\n",
      "Epoch 319/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.9270 - acc: 0.7696 - val_loss: 0.9967 - val_acc: 0.7470\n",
      "Epoch 320/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.9263 - acc: 0.7696 - val_loss: 0.9934 - val_acc: 0.7540\n",
      "Epoch 321/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.9264 - acc: 0.7697 - val_loss: 0.9900 - val_acc: 0.7500\n",
      "Epoch 322/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.9262 - acc: 0.7699 - val_loss: 0.9852 - val_acc: 0.7560\n",
      "Epoch 323/1000\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.9245 - acc: 0.7702 - val_loss: 0.9901 - val_acc: 0.7540\n",
      "Epoch 324/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.9240 - acc: 0.7690 - val_loss: 0.9994 - val_acc: 0.7580\n",
      "Epoch 325/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.9238 - acc: 0.7687 - val_loss: 0.9851 - val_acc: 0.7620\n",
      "Epoch 326/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.9228 - acc: 0.7696 - val_loss: 0.9919 - val_acc: 0.7550\n",
      "Epoch 327/1000\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 0.9224 - acc: 0.7720 - val_loss: 0.9846 - val_acc: 0.7610\n",
      "Epoch 328/1000\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 0.9211 - acc: 0.7722 - val_loss: 0.9874 - val_acc: 0.7560\n",
      "Epoch 329/1000\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.9219 - acc: 0.7711 - val_loss: 0.9898 - val_acc: 0.7550\n",
      "Epoch 330/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.9219 - acc: 0.7701 - val_loss: 0.9858 - val_acc: 0.7550\n",
      "Epoch 331/1000\n",
      "9000/9000 [==============================] - 0s 52us/step - loss: 0.9210 - acc: 0.7713 - val_loss: 0.9893 - val_acc: 0.7610\n",
      "Epoch 332/1000\n",
      "9000/9000 [==============================] - 0s 46us/step - loss: 0.9208 - acc: 0.7701 - val_loss: 0.9911 - val_acc: 0.7420\n",
      "Epoch 333/1000\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 0.9201 - acc: 0.7716 - val_loss: 0.9837 - val_acc: 0.7610\n",
      "Epoch 334/1000\n",
      "9000/9000 [==============================] - 0s 40us/step - loss: 0.9189 - acc: 0.7720 - val_loss: 0.9811 - val_acc: 0.7590\n",
      "Epoch 335/1000\n",
      "9000/9000 [==============================] - 0s 39us/step - loss: 0.9186 - acc: 0.7711 - val_loss: 0.9824 - val_acc: 0.7550\n",
      "Epoch 336/1000\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 0.9181 - acc: 0.7709 - val_loss: 0.9870 - val_acc: 0.7500\n",
      "Epoch 337/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.9178 - acc: 0.7731 - val_loss: 0.9960 - val_acc: 0.7490\n",
      "Epoch 338/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.9176 - acc: 0.7713 - val_loss: 0.9864 - val_acc: 0.7540\n",
      "Epoch 339/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.9161 - acc: 0.7729 - val_loss: 0.9808 - val_acc: 0.7560\n",
      "Epoch 340/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.9163 - acc: 0.7708 - val_loss: 1.0153 - val_acc: 0.7380\n",
      "Epoch 341/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.9169 - acc: 0.7708 - val_loss: 0.9879 - val_acc: 0.7440\n",
      "Epoch 342/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.9152 - acc: 0.7704 - val_loss: 0.9818 - val_acc: 0.7500\n",
      "Epoch 343/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.9154 - acc: 0.7709 - val_loss: 0.9787 - val_acc: 0.7550\n",
      "Epoch 344/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.9141 - acc: 0.7733 - val_loss: 0.9833 - val_acc: 0.7560\n",
      "Epoch 345/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.9139 - acc: 0.7708 - val_loss: 0.9817 - val_acc: 0.7590\n",
      "Epoch 346/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.9139 - acc: 0.7707 - val_loss: 0.9858 - val_acc: 0.7540\n",
      "Epoch 347/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.9130 - acc: 0.7702 - val_loss: 0.9775 - val_acc: 0.7590\n",
      "Epoch 348/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.9123 - acc: 0.7731 - val_loss: 0.9765 - val_acc: 0.7550\n",
      "Epoch 349/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.9124 - acc: 0.7723 - val_loss: 0.9803 - val_acc: 0.7520\n",
      "Epoch 350/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.9113 - acc: 0.7714 - val_loss: 0.9910 - val_acc: 0.7510\n",
      "Epoch 351/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.9117 - acc: 0.7722 - val_loss: 0.9834 - val_acc: 0.7560\n",
      "Epoch 352/1000\n",
      "9000/9000 [==============================] - 0s 45us/step - loss: 0.9106 - acc: 0.7718 - val_loss: 0.9787 - val_acc: 0.7580\n",
      "Epoch 353/1000\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 0.9104 - acc: 0.7729 - val_loss: 0.9834 - val_acc: 0.7470\n",
      "Epoch 354/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.9100 - acc: 0.7722 - val_loss: 0.9713 - val_acc: 0.7620\n",
      "Epoch 355/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.9092 - acc: 0.7707 - val_loss: 0.9764 - val_acc: 0.7560\n",
      "Epoch 356/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.9094 - acc: 0.7723 - val_loss: 0.9906 - val_acc: 0.7480\n",
      "Epoch 357/1000\n",
      "9000/9000 [==============================] - 0s 35us/step - loss: 0.9104 - acc: 0.7728 - val_loss: 0.9764 - val_acc: 0.7570\n",
      "Epoch 358/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.9075 - acc: 0.7710 - val_loss: 0.9782 - val_acc: 0.7530\n",
      "Epoch 359/1000\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.9075 - acc: 0.7717 - val_loss: 0.9747 - val_acc: 0.7560\n",
      "Epoch 360/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.9069 - acc: 0.7742 - val_loss: 0.9785 - val_acc: 0.7600\n",
      "Epoch 361/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.9066 - acc: 0.7731 - val_loss: 0.9724 - val_acc: 0.7590\n",
      "Epoch 362/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.9060 - acc: 0.7742 - val_loss: 0.9843 - val_acc: 0.7490\n",
      "Epoch 363/1000\n",
      "9000/9000 [==============================] - 0s 38us/step - loss: 0.9075 - acc: 0.7717 - val_loss: 0.9855 - val_acc: 0.7570\n",
      "Epoch 364/1000\n",
      "9000/9000 [==============================] - 0s 38us/step - loss: 0.9054 - acc: 0.7731 - val_loss: 0.9698 - val_acc: 0.7620\n",
      "Epoch 365/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.9045 - acc: 0.7749 - val_loss: 0.9735 - val_acc: 0.7510\n",
      "Epoch 366/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.9041 - acc: 0.7732 - val_loss: 0.9773 - val_acc: 0.7580\n",
      "Epoch 367/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.9041 - acc: 0.7744 - val_loss: 0.9675 - val_acc: 0.7680\n",
      "Epoch 368/1000\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 0.9033 - acc: 0.7730 - val_loss: 0.9787 - val_acc: 0.7570\n",
      "Epoch 369/1000\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 0.9040 - acc: 0.7727 - val_loss: 0.9734 - val_acc: 0.7460\n",
      "Epoch 370/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.9025 - acc: 0.7742 - val_loss: 0.9803 - val_acc: 0.7510\n",
      "Epoch 371/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.9028 - acc: 0.7719 - val_loss: 0.9740 - val_acc: 0.7600\n",
      "Epoch 372/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.9030 - acc: 0.7738 - val_loss: 0.9677 - val_acc: 0.7560\n",
      "Epoch 373/1000\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 0.9004 - acc: 0.7741 - val_loss: 0.9772 - val_acc: 0.7630\n",
      "Epoch 374/1000\n",
      "9000/9000 [==============================] - 0s 40us/step - loss: 0.9013 - acc: 0.7733 - val_loss: 0.9750 - val_acc: 0.7520\n",
      "Epoch 375/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.9008 - acc: 0.7720 - val_loss: 0.9660 - val_acc: 0.7610\n",
      "Epoch 376/1000\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.9005 - acc: 0.7750 - val_loss: 0.9827 - val_acc: 0.7450\n",
      "Epoch 377/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.9004 - acc: 0.7728 - val_loss: 0.9668 - val_acc: 0.7630\n",
      "Epoch 378/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8992 - acc: 0.7750 - val_loss: 0.9853 - val_acc: 0.7540\n",
      "Epoch 379/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.9000 - acc: 0.7756 - val_loss: 0.9698 - val_acc: 0.7620\n",
      "Epoch 380/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8983 - acc: 0.7757 - val_loss: 0.9698 - val_acc: 0.7620\n",
      "Epoch 381/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8978 - acc: 0.7750 - val_loss: 0.9776 - val_acc: 0.7510\n",
      "Epoch 382/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.9003 - acc: 0.7761 - val_loss: 0.9680 - val_acc: 0.7540\n",
      "Epoch 383/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8975 - acc: 0.7739 - val_loss: 0.9646 - val_acc: 0.7570\n",
      "Epoch 384/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8964 - acc: 0.7774 - val_loss: 0.9673 - val_acc: 0.7630\n",
      "Epoch 385/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8962 - acc: 0.7759 - val_loss: 0.9656 - val_acc: 0.7610\n",
      "Epoch 386/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8958 - acc: 0.7753 - val_loss: 0.9638 - val_acc: 0.7620\n",
      "Epoch 387/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8962 - acc: 0.7750 - val_loss: 0.9716 - val_acc: 0.7620\n",
      "Epoch 388/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8966 - acc: 0.7744 - val_loss: 0.9735 - val_acc: 0.7500\n",
      "Epoch 389/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8958 - acc: 0.7747 - val_loss: 0.9724 - val_acc: 0.7630\n",
      "Epoch 390/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8948 - acc: 0.7734 - val_loss: 0.9654 - val_acc: 0.7580\n",
      "Epoch 391/1000\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.8950 - acc: 0.7764 - val_loss: 0.9732 - val_acc: 0.7620\n",
      "Epoch 392/1000\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.8944 - acc: 0.7742 - val_loss: 0.9614 - val_acc: 0.7620\n",
      "Epoch 393/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8935 - acc: 0.7741 - val_loss: 0.9636 - val_acc: 0.7450\n",
      "Epoch 394/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8931 - acc: 0.7728 - val_loss: 0.9601 - val_acc: 0.7550\n",
      "Epoch 395/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8937 - acc: 0.7741 - val_loss: 0.9612 - val_acc: 0.7610\n",
      "Epoch 396/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8927 - acc: 0.7774 - val_loss: 0.9566 - val_acc: 0.7580\n",
      "Epoch 397/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8919 - acc: 0.7763 - val_loss: 0.9607 - val_acc: 0.7530\n",
      "Epoch 398/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8924 - acc: 0.7762 - val_loss: 0.9674 - val_acc: 0.7540\n",
      "Epoch 399/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8923 - acc: 0.7762 - val_loss: 0.9654 - val_acc: 0.7620\n",
      "Epoch 400/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8901 - acc: 0.7771 - val_loss: 0.9570 - val_acc: 0.7580\n",
      "Epoch 401/1000\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.8904 - acc: 0.7761 - val_loss: 0.9605 - val_acc: 0.7580\n",
      "Epoch 402/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8903 - acc: 0.7769 - val_loss: 0.9599 - val_acc: 0.7500\n",
      "Epoch 403/1000\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.8906 - acc: 0.7773 - val_loss: 0.9710 - val_acc: 0.7570\n",
      "Epoch 404/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8900 - acc: 0.7764 - val_loss: 0.9547 - val_acc: 0.7640\n",
      "Epoch 405/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8882 - acc: 0.7759 - val_loss: 0.9621 - val_acc: 0.7490\n",
      "Epoch 406/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8883 - acc: 0.7766 - val_loss: 0.9559 - val_acc: 0.7600\n",
      "Epoch 407/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8883 - acc: 0.7769 - val_loss: 0.9535 - val_acc: 0.7640\n",
      "Epoch 408/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8879 - acc: 0.7771 - val_loss: 0.9867 - val_acc: 0.7320\n",
      "Epoch 409/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8898 - acc: 0.7769 - val_loss: 0.9532 - val_acc: 0.7600\n",
      "Epoch 410/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8878 - acc: 0.7771 - val_loss: 0.9595 - val_acc: 0.7600\n",
      "Epoch 411/1000\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.8868 - acc: 0.7773 - val_loss: 0.9552 - val_acc: 0.7650\n",
      "Epoch 412/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8870 - acc: 0.7789 - val_loss: 0.9611 - val_acc: 0.7540\n",
      "Epoch 413/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8869 - acc: 0.7763 - val_loss: 0.9563 - val_acc: 0.7660\n",
      "Epoch 414/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8856 - acc: 0.7780 - val_loss: 0.9511 - val_acc: 0.7660\n",
      "Epoch 415/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8849 - acc: 0.7783 - val_loss: 0.9547 - val_acc: 0.7580\n",
      "Epoch 416/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8849 - acc: 0.7767 - val_loss: 0.9544 - val_acc: 0.7650\n",
      "Epoch 417/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8837 - acc: 0.7774 - val_loss: 0.9590 - val_acc: 0.7490\n",
      "Epoch 418/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.8849 - acc: 0.7763 - val_loss: 0.9666 - val_acc: 0.7540\n",
      "Epoch 419/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8843 - acc: 0.7760 - val_loss: 0.9552 - val_acc: 0.7640\n",
      "Epoch 420/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8839 - acc: 0.7764 - val_loss: 0.9555 - val_acc: 0.7690\n",
      "Epoch 421/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8835 - acc: 0.7780 - val_loss: 0.9634 - val_acc: 0.7590\n",
      "Epoch 422/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8839 - acc: 0.7773 - val_loss: 0.9643 - val_acc: 0.7530\n",
      "Epoch 423/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8849 - acc: 0.7772 - val_loss: 0.9539 - val_acc: 0.7700\n",
      "Epoch 424/1000\n",
      "9000/9000 [==============================] - 0s 43us/step - loss: 0.8838 - acc: 0.7776 - val_loss: 0.9604 - val_acc: 0.7670\n",
      "Epoch 425/1000\n",
      "9000/9000 [==============================] - 0s 40us/step - loss: 0.8830 - acc: 0.7779 - val_loss: 0.9629 - val_acc: 0.7650\n",
      "Epoch 426/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8827 - acc: 0.7786 - val_loss: 0.9548 - val_acc: 0.7550\n",
      "Epoch 427/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8807 - acc: 0.7782 - val_loss: 0.9474 - val_acc: 0.7630\n",
      "Epoch 428/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8817 - acc: 0.7786 - val_loss: 0.9517 - val_acc: 0.7690\n",
      "Epoch 429/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8810 - acc: 0.7792 - val_loss: 0.9662 - val_acc: 0.7520\n",
      "Epoch 430/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8821 - acc: 0.7800 - val_loss: 0.9494 - val_acc: 0.7550\n",
      "Epoch 431/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8787 - acc: 0.7802 - val_loss: 0.9553 - val_acc: 0.7620\n",
      "Epoch 432/1000\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.8798 - acc: 0.7771 - val_loss: 0.9530 - val_acc: 0.7490\n",
      "Epoch 433/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8804 - acc: 0.7796 - val_loss: 0.9507 - val_acc: 0.7670\n",
      "Epoch 434/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8794 - acc: 0.7784 - val_loss: 0.9478 - val_acc: 0.7600\n",
      "Epoch 435/1000\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.8789 - acc: 0.7779 - val_loss: 0.9538 - val_acc: 0.7600\n",
      "Epoch 436/1000\n",
      "9000/9000 [==============================] - 0s 41us/step - loss: 0.8785 - acc: 0.7806 - val_loss: 0.9493 - val_acc: 0.7610\n",
      "Epoch 437/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8778 - acc: 0.7771 - val_loss: 0.9606 - val_acc: 0.7550\n",
      "Epoch 438/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8776 - acc: 0.7801 - val_loss: 0.9513 - val_acc: 0.7590\n",
      "Epoch 439/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8773 - acc: 0.7791 - val_loss: 0.9454 - val_acc: 0.7620\n",
      "Epoch 440/1000\n",
      "9000/9000 [==============================] - 0s 36us/step - loss: 0.8759 - acc: 0.7807 - val_loss: 0.9700 - val_acc: 0.7510\n",
      "Epoch 441/1000\n",
      "9000/9000 [==============================] - 0s 38us/step - loss: 0.8783 - acc: 0.7804 - val_loss: 0.9524 - val_acc: 0.7630\n",
      "Epoch 442/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8771 - acc: 0.7800 - val_loss: 0.9542 - val_acc: 0.7690\n",
      "Epoch 443/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8764 - acc: 0.7799 - val_loss: 0.9873 - val_acc: 0.7430\n",
      "Epoch 444/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8769 - acc: 0.7826 - val_loss: 0.9439 - val_acc: 0.7620\n",
      "Epoch 445/1000\n",
      "9000/9000 [==============================] - 0s 38us/step - loss: 0.8762 - acc: 0.7799 - val_loss: 0.9623 - val_acc: 0.7660\n",
      "Epoch 446/1000\n",
      "9000/9000 [==============================] - 0s 35us/step - loss: 0.8771 - acc: 0.7799 - val_loss: 0.9563 - val_acc: 0.7620\n",
      "Epoch 447/1000\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.8759 - acc: 0.7810 - val_loss: 0.9582 - val_acc: 0.7580\n",
      "Epoch 448/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8762 - acc: 0.7788 - val_loss: 0.9441 - val_acc: 0.7720\n",
      "Epoch 449/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8739 - acc: 0.7799 - val_loss: 0.9469 - val_acc: 0.7700\n",
      "Epoch 450/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8739 - acc: 0.7806 - val_loss: 0.9469 - val_acc: 0.7630\n",
      "Epoch 451/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8737 - acc: 0.7794 - val_loss: 0.9507 - val_acc: 0.7670\n",
      "Epoch 452/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8738 - acc: 0.7797 - val_loss: 0.9604 - val_acc: 0.7550\n",
      "Epoch 453/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8733 - acc: 0.7829 - val_loss: 0.9439 - val_acc: 0.7700\n",
      "Epoch 454/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8726 - acc: 0.7823 - val_loss: 0.9514 - val_acc: 0.7610\n",
      "Epoch 455/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8727 - acc: 0.7803 - val_loss: 0.9422 - val_acc: 0.7630\n",
      "Epoch 456/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8719 - acc: 0.7803 - val_loss: 0.9500 - val_acc: 0.7550\n",
      "Epoch 457/1000\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 0.8715 - acc: 0.7822 - val_loss: 0.9429 - val_acc: 0.7610\n",
      "Epoch 458/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8707 - acc: 0.7821 - val_loss: 0.9807 - val_acc: 0.7430\n",
      "Epoch 459/1000\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.8731 - acc: 0.7791 - val_loss: 0.9555 - val_acc: 0.7400\n",
      "Epoch 460/1000\n",
      "9000/9000 [==============================] - 0s 46us/step - loss: 0.8718 - acc: 0.7824 - val_loss: 0.9586 - val_acc: 0.7600\n",
      "Epoch 461/1000\n",
      "9000/9000 [==============================] - 0s 49us/step - loss: 0.8726 - acc: 0.7814 - val_loss: 0.9469 - val_acc: 0.7670\n",
      "Epoch 462/1000\n",
      "9000/9000 [==============================] - 0s 46us/step - loss: 0.8704 - acc: 0.7816 - val_loss: 0.9525 - val_acc: 0.7640\n",
      "Epoch 463/1000\n",
      "9000/9000 [==============================] - 0s 44us/step - loss: 0.8708 - acc: 0.7808 - val_loss: 0.9681 - val_acc: 0.7460\n",
      "Epoch 464/1000\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 0.8696 - acc: 0.7810 - val_loss: 0.9372 - val_acc: 0.7590\n",
      "Epoch 465/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8687 - acc: 0.7831 - val_loss: 0.9436 - val_acc: 0.7640\n",
      "Epoch 466/1000\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.8688 - acc: 0.7820 - val_loss: 0.9510 - val_acc: 0.7660\n",
      "Epoch 467/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8689 - acc: 0.7801 - val_loss: 0.9392 - val_acc: 0.7610\n",
      "Epoch 468/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8679 - acc: 0.7823 - val_loss: 0.9429 - val_acc: 0.7640\n",
      "Epoch 469/1000\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.8669 - acc: 0.7827 - val_loss: 0.9649 - val_acc: 0.7540\n",
      "Epoch 470/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8700 - acc: 0.7809 - val_loss: 0.9387 - val_acc: 0.7600\n",
      "Epoch 471/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8687 - acc: 0.7848 - val_loss: 0.9532 - val_acc: 0.7540\n",
      "Epoch 472/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8681 - acc: 0.7821 - val_loss: 0.9477 - val_acc: 0.7560\n",
      "Epoch 473/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8671 - acc: 0.7824 - val_loss: 0.9552 - val_acc: 0.7540\n",
      "Epoch 474/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8671 - acc: 0.7816 - val_loss: 0.9347 - val_acc: 0.7700\n",
      "Epoch 475/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8664 - acc: 0.7826 - val_loss: 0.9360 - val_acc: 0.7650\n",
      "Epoch 476/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8659 - acc: 0.7818 - val_loss: 0.9382 - val_acc: 0.7680\n",
      "Epoch 477/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8648 - acc: 0.7837 - val_loss: 0.9398 - val_acc: 0.7630\n",
      "Epoch 478/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8655 - acc: 0.7833 - val_loss: 0.9646 - val_acc: 0.7390\n",
      "Epoch 479/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8659 - acc: 0.7828 - val_loss: 0.9572 - val_acc: 0.7530\n",
      "Epoch 480/1000\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.8657 - acc: 0.7824 - val_loss: 0.9862 - val_acc: 0.7310\n",
      "Epoch 481/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8672 - acc: 0.7813 - val_loss: 0.9511 - val_acc: 0.7590\n",
      "Epoch 482/1000\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.8654 - acc: 0.7852 - val_loss: 0.9369 - val_acc: 0.7560\n",
      "Epoch 483/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8643 - acc: 0.7824 - val_loss: 0.9388 - val_acc: 0.7640\n",
      "Epoch 484/1000\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.8635 - acc: 0.7826 - val_loss: 0.9352 - val_acc: 0.7600\n",
      "Epoch 485/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8633 - acc: 0.7850 - val_loss: 0.9395 - val_acc: 0.7610\n",
      "Epoch 486/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8640 - acc: 0.7849 - val_loss: 0.9354 - val_acc: 0.7600\n",
      "Epoch 487/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8628 - acc: 0.7840 - val_loss: 0.9382 - val_acc: 0.7700\n",
      "Epoch 488/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8622 - acc: 0.7842 - val_loss: 0.9489 - val_acc: 0.7560\n",
      "Epoch 489/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8631 - acc: 0.7857 - val_loss: 0.9357 - val_acc: 0.7600\n",
      "Epoch 490/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8627 - acc: 0.7857 - val_loss: 0.9336 - val_acc: 0.7690\n",
      "Epoch 491/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8625 - acc: 0.7840 - val_loss: 0.9331 - val_acc: 0.7650\n",
      "Epoch 492/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8616 - acc: 0.7837 - val_loss: 0.9763 - val_acc: 0.7410\n",
      "Epoch 493/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8626 - acc: 0.7841 - val_loss: 0.9422 - val_acc: 0.7610\n",
      "Epoch 494/1000\n",
      "9000/9000 [==============================] - 0s 45us/step - loss: 0.8611 - acc: 0.7854 - val_loss: 0.9565 - val_acc: 0.7480\n",
      "Epoch 495/1000\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 0.8622 - acc: 0.7839 - val_loss: 0.9362 - val_acc: 0.7630\n",
      "Epoch 496/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8611 - acc: 0.7827 - val_loss: 0.9531 - val_acc: 0.7630\n",
      "Epoch 497/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8606 - acc: 0.7844 - val_loss: 0.9441 - val_acc: 0.7610\n",
      "Epoch 498/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8608 - acc: 0.7850 - val_loss: 0.9327 - val_acc: 0.7650\n",
      "Epoch 499/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8597 - acc: 0.7849 - val_loss: 0.9348 - val_acc: 0.7660\n",
      "Epoch 500/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8587 - acc: 0.7848 - val_loss: 0.9465 - val_acc: 0.7590\n",
      "Epoch 501/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8587 - acc: 0.7838 - val_loss: 0.9277 - val_acc: 0.7550\n",
      "Epoch 502/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8582 - acc: 0.7872 - val_loss: 0.9662 - val_acc: 0.7430\n",
      "Epoch 503/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8592 - acc: 0.7839 - val_loss: 0.9401 - val_acc: 0.7580\n",
      "Epoch 504/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8586 - acc: 0.7849 - val_loss: 0.9467 - val_acc: 0.7610\n",
      "Epoch 505/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8598 - acc: 0.7843 - val_loss: 0.9417 - val_acc: 0.7590\n",
      "Epoch 506/1000\n",
      "9000/9000 [==============================] - 0s 36us/step - loss: 0.8586 - acc: 0.7846 - val_loss: 0.9371 - val_acc: 0.7660\n",
      "Epoch 507/1000\n",
      "9000/9000 [==============================] - 0s 38us/step - loss: 0.8577 - acc: 0.7848 - val_loss: 0.9289 - val_acc: 0.7610\n",
      "Epoch 508/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8572 - acc: 0.7868 - val_loss: 0.9389 - val_acc: 0.7560\n",
      "Epoch 509/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8570 - acc: 0.7839 - val_loss: 0.9350 - val_acc: 0.7620\n",
      "Epoch 510/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.8558 - acc: 0.7837 - val_loss: 0.9357 - val_acc: 0.7620\n",
      "Epoch 511/1000\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 0.8568 - acc: 0.7848 - val_loss: 0.9482 - val_acc: 0.7570\n",
      "Epoch 512/1000\n",
      "9000/9000 [==============================] - 0s 36us/step - loss: 0.8579 - acc: 0.7863 - val_loss: 0.9378 - val_acc: 0.7580\n",
      "Epoch 513/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8559 - acc: 0.7858 - val_loss: 0.9283 - val_acc: 0.7710\n",
      "Epoch 514/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8552 - acc: 0.7860 - val_loss: 0.9263 - val_acc: 0.7700\n",
      "Epoch 515/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8544 - acc: 0.7850 - val_loss: 0.9556 - val_acc: 0.7540\n",
      "Epoch 516/1000\n",
      "9000/9000 [==============================] - 0s 41us/step - loss: 0.8564 - acc: 0.7862 - val_loss: 0.9354 - val_acc: 0.7620\n",
      "Epoch 517/1000\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 0.8561 - acc: 0.7872 - val_loss: 0.9272 - val_acc: 0.7620\n",
      "Epoch 518/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8540 - acc: 0.7868 - val_loss: 0.9262 - val_acc: 0.7620\n",
      "Epoch 519/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8553 - acc: 0.7869 - val_loss: 0.9580 - val_acc: 0.7470\n",
      "Epoch 520/1000\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.8560 - acc: 0.7857 - val_loss: 0.9503 - val_acc: 0.7660\n",
      "Epoch 521/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8540 - acc: 0.7860 - val_loss: 0.9303 - val_acc: 0.7640\n",
      "Epoch 522/1000\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.8536 - acc: 0.7849 - val_loss: 0.9415 - val_acc: 0.7540\n",
      "Epoch 523/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8556 - acc: 0.7859 - val_loss: 0.9283 - val_acc: 0.7610\n",
      "Epoch 524/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8540 - acc: 0.7883 - val_loss: 0.9297 - val_acc: 0.7720\n",
      "Epoch 525/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8532 - acc: 0.7876 - val_loss: 0.9257 - val_acc: 0.7720\n",
      "Epoch 526/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8534 - acc: 0.7858 - val_loss: 0.9239 - val_acc: 0.7640\n",
      "Epoch 527/1000\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.8526 - acc: 0.7910 - val_loss: 0.9295 - val_acc: 0.7690\n",
      "Epoch 528/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8528 - acc: 0.7866 - val_loss: 0.9320 - val_acc: 0.7610\n",
      "Epoch 529/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8530 - acc: 0.7851 - val_loss: 0.9409 - val_acc: 0.7530\n",
      "Epoch 530/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8526 - acc: 0.7862 - val_loss: 0.9442 - val_acc: 0.7580\n",
      "Epoch 531/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8523 - acc: 0.7847 - val_loss: 0.9255 - val_acc: 0.7690\n",
      "Epoch 532/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.8510 - acc: 0.7867 - val_loss: 0.9234 - val_acc: 0.7620\n",
      "Epoch 533/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8512 - acc: 0.7871 - val_loss: 0.9257 - val_acc: 0.7680\n",
      "Epoch 534/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8509 - acc: 0.7886 - val_loss: 0.9412 - val_acc: 0.7650\n",
      "Epoch 535/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8520 - acc: 0.7866 - val_loss: 0.9255 - val_acc: 0.7680\n",
      "Epoch 536/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8507 - acc: 0.7881 - val_loss: 0.9373 - val_acc: 0.7510\n",
      "Epoch 537/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8502 - acc: 0.7870 - val_loss: 0.9273 - val_acc: 0.7580\n",
      "Epoch 538/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8498 - acc: 0.7903 - val_loss: 0.9481 - val_acc: 0.7470\n",
      "Epoch 539/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8508 - acc: 0.7862 - val_loss: 0.9232 - val_acc: 0.7610\n",
      "Epoch 540/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8494 - acc: 0.7880 - val_loss: 0.9345 - val_acc: 0.7490\n",
      "Epoch 541/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8494 - acc: 0.7872 - val_loss: 0.9323 - val_acc: 0.7630\n",
      "Epoch 542/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8499 - acc: 0.7879 - val_loss: 0.9361 - val_acc: 0.7590\n",
      "Epoch 543/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8499 - acc: 0.7888 - val_loss: 0.9384 - val_acc: 0.7650\n",
      "Epoch 544/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8491 - acc: 0.7887 - val_loss: 0.9640 - val_acc: 0.7340\n",
      "Epoch 545/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8507 - acc: 0.7903 - val_loss: 0.9196 - val_acc: 0.7660\n",
      "Epoch 546/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8476 - acc: 0.7881 - val_loss: 0.9296 - val_acc: 0.7590\n",
      "Epoch 547/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8475 - acc: 0.7908 - val_loss: 0.9288 - val_acc: 0.7660\n",
      "Epoch 548/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8478 - acc: 0.7892 - val_loss: 0.9325 - val_acc: 0.7690\n",
      "Epoch 549/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8483 - acc: 0.7868 - val_loss: 0.9178 - val_acc: 0.7620\n",
      "Epoch 550/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8473 - acc: 0.7878 - val_loss: 0.9290 - val_acc: 0.7660\n",
      "Epoch 551/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8473 - acc: 0.7901 - val_loss: 0.9197 - val_acc: 0.7660\n",
      "Epoch 552/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8472 - acc: 0.7900 - val_loss: 0.9253 - val_acc: 0.7690\n",
      "Epoch 553/1000\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.8472 - acc: 0.7887 - val_loss: 0.9380 - val_acc: 0.7570\n",
      "Epoch 554/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8477 - acc: 0.7902 - val_loss: 0.9354 - val_acc: 0.7680\n",
      "Epoch 555/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8479 - acc: 0.7897 - val_loss: 0.9287 - val_acc: 0.7560\n",
      "Epoch 556/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8478 - acc: 0.7881 - val_loss: 0.9434 - val_acc: 0.7580\n",
      "Epoch 557/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.8466 - acc: 0.7902 - val_loss: 0.9387 - val_acc: 0.7490\n",
      "Epoch 558/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8462 - acc: 0.7894 - val_loss: 0.9269 - val_acc: 0.7630\n",
      "Epoch 559/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8458 - acc: 0.7888 - val_loss: 0.9244 - val_acc: 0.7680\n",
      "Epoch 560/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8447 - acc: 0.7909 - val_loss: 0.9318 - val_acc: 0.7570\n",
      "Epoch 561/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8460 - acc: 0.7897 - val_loss: 0.9209 - val_acc: 0.7530\n",
      "Epoch 562/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8446 - acc: 0.7896 - val_loss: 0.9357 - val_acc: 0.7580\n",
      "Epoch 563/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8450 - acc: 0.7908 - val_loss: 0.9404 - val_acc: 0.7650\n",
      "Epoch 564/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8449 - acc: 0.7890 - val_loss: 0.9283 - val_acc: 0.7540\n",
      "Epoch 565/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8439 - acc: 0.7891 - val_loss: 0.9307 - val_acc: 0.7570\n",
      "Epoch 566/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8449 - acc: 0.7890 - val_loss: 0.9464 - val_acc: 0.7630\n",
      "Epoch 567/1000\n",
      "9000/9000 [==============================] - 0s 49us/step - loss: 0.8438 - acc: 0.7902 - val_loss: 0.9286 - val_acc: 0.7630\n",
      "Epoch 568/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8431 - acc: 0.7907 - val_loss: 0.9290 - val_acc: 0.7630\n",
      "Epoch 569/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8441 - acc: 0.7896 - val_loss: 0.9575 - val_acc: 0.7470\n",
      "Epoch 570/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8443 - acc: 0.7886 - val_loss: 0.9290 - val_acc: 0.7660\n",
      "Epoch 571/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8428 - acc: 0.7904 - val_loss: 0.9161 - val_acc: 0.7680\n",
      "Epoch 572/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8421 - acc: 0.7912 - val_loss: 0.9239 - val_acc: 0.7780\n",
      "Epoch 573/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8421 - acc: 0.7919 - val_loss: 0.9269 - val_acc: 0.7660\n",
      "Epoch 574/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8414 - acc: 0.7900 - val_loss: 0.9260 - val_acc: 0.7700\n",
      "Epoch 575/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8417 - acc: 0.7880 - val_loss: 0.9178 - val_acc: 0.7710\n",
      "Epoch 576/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.8411 - acc: 0.7911 - val_loss: 0.9453 - val_acc: 0.7610\n",
      "Epoch 577/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8442 - acc: 0.7906 - val_loss: 0.9255 - val_acc: 0.7600\n",
      "Epoch 578/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.8427 - acc: 0.7901 - val_loss: 0.9203 - val_acc: 0.7610\n",
      "Epoch 579/1000\n",
      "9000/9000 [==============================] - 0s 44us/step - loss: 0.8402 - acc: 0.7922 - val_loss: 0.9218 - val_acc: 0.7650\n",
      "Epoch 580/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.8394 - acc: 0.7911 - val_loss: 0.9297 - val_acc: 0.7630\n",
      "Epoch 581/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8403 - acc: 0.7913 - val_loss: 0.9215 - val_acc: 0.7660\n",
      "Epoch 582/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8408 - acc: 0.7914 - val_loss: 0.9162 - val_acc: 0.7680\n",
      "Epoch 583/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.8399 - acc: 0.7918 - val_loss: 0.9377 - val_acc: 0.7610\n",
      "Epoch 584/1000\n",
      "9000/9000 [==============================] - 0s 44us/step - loss: 0.8414 - acc: 0.7890 - val_loss: 0.9407 - val_acc: 0.7660\n",
      "Epoch 585/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8426 - acc: 0.7888 - val_loss: 0.9374 - val_acc: 0.7520\n",
      "Epoch 586/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8414 - acc: 0.7920 - val_loss: 0.9365 - val_acc: 0.7550\n",
      "Epoch 587/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8408 - acc: 0.7916 - val_loss: 0.9317 - val_acc: 0.7420\n",
      "Epoch 588/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8386 - acc: 0.7948 - val_loss: 0.9333 - val_acc: 0.7720\n",
      "Epoch 589/1000\n",
      "9000/9000 [==============================] - 0s 40us/step - loss: 0.8407 - acc: 0.7920 - val_loss: 0.9313 - val_acc: 0.7700\n",
      "Epoch 590/1000\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.8408 - acc: 0.7907 - val_loss: 0.9344 - val_acc: 0.7570\n",
      "Epoch 591/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8390 - acc: 0.7929 - val_loss: 0.9654 - val_acc: 0.7320\n",
      "Epoch 592/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8412 - acc: 0.7897 - val_loss: 0.9324 - val_acc: 0.7640\n",
      "Epoch 593/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8392 - acc: 0.7928 - val_loss: 0.9239 - val_acc: 0.7620\n",
      "Epoch 594/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8382 - acc: 0.7933 - val_loss: 0.9308 - val_acc: 0.7690\n",
      "Epoch 595/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8386 - acc: 0.7903 - val_loss: 0.9334 - val_acc: 0.7650\n",
      "Epoch 596/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8389 - acc: 0.7898 - val_loss: 0.9265 - val_acc: 0.7600\n",
      "Epoch 597/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8377 - acc: 0.7908 - val_loss: 0.9303 - val_acc: 0.7540\n",
      "Epoch 598/1000\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.8399 - acc: 0.7904 - val_loss: 0.9238 - val_acc: 0.7730\n",
      "Epoch 599/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8395 - acc: 0.7886 - val_loss: 0.9274 - val_acc: 0.7690\n",
      "Epoch 600/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.8376 - acc: 0.7933 - val_loss: 0.9331 - val_acc: 0.7740\n",
      "Epoch 601/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8386 - acc: 0.7906 - val_loss: 0.9170 - val_acc: 0.7720\n",
      "Epoch 602/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8370 - acc: 0.7950 - val_loss: 0.9238 - val_acc: 0.7670\n",
      "Epoch 603/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8379 - acc: 0.7933 - val_loss: 0.9345 - val_acc: 0.7520\n",
      "Epoch 604/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8368 - acc: 0.7933 - val_loss: 0.9600 - val_acc: 0.7410\n",
      "Epoch 605/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.8396 - acc: 0.7918 - val_loss: 0.9198 - val_acc: 0.7710\n",
      "Epoch 606/1000\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.8353 - acc: 0.7910 - val_loss: 0.9182 - val_acc: 0.7650\n",
      "Epoch 607/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8360 - acc: 0.7946 - val_loss: 0.9525 - val_acc: 0.7410\n",
      "Epoch 608/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8357 - acc: 0.7910 - val_loss: 0.9462 - val_acc: 0.7530\n",
      "Epoch 609/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8381 - acc: 0.7931 - val_loss: 0.9320 - val_acc: 0.7710\n",
      "Epoch 610/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8368 - acc: 0.7909 - val_loss: 0.9304 - val_acc: 0.7700\n",
      "Epoch 611/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8359 - acc: 0.7912 - val_loss: 0.9259 - val_acc: 0.7620\n",
      "Epoch 612/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8348 - acc: 0.7934 - val_loss: 0.9327 - val_acc: 0.7650\n",
      "Epoch 613/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8370 - acc: 0.7902 - val_loss: 0.9319 - val_acc: 0.7710\n",
      "Epoch 614/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8344 - acc: 0.7923 - val_loss: 0.9226 - val_acc: 0.7640\n",
      "Epoch 615/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8348 - acc: 0.7948 - val_loss: 0.9541 - val_acc: 0.7530\n",
      "Epoch 616/1000\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.8357 - acc: 0.7939 - val_loss: 0.9154 - val_acc: 0.7680\n",
      "Epoch 617/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8333 - acc: 0.7950 - val_loss: 0.9737 - val_acc: 0.7550\n",
      "Epoch 618/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8363 - acc: 0.7926 - val_loss: 1.0009 - val_acc: 0.7340\n",
      "Epoch 619/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8357 - acc: 0.7929 - val_loss: 0.9285 - val_acc: 0.7620\n",
      "Epoch 620/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8352 - acc: 0.7940 - val_loss: 0.9095 - val_acc: 0.7690\n",
      "Epoch 621/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.8317 - acc: 0.7963 - val_loss: 0.9209 - val_acc: 0.7700\n",
      "Epoch 622/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8327 - acc: 0.7956 - val_loss: 0.9290 - val_acc: 0.7730\n",
      "Epoch 623/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8338 - acc: 0.7959 - val_loss: 0.9136 - val_acc: 0.7690\n",
      "Epoch 624/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8317 - acc: 0.7931 - val_loss: 0.9154 - val_acc: 0.7590\n",
      "Epoch 625/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8330 - acc: 0.7944 - val_loss: 0.9204 - val_acc: 0.7700\n",
      "Epoch 626/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8335 - acc: 0.7934 - val_loss: 0.9434 - val_acc: 0.7580\n",
      "Epoch 627/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8339 - acc: 0.7947 - val_loss: 0.9365 - val_acc: 0.7610\n",
      "Epoch 628/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8309 - acc: 0.7940 - val_loss: 1.0294 - val_acc: 0.7300\n",
      "Epoch 629/1000\n",
      "9000/9000 [==============================] - 0s 26us/step - loss: 0.8378 - acc: 0.7906 - val_loss: 0.9286 - val_acc: 0.7630\n",
      "Epoch 630/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.8313 - acc: 0.7942 - val_loss: 0.9216 - val_acc: 0.7680\n",
      "Epoch 631/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8310 - acc: 0.7954 - val_loss: 0.9185 - val_acc: 0.7590\n",
      "Epoch 632/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8315 - acc: 0.7928 - val_loss: 0.9072 - val_acc: 0.7740\n",
      "Epoch 633/1000\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 0.8295 - acc: 0.7943 - val_loss: 0.9176 - val_acc: 0.7730\n",
      "Epoch 634/1000\n",
      "9000/9000 [==============================] - 0s 40us/step - loss: 0.8310 - acc: 0.7936 - val_loss: 0.9174 - val_acc: 0.7690\n",
      "Epoch 635/1000\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 0.8311 - acc: 0.7923 - val_loss: 0.9100 - val_acc: 0.7760\n",
      "Epoch 636/1000\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.8312 - acc: 0.7944 - val_loss: 0.9306 - val_acc: 0.7630\n",
      "Epoch 637/1000\n",
      "9000/9000 [==============================] - 0s 42us/step - loss: 0.8315 - acc: 0.7948 - val_loss: 0.9170 - val_acc: 0.7760\n",
      "Epoch 638/1000\n",
      "9000/9000 [==============================] - 0s 41us/step - loss: 0.8301 - acc: 0.7939 - val_loss: 0.9368 - val_acc: 0.7620\n",
      "Epoch 639/1000\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.8307 - acc: 0.7962 - val_loss: 0.9221 - val_acc: 0.7610\n",
      "Epoch 640/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8304 - acc: 0.7934 - val_loss: 0.9350 - val_acc: 0.7360\n",
      "Epoch 641/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8303 - acc: 0.7959 - val_loss: 0.9109 - val_acc: 0.7750\n",
      "Epoch 642/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8300 - acc: 0.7956 - val_loss: 0.9180 - val_acc: 0.7620\n",
      "Epoch 643/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8287 - acc: 0.7981 - val_loss: 0.9168 - val_acc: 0.7610\n",
      "Epoch 644/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.8287 - acc: 0.7959 - val_loss: 0.9066 - val_acc: 0.7700\n",
      "Epoch 645/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8282 - acc: 0.7969 - val_loss: 0.9279 - val_acc: 0.7550\n",
      "Epoch 646/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8290 - acc: 0.7959 - val_loss: 0.9410 - val_acc: 0.7620\n",
      "Epoch 647/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8316 - acc: 0.7944 - val_loss: 0.9187 - val_acc: 0.7530\n",
      "Epoch 648/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8300 - acc: 0.7949 - val_loss: 0.9486 - val_acc: 0.7530\n",
      "Epoch 649/1000\n",
      "9000/9000 [==============================] - 0s 42us/step - loss: 0.8314 - acc: 0.7928 - val_loss: 0.9123 - val_acc: 0.7760\n",
      "Epoch 650/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8278 - acc: 0.7960 - val_loss: 0.9173 - val_acc: 0.7680\n",
      "Epoch 651/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8290 - acc: 0.7933 - val_loss: 0.9230 - val_acc: 0.7660\n",
      "Epoch 652/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8276 - acc: 0.7947 - val_loss: 0.9224 - val_acc: 0.7710\n",
      "Epoch 653/1000\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.8282 - acc: 0.7938 - val_loss: 0.9096 - val_acc: 0.7590\n",
      "Epoch 654/1000\n",
      "9000/9000 [==============================] - 0s 40us/step - loss: 0.8262 - acc: 0.7973 - val_loss: 0.9506 - val_acc: 0.7440\n",
      "Epoch 655/1000\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.8311 - acc: 0.7969 - val_loss: 0.9164 - val_acc: 0.7680\n",
      "Epoch 656/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8264 - acc: 0.7956 - val_loss: 0.9284 - val_acc: 0.7650\n",
      "Epoch 657/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8279 - acc: 0.7956 - val_loss: 0.9579 - val_acc: 0.7550\n",
      "Epoch 658/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8294 - acc: 0.7963 - val_loss: 0.9463 - val_acc: 0.7560\n",
      "Epoch 659/1000\n",
      "9000/9000 [==============================] - 0s 45us/step - loss: 0.8264 - acc: 0.7949 - val_loss: 0.9459 - val_acc: 0.7670\n",
      "Epoch 660/1000\n",
      "9000/9000 [==============================] - 0s 35us/step - loss: 0.8288 - acc: 0.7949 - val_loss: 0.9206 - val_acc: 0.7620\n",
      "Epoch 661/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8260 - acc: 0.7970 - val_loss: 0.9175 - val_acc: 0.7600\n",
      "Epoch 662/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8258 - acc: 0.7976 - val_loss: 0.9251 - val_acc: 0.7710\n",
      "Epoch 663/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8265 - acc: 0.7958 - val_loss: 0.9189 - val_acc: 0.7760\n",
      "Epoch 664/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8255 - acc: 0.7961 - val_loss: 0.9338 - val_acc: 0.7490\n",
      "Epoch 665/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8284 - acc: 0.7957 - val_loss: 0.9188 - val_acc: 0.7740\n",
      "Epoch 666/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8266 - acc: 0.7961 - val_loss: 0.9267 - val_acc: 0.7510\n",
      "Epoch 667/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8249 - acc: 0.7971 - val_loss: 0.9171 - val_acc: 0.7690\n",
      "Epoch 668/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8262 - acc: 0.7950 - val_loss: 0.9146 - val_acc: 0.7640\n",
      "Epoch 669/1000\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.8246 - acc: 0.7954 - val_loss: 0.9730 - val_acc: 0.7430\n",
      "Epoch 670/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8251 - acc: 0.7969 - val_loss: 0.9719 - val_acc: 0.7180\n",
      "Epoch 671/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8264 - acc: 0.7969 - val_loss: 0.9206 - val_acc: 0.7510\n",
      "Epoch 672/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8257 - acc: 0.7940 - val_loss: 0.9132 - val_acc: 0.7640\n",
      "Epoch 673/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8257 - acc: 0.7968 - val_loss: 0.9439 - val_acc: 0.7550\n",
      "Epoch 674/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8252 - acc: 0.7967 - val_loss: 0.9807 - val_acc: 0.7430\n",
      "Epoch 675/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8269 - acc: 0.7960 - val_loss: 0.9289 - val_acc: 0.7650\n",
      "Epoch 676/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8248 - acc: 0.7986 - val_loss: 0.9069 - val_acc: 0.7760\n",
      "Epoch 677/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8232 - acc: 0.7942 - val_loss: 0.9170 - val_acc: 0.7760\n",
      "Epoch 678/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8224 - acc: 0.7991 - val_loss: 0.9196 - val_acc: 0.7530\n",
      "Epoch 679/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8239 - acc: 0.7980 - val_loss: 0.9213 - val_acc: 0.7530\n",
      "Epoch 680/1000\n",
      "9000/9000 [==============================] - 0s 35us/step - loss: 0.8245 - acc: 0.7977 - val_loss: 0.9291 - val_acc: 0.7520\n",
      "Epoch 681/1000\n",
      "9000/9000 [==============================] - 0s 35us/step - loss: 0.8262 - acc: 0.7980 - val_loss: 0.9134 - val_acc: 0.7720\n",
      "Epoch 682/1000\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 0.8249 - acc: 0.7968 - val_loss: 0.9130 - val_acc: 0.7530\n",
      "Epoch 683/1000\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.8234 - acc: 0.7993 - val_loss: 0.9189 - val_acc: 0.7550\n",
      "Epoch 684/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.8239 - acc: 0.7986 - val_loss: 0.9208 - val_acc: 0.7500\n",
      "Epoch 685/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.8229 - acc: 0.7989 - val_loss: 0.9305 - val_acc: 0.7630\n",
      "Epoch 686/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8239 - acc: 0.8002 - val_loss: 0.9203 - val_acc: 0.7520\n",
      "Epoch 687/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8226 - acc: 0.8010 - val_loss: 0.9324 - val_acc: 0.7520\n",
      "Epoch 688/1000\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.8240 - acc: 0.7960 - val_loss: 0.9147 - val_acc: 0.7700\n",
      "Epoch 689/1000\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.8229 - acc: 0.7986 - val_loss: 0.9345 - val_acc: 0.7580\n",
      "Epoch 690/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.8241 - acc: 0.7977 - val_loss: 0.9274 - val_acc: 0.7440\n",
      "Epoch 691/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.8233 - acc: 0.7980 - val_loss: 0.9215 - val_acc: 0.7710\n",
      "Epoch 692/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8224 - acc: 0.7983 - val_loss: 0.9214 - val_acc: 0.7660\n",
      "Epoch 693/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8226 - acc: 0.7988 - val_loss: 0.9357 - val_acc: 0.7570\n",
      "Epoch 694/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8206 - acc: 0.7983 - val_loss: 0.9028 - val_acc: 0.7720\n",
      "Epoch 695/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8204 - acc: 0.7974 - val_loss: 0.9493 - val_acc: 0.7520\n",
      "Epoch 696/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8241 - acc: 0.7976 - val_loss: 0.9288 - val_acc: 0.7610\n",
      "Epoch 697/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8218 - acc: 0.7971 - val_loss: 0.9182 - val_acc: 0.7700\n",
      "Epoch 698/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8219 - acc: 0.7947 - val_loss: 0.9585 - val_acc: 0.7520\n",
      "Epoch 699/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8226 - acc: 0.7980 - val_loss: 0.9153 - val_acc: 0.7760\n",
      "Epoch 700/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8196 - acc: 0.7987 - val_loss: 0.9232 - val_acc: 0.7700\n",
      "Epoch 701/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8215 - acc: 0.7992 - val_loss: 0.9182 - val_acc: 0.7660\n",
      "Epoch 702/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8221 - acc: 0.7987 - val_loss: 0.9397 - val_acc: 0.7540\n",
      "Epoch 703/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.8215 - acc: 0.7988 - val_loss: 0.9165 - val_acc: 0.7700\n",
      "Epoch 704/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8217 - acc: 0.7979 - val_loss: 0.9155 - val_acc: 0.7690\n",
      "Epoch 705/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.8195 - acc: 0.7981 - val_loss: 0.9038 - val_acc: 0.7780\n",
      "Epoch 706/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8217 - acc: 0.7971 - val_loss: 0.9234 - val_acc: 0.7600\n",
      "Epoch 707/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8211 - acc: 0.7962 - val_loss: 0.9061 - val_acc: 0.7780\n",
      "Epoch 708/1000\n",
      "9000/9000 [==============================] - 0s 48us/step - loss: 0.8185 - acc: 0.7982 - val_loss: 0.9109 - val_acc: 0.7730\n",
      "Epoch 709/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 0s 36us/step - loss: 0.8223 - acc: 0.7980 - val_loss: 0.9105 - val_acc: 0.7810\n",
      "Epoch 710/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8197 - acc: 0.7986 - val_loss: 0.9130 - val_acc: 0.7660\n",
      "Epoch 711/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8196 - acc: 0.7998 - val_loss: 0.9209 - val_acc: 0.7780\n",
      "Epoch 712/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8194 - acc: 0.8007 - val_loss: 0.8996 - val_acc: 0.7740\n",
      "Epoch 713/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8178 - acc: 0.7977 - val_loss: 0.9324 - val_acc: 0.7630\n",
      "Epoch 714/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8192 - acc: 0.8001 - val_loss: 0.9026 - val_acc: 0.7700\n",
      "Epoch 715/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8190 - acc: 0.7988 - val_loss: 0.9581 - val_acc: 0.7610\n",
      "Epoch 716/1000\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.8193 - acc: 0.7969 - val_loss: 0.9033 - val_acc: 0.7680\n",
      "Epoch 717/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8177 - acc: 0.7980 - val_loss: 0.9198 - val_acc: 0.7750\n",
      "Epoch 718/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8195 - acc: 0.7993 - val_loss: 0.9166 - val_acc: 0.7810\n",
      "Epoch 719/1000\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.8184 - acc: 0.8009 - val_loss: 0.9493 - val_acc: 0.7590\n",
      "Epoch 720/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8187 - acc: 0.8010 - val_loss: 0.9069 - val_acc: 0.7770\n",
      "Epoch 721/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8175 - acc: 0.8001 - val_loss: 0.9101 - val_acc: 0.7570\n",
      "Epoch 722/1000\n",
      "9000/9000 [==============================] - 0s 35us/step - loss: 0.8171 - acc: 0.8011 - val_loss: 0.9228 - val_acc: 0.7640\n",
      "Epoch 723/1000\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 0.8190 - acc: 0.8026 - val_loss: 0.9113 - val_acc: 0.7810\n",
      "Epoch 724/1000\n",
      "9000/9000 [==============================] - 0s 35us/step - loss: 0.8166 - acc: 0.8014 - val_loss: 0.9733 - val_acc: 0.7500\n",
      "Epoch 725/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8196 - acc: 0.7959 - val_loss: 0.9238 - val_acc: 0.7660\n",
      "Epoch 726/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8197 - acc: 0.7992 - val_loss: 0.9228 - val_acc: 0.7770\n",
      "Epoch 727/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.8179 - acc: 0.7994 - val_loss: 0.9028 - val_acc: 0.7750\n",
      "Epoch 728/1000\n",
      "9000/9000 [==============================] - 0s 42us/step - loss: 0.8171 - acc: 0.8002 - val_loss: 0.9018 - val_acc: 0.7650\n",
      "Epoch 729/1000\n",
      "9000/9000 [==============================] - 0s 36us/step - loss: 0.8165 - acc: 0.8028 - val_loss: 0.9343 - val_acc: 0.7650\n",
      "Epoch 730/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8197 - acc: 0.7981 - val_loss: 0.9163 - val_acc: 0.7760\n",
      "Epoch 731/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.8196 - acc: 0.8026 - val_loss: 0.9076 - val_acc: 0.7560\n",
      "Epoch 732/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.8171 - acc: 0.8006 - val_loss: 0.9080 - val_acc: 0.7580\n",
      "Epoch 733/1000\n",
      "9000/9000 [==============================] - 0s 43us/step - loss: 0.8172 - acc: 0.8037 - val_loss: 0.9226 - val_acc: 0.7550\n",
      "Epoch 734/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8149 - acc: 0.7994 - val_loss: 0.9009 - val_acc: 0.7890\n",
      "Epoch 735/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8159 - acc: 0.8009 - val_loss: 0.9425 - val_acc: 0.7660\n",
      "Epoch 736/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8183 - acc: 0.8007 - val_loss: 0.9124 - val_acc: 0.7760\n",
      "Epoch 737/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8154 - acc: 0.8022 - val_loss: 0.9059 - val_acc: 0.7740\n",
      "Epoch 738/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8150 - acc: 0.8022 - val_loss: 0.9335 - val_acc: 0.7460\n",
      "Epoch 739/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8159 - acc: 0.8029 - val_loss: 0.9280 - val_acc: 0.7690\n",
      "Epoch 740/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8152 - acc: 0.8022 - val_loss: 0.9399 - val_acc: 0.7440\n",
      "Epoch 741/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8160 - acc: 0.8021 - val_loss: 0.9236 - val_acc: 0.7750\n",
      "Epoch 742/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8147 - acc: 0.8010 - val_loss: 0.9105 - val_acc: 0.7730\n",
      "Epoch 743/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8141 - acc: 0.8004 - val_loss: 0.9153 - val_acc: 0.7740\n",
      "Epoch 744/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8131 - acc: 0.8023 - val_loss: 0.9310 - val_acc: 0.7610\n",
      "Epoch 745/1000\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.8154 - acc: 0.8006 - val_loss: 1.0350 - val_acc: 0.7160\n",
      "Epoch 746/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8196 - acc: 0.7994 - val_loss: 0.9285 - val_acc: 0.7620\n",
      "Epoch 747/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8137 - acc: 0.8027 - val_loss: 0.9135 - val_acc: 0.7740\n",
      "Epoch 748/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8143 - acc: 0.8024 - val_loss: 0.9204 - val_acc: 0.7580\n",
      "Epoch 749/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8137 - acc: 0.7993 - val_loss: 0.9223 - val_acc: 0.7530\n",
      "Epoch 750/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.8136 - acc: 0.8034 - val_loss: 0.9149 - val_acc: 0.7700\n",
      "Epoch 751/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8128 - acc: 0.7998 - val_loss: 0.9190 - val_acc: 0.7620\n",
      "Epoch 752/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8139 - acc: 0.8051 - val_loss: 0.9157 - val_acc: 0.7620\n",
      "Epoch 753/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8143 - acc: 0.7990 - val_loss: 0.9280 - val_acc: 0.7540\n",
      "Epoch 754/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8148 - acc: 0.8009 - val_loss: 0.9225 - val_acc: 0.7570\n",
      "Epoch 755/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8148 - acc: 0.8012 - val_loss: 0.9372 - val_acc: 0.7610\n",
      "Epoch 756/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8137 - acc: 0.8020 - val_loss: 0.9029 - val_acc: 0.7760\n",
      "Epoch 757/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8142 - acc: 0.8009 - val_loss: 0.9451 - val_acc: 0.7560\n",
      "Epoch 758/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8138 - acc: 0.8009 - val_loss: 0.9032 - val_acc: 0.7700\n",
      "Epoch 759/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8135 - acc: 0.8021 - val_loss: 0.9105 - val_acc: 0.7740\n",
      "Epoch 760/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8125 - acc: 0.8027 - val_loss: 0.9225 - val_acc: 0.7730\n",
      "Epoch 761/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8137 - acc: 0.8028 - val_loss: 0.9454 - val_acc: 0.7580\n",
      "Epoch 762/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8149 - acc: 0.8014 - val_loss: 0.9436 - val_acc: 0.7680\n",
      "Epoch 763/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8156 - acc: 0.8016 - val_loss: 0.9318 - val_acc: 0.7660\n",
      "Epoch 764/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8128 - acc: 0.8039 - val_loss: 0.9395 - val_acc: 0.7740\n",
      "Epoch 765/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8136 - acc: 0.8037 - val_loss: 0.9140 - val_acc: 0.7560\n",
      "Epoch 766/1000\n",
      "9000/9000 [==============================] - 0s 26us/step - loss: 0.8151 - acc: 0.8013 - val_loss: 0.8987 - val_acc: 0.7610\n",
      "Epoch 767/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8123 - acc: 0.8003 - val_loss: 0.9481 - val_acc: 0.7640\n",
      "Epoch 768/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8133 - acc: 0.8004 - val_loss: 0.8960 - val_acc: 0.7700\n",
      "Epoch 769/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.8128 - acc: 0.8038 - val_loss: 0.9203 - val_acc: 0.7640\n",
      "Epoch 770/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8143 - acc: 0.8021 - val_loss: 0.9281 - val_acc: 0.7620\n",
      "Epoch 771/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8133 - acc: 0.8012 - val_loss: 0.9002 - val_acc: 0.7790\n",
      "Epoch 772/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8111 - acc: 0.7980 - val_loss: 0.9050 - val_acc: 0.7680\n",
      "Epoch 773/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8109 - acc: 0.8032 - val_loss: 0.9209 - val_acc: 0.7770\n",
      "Epoch 774/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8126 - acc: 0.8016 - val_loss: 0.9242 - val_acc: 0.7650\n",
      "Epoch 775/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8118 - acc: 0.8013 - val_loss: 0.9174 - val_acc: 0.7740\n",
      "Epoch 776/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8111 - acc: 0.8031 - val_loss: 0.9136 - val_acc: 0.7670\n",
      "Epoch 777/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8127 - acc: 0.8032 - val_loss: 0.8930 - val_acc: 0.7740\n",
      "Epoch 778/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8111 - acc: 0.8030 - val_loss: 0.9190 - val_acc: 0.7500\n",
      "Epoch 779/1000\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.8107 - acc: 0.8033 - val_loss: 0.9153 - val_acc: 0.7720\n",
      "Epoch 780/1000\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.8109 - acc: 0.8034 - val_loss: 0.8998 - val_acc: 0.7760\n",
      "Epoch 781/1000\n",
      "9000/9000 [==============================] - 0s 41us/step - loss: 0.8104 - acc: 0.8049 - val_loss: 0.9278 - val_acc: 0.7760\n",
      "Epoch 782/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.8104 - acc: 0.8042 - val_loss: 0.9627 - val_acc: 0.7650\n",
      "Epoch 783/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8135 - acc: 0.8046 - val_loss: 0.9240 - val_acc: 0.7690\n",
      "Epoch 784/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8088 - acc: 0.8028 - val_loss: 0.9265 - val_acc: 0.7530\n",
      "Epoch 785/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.8133 - acc: 0.8012 - val_loss: 0.9236 - val_acc: 0.7690\n",
      "Epoch 786/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8110 - acc: 0.8029 - val_loss: 0.9230 - val_acc: 0.7700\n",
      "Epoch 787/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8086 - acc: 0.8047 - val_loss: 0.9329 - val_acc: 0.7480\n",
      "Epoch 788/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8083 - acc: 0.8021 - val_loss: 0.9079 - val_acc: 0.7780\n",
      "Epoch 789/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8093 - acc: 0.8066 - val_loss: 0.9368 - val_acc: 0.7350\n",
      "Epoch 790/1000\n",
      "9000/9000 [==============================] - ETA: 0s - loss: 0.8074 - acc: 0.804 - 0s 29us/step - loss: 0.8089 - acc: 0.8037 - val_loss: 0.9331 - val_acc: 0.7690\n",
      "Epoch 791/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8106 - acc: 0.8071 - val_loss: 0.9029 - val_acc: 0.7710\n",
      "Epoch 792/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.8079 - acc: 0.8029 - val_loss: 0.8946 - val_acc: 0.7790\n",
      "Epoch 793/1000\n",
      "9000/9000 [==============================] - 0s 44us/step - loss: 0.8083 - acc: 0.8057 - val_loss: 0.9002 - val_acc: 0.7850\n",
      "Epoch 794/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8070 - acc: 0.8037 - val_loss: 0.9237 - val_acc: 0.7600\n",
      "Epoch 795/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8110 - acc: 0.8061 - val_loss: 0.9182 - val_acc: 0.7720\n",
      "Epoch 796/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8082 - acc: 0.8028 - val_loss: 0.9786 - val_acc: 0.7500\n",
      "Epoch 797/1000\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 0.8099 - acc: 0.8050 - val_loss: 0.9074 - val_acc: 0.7700\n",
      "Epoch 798/1000\n",
      "9000/9000 [==============================] - 0s 41us/step - loss: 0.8110 - acc: 0.8021 - val_loss: 0.9135 - val_acc: 0.7790\n",
      "Epoch 799/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8080 - acc: 0.8038 - val_loss: 0.9143 - val_acc: 0.7710\n",
      "Epoch 800/1000\n",
      "9000/9000 [==============================] - 0s 36us/step - loss: 0.8075 - acc: 0.8028 - val_loss: 0.8972 - val_acc: 0.7760\n",
      "Epoch 801/1000\n",
      "9000/9000 [==============================] - 0s 35us/step - loss: 0.8086 - acc: 0.8043 - val_loss: 1.0123 - val_acc: 0.7390\n",
      "Epoch 802/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8126 - acc: 0.8017 - val_loss: 0.9311 - val_acc: 0.7540\n",
      "Epoch 803/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8083 - acc: 0.8072 - val_loss: 0.8986 - val_acc: 0.7740\n",
      "Epoch 804/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8053 - acc: 0.8042 - val_loss: 0.9057 - val_acc: 0.7650\n",
      "Epoch 805/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.8072 - acc: 0.8064 - val_loss: 0.9066 - val_acc: 0.7700\n",
      "Epoch 806/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8059 - acc: 0.8063 - val_loss: 0.9126 - val_acc: 0.7730\n",
      "Epoch 807/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8081 - acc: 0.8047 - val_loss: 0.8955 - val_acc: 0.7740\n",
      "Epoch 808/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8053 - acc: 0.8042 - val_loss: 0.9010 - val_acc: 0.7730\n",
      "Epoch 809/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8051 - acc: 0.8059 - val_loss: 0.9366 - val_acc: 0.7570\n",
      "Epoch 810/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8095 - acc: 0.8021 - val_loss: 0.9487 - val_acc: 0.7460\n",
      "Epoch 811/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8088 - acc: 0.8028 - val_loss: 0.9127 - val_acc: 0.7600\n",
      "Epoch 812/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8088 - acc: 0.8063 - val_loss: 0.9313 - val_acc: 0.7530\n",
      "Epoch 813/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8055 - acc: 0.8087 - val_loss: 0.8966 - val_acc: 0.7640\n",
      "Epoch 814/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8049 - acc: 0.8070 - val_loss: 0.8993 - val_acc: 0.7750\n",
      "Epoch 815/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8070 - acc: 0.8048 - val_loss: 0.9039 - val_acc: 0.7850\n",
      "Epoch 816/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8063 - acc: 0.8026 - val_loss: 0.9128 - val_acc: 0.7780\n",
      "Epoch 817/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8079 - acc: 0.8027 - val_loss: 0.9039 - val_acc: 0.7590\n",
      "Epoch 818/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8040 - acc: 0.8053 - val_loss: 0.9115 - val_acc: 0.7690\n",
      "Epoch 819/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8057 - acc: 0.8091 - val_loss: 0.8972 - val_acc: 0.7770\n",
      "Epoch 820/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8076 - acc: 0.8032 - val_loss: 0.9952 - val_acc: 0.7140\n",
      "Epoch 821/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8114 - acc: 0.8018 - val_loss: 0.9412 - val_acc: 0.7660\n",
      "Epoch 822/1000\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.8064 - acc: 0.8067 - val_loss: 0.9910 - val_acc: 0.7360\n",
      "Epoch 823/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8100 - acc: 0.8054 - val_loss: 0.9305 - val_acc: 0.7520\n",
      "Epoch 824/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8067 - acc: 0.8041 - val_loss: 0.8994 - val_acc: 0.7870\n",
      "Epoch 825/1000\n",
      "9000/9000 [==============================] - 0s 27us/step - loss: 0.8042 - acc: 0.8039 - val_loss: 1.0025 - val_acc: 0.7280\n",
      "Epoch 826/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8072 - acc: 0.8054 - val_loss: 1.0100 - val_acc: 0.7130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 827/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8088 - acc: 0.8051 - val_loss: 0.9422 - val_acc: 0.7640\n",
      "Epoch 828/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8071 - acc: 0.8017 - val_loss: 0.9334 - val_acc: 0.7680\n",
      "Epoch 829/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.8057 - acc: 0.8071 - val_loss: 0.9219 - val_acc: 0.7550\n",
      "Epoch 830/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8046 - acc: 0.8066 - val_loss: 0.9035 - val_acc: 0.7860\n",
      "Epoch 831/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8079 - acc: 0.8046 - val_loss: 0.9136 - val_acc: 0.7630\n",
      "Epoch 832/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8062 - acc: 0.8029 - val_loss: 0.9024 - val_acc: 0.7750\n",
      "Epoch 833/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8051 - acc: 0.8047 - val_loss: 0.9094 - val_acc: 0.7780\n",
      "Epoch 834/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8042 - acc: 0.8056 - val_loss: 0.9252 - val_acc: 0.7690\n",
      "Epoch 835/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8074 - acc: 0.8047 - val_loss: 0.9170 - val_acc: 0.7710\n",
      "Epoch 836/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8028 - acc: 0.8093 - val_loss: 0.9034 - val_acc: 0.7860\n",
      "Epoch 837/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8051 - acc: 0.8038 - val_loss: 0.8974 - val_acc: 0.7740\n",
      "Epoch 838/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8038 - acc: 0.8072 - val_loss: 0.8926 - val_acc: 0.7760\n",
      "Epoch 839/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8020 - acc: 0.8052 - val_loss: 0.9064 - val_acc: 0.7640\n",
      "Epoch 840/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8043 - acc: 0.8073 - val_loss: 0.9063 - val_acc: 0.7670\n",
      "Epoch 841/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8021 - acc: 0.8089 - val_loss: 0.9003 - val_acc: 0.7880\n",
      "Epoch 842/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8025 - acc: 0.8057 - val_loss: 0.9100 - val_acc: 0.7650\n",
      "Epoch 843/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8007 - acc: 0.8071 - val_loss: 0.8970 - val_acc: 0.7720\n",
      "Epoch 844/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8053 - acc: 0.8013 - val_loss: 0.9078 - val_acc: 0.7710\n",
      "Epoch 845/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8035 - acc: 0.8099 - val_loss: 0.9296 - val_acc: 0.7520\n",
      "Epoch 846/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8018 - acc: 0.8086 - val_loss: 0.9093 - val_acc: 0.7750\n",
      "Epoch 847/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8007 - acc: 0.8061 - val_loss: 0.9333 - val_acc: 0.7430\n",
      "Epoch 848/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8035 - acc: 0.8071 - val_loss: 0.8986 - val_acc: 0.7820\n",
      "Epoch 849/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8028 - acc: 0.8083 - val_loss: 0.9354 - val_acc: 0.7480\n",
      "Epoch 850/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8043 - acc: 0.8056 - val_loss: 0.9072 - val_acc: 0.7630\n",
      "Epoch 851/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8040 - acc: 0.8048 - val_loss: 0.9282 - val_acc: 0.7560\n",
      "Epoch 852/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8056 - acc: 0.8072 - val_loss: 0.9149 - val_acc: 0.7690\n",
      "Epoch 853/1000\n",
      "9000/9000 [==============================] - 0s 41us/step - loss: 0.8032 - acc: 0.8073 - val_loss: 0.9035 - val_acc: 0.7740\n",
      "Epoch 854/1000\n",
      "9000/9000 [==============================] - 0s 39us/step - loss: 0.8003 - acc: 0.8069 - val_loss: 0.9202 - val_acc: 0.7730\n",
      "Epoch 855/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8022 - acc: 0.8073 - val_loss: 0.9078 - val_acc: 0.7700\n",
      "Epoch 856/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8011 - acc: 0.8062 - val_loss: 0.9187 - val_acc: 0.7700\n",
      "Epoch 857/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8015 - acc: 0.8073 - val_loss: 0.9146 - val_acc: 0.7640\n",
      "Epoch 858/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8021 - acc: 0.8098 - val_loss: 0.9125 - val_acc: 0.7750\n",
      "Epoch 859/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8028 - acc: 0.8087 - val_loss: 0.9358 - val_acc: 0.7600\n",
      "Epoch 860/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.8019 - acc: 0.8074 - val_loss: 0.9154 - val_acc: 0.7610\n",
      "Epoch 861/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8010 - acc: 0.8094 - val_loss: 0.9229 - val_acc: 0.7760\n",
      "Epoch 862/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8027 - acc: 0.8071 - val_loss: 0.9141 - val_acc: 0.7590\n",
      "Epoch 863/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8030 - acc: 0.8072 - val_loss: 0.9057 - val_acc: 0.7750\n",
      "Epoch 864/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8003 - acc: 0.8063 - val_loss: 0.8937 - val_acc: 0.7660\n",
      "Epoch 865/1000\n",
      "9000/9000 [==============================] - 0s 39us/step - loss: 0.8008 - acc: 0.8088 - val_loss: 0.9122 - val_acc: 0.7710\n",
      "Epoch 866/1000\n",
      "9000/9000 [==============================] - 0s 36us/step - loss: 0.8020 - acc: 0.8048 - val_loss: 0.9202 - val_acc: 0.7620\n",
      "Epoch 867/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8024 - acc: 0.8088 - val_loss: 0.9303 - val_acc: 0.7570\n",
      "Epoch 868/1000\n",
      "9000/9000 [==============================] - 0s 40us/step - loss: 0.8024 - acc: 0.8067 - val_loss: 0.8955 - val_acc: 0.7860\n",
      "Epoch 869/1000\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 0.8009 - acc: 0.8067 - val_loss: 0.9318 - val_acc: 0.7360\n",
      "Epoch 870/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8027 - acc: 0.8060 - val_loss: 0.8988 - val_acc: 0.7800\n",
      "Epoch 871/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.8003 - acc: 0.8053 - val_loss: 0.9391 - val_acc: 0.7530\n",
      "Epoch 872/1000\n",
      "9000/9000 [==============================] - 0s 40us/step - loss: 0.8012 - acc: 0.8083 - val_loss: 0.8927 - val_acc: 0.7760\n",
      "Epoch 873/1000\n",
      "9000/9000 [==============================] - 0s 46us/step - loss: 0.7998 - acc: 0.8081 - val_loss: 0.9629 - val_acc: 0.7450\n",
      "Epoch 874/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.8050 - acc: 0.8069 - val_loss: 0.9009 - val_acc: 0.7640\n",
      "Epoch 875/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.7979 - acc: 0.8111 - val_loss: 0.9466 - val_acc: 0.7360\n",
      "Epoch 876/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.7998 - acc: 0.8072 - val_loss: 0.8963 - val_acc: 0.7840\n",
      "Epoch 877/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.7988 - acc: 0.8062 - val_loss: 0.9328 - val_acc: 0.7390\n",
      "Epoch 878/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8021 - acc: 0.8068 - val_loss: 0.9034 - val_acc: 0.7750\n",
      "Epoch 879/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.7988 - acc: 0.8120 - val_loss: 0.9476 - val_acc: 0.7290\n",
      "Epoch 880/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.8009 - acc: 0.8078 - val_loss: 0.9795 - val_acc: 0.7570\n",
      "Epoch 881/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.8053 - acc: 0.8071 - val_loss: 0.8954 - val_acc: 0.7860\n",
      "Epoch 882/1000\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.7972 - acc: 0.8111 - val_loss: 0.8951 - val_acc: 0.7850\n",
      "Epoch 883/1000\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 0.7959 - acc: 0.8103 - val_loss: 0.9067 - val_acc: 0.7720\n",
      "Epoch 884/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.7990 - acc: 0.8046 - val_loss: 0.9272 - val_acc: 0.7600\n",
      "Epoch 885/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8010 - acc: 0.8078 - val_loss: 0.9128 - val_acc: 0.7840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 886/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.7976 - acc: 0.8107 - val_loss: 0.9209 - val_acc: 0.7670\n",
      "Epoch 887/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.7983 - acc: 0.8111 - val_loss: 0.9730 - val_acc: 0.7450\n",
      "Epoch 888/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8027 - acc: 0.8066 - val_loss: 0.9140 - val_acc: 0.7820\n",
      "Epoch 889/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.7980 - acc: 0.8076 - val_loss: 0.8944 - val_acc: 0.7700\n",
      "Epoch 890/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.7967 - acc: 0.8097 - val_loss: 0.9554 - val_acc: 0.7530\n",
      "Epoch 891/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8000 - acc: 0.8086 - val_loss: 0.9075 - val_acc: 0.7630\n",
      "Epoch 892/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.7987 - acc: 0.8104 - val_loss: 0.9867 - val_acc: 0.7460\n",
      "Epoch 893/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.8019 - acc: 0.8101 - val_loss: 0.9315 - val_acc: 0.7430\n",
      "Epoch 894/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.7977 - acc: 0.8101 - val_loss: 0.8964 - val_acc: 0.7680\n",
      "Epoch 895/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8004 - acc: 0.8060 - val_loss: 0.9470 - val_acc: 0.7680\n",
      "Epoch 896/1000\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.8016 - acc: 0.8084 - val_loss: 0.9094 - val_acc: 0.7800\n",
      "Epoch 897/1000\n",
      "9000/9000 [==============================] - 0s 36us/step - loss: 0.7979 - acc: 0.8076 - val_loss: 0.8978 - val_acc: 0.7880\n",
      "Epoch 898/1000\n",
      "9000/9000 [==============================] - 0s 35us/step - loss: 0.7957 - acc: 0.8098 - val_loss: 0.9274 - val_acc: 0.7680\n",
      "Epoch 899/1000\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.7987 - acc: 0.8124 - val_loss: 0.9477 - val_acc: 0.7560\n",
      "Epoch 900/1000\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.7985 - acc: 0.8098 - val_loss: 0.8961 - val_acc: 0.7680\n",
      "Epoch 901/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.7984 - acc: 0.8097 - val_loss: 0.8907 - val_acc: 0.7760\n",
      "Epoch 902/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.7934 - acc: 0.8100 - val_loss: 1.0148 - val_acc: 0.7260\n",
      "Epoch 903/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.8005 - acc: 0.8049 - val_loss: 0.9300 - val_acc: 0.7680\n",
      "Epoch 904/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.7986 - acc: 0.8080 - val_loss: 0.9047 - val_acc: 0.7720\n",
      "Epoch 905/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.7986 - acc: 0.8088 - val_loss: 0.8899 - val_acc: 0.7740\n",
      "Epoch 906/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.7953 - acc: 0.8122 - val_loss: 0.9054 - val_acc: 0.7770\n",
      "Epoch 907/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.7956 - acc: 0.8081 - val_loss: 0.9092 - val_acc: 0.7790\n",
      "Epoch 908/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.7982 - acc: 0.8102 - val_loss: 0.9802 - val_acc: 0.7250\n",
      "Epoch 909/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.7994 - acc: 0.8091 - val_loss: 0.9290 - val_acc: 0.7670\n",
      "Epoch 910/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.7966 - acc: 0.8139 - val_loss: 0.9105 - val_acc: 0.7640\n",
      "Epoch 911/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.7981 - acc: 0.8101 - val_loss: 0.9362 - val_acc: 0.7540\n",
      "Epoch 912/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.7998 - acc: 0.8079 - val_loss: 0.8924 - val_acc: 0.7720\n",
      "Epoch 913/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.7965 - acc: 0.8106 - val_loss: 0.9102 - val_acc: 0.7710\n",
      "Epoch 914/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.7964 - acc: 0.8104 - val_loss: 0.9257 - val_acc: 0.7740\n",
      "Epoch 915/1000\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.7985 - acc: 0.8086 - val_loss: 0.9572 - val_acc: 0.7280\n",
      "Epoch 916/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.7967 - acc: 0.8104 - val_loss: 0.9209 - val_acc: 0.7710\n",
      "Epoch 917/1000\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.7955 - acc: 0.8119 - val_loss: 0.9618 - val_acc: 0.7340\n",
      "Epoch 918/1000\n",
      "9000/9000 [==============================] - 0s 40us/step - loss: 0.7996 - acc: 0.8101 - val_loss: 0.9817 - val_acc: 0.7210\n",
      "Epoch 919/1000\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 0.7969 - acc: 0.8089 - val_loss: 0.9329 - val_acc: 0.7680\n",
      "Epoch 920/1000\n",
      "9000/9000 [==============================] - 0s 49us/step - loss: 0.7974 - acc: 0.8139 - val_loss: 0.9417 - val_acc: 0.7490\n",
      "Epoch 921/1000\n",
      "9000/9000 [==============================] - 0s 54us/step - loss: 0.7944 - acc: 0.8142 - val_loss: 0.8984 - val_acc: 0.7770\n",
      "Epoch 922/1000\n",
      "9000/9000 [==============================] - 0s 43us/step - loss: 0.7973 - acc: 0.8114 - val_loss: 0.9096 - val_acc: 0.7710\n",
      "Epoch 923/1000\n",
      "9000/9000 [==============================] - 0s 36us/step - loss: 0.7951 - acc: 0.8131 - val_loss: 0.9028 - val_acc: 0.7670\n",
      "Epoch 924/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.7954 - acc: 0.8101 - val_loss: 0.8904 - val_acc: 0.7820\n",
      "Epoch 925/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.7943 - acc: 0.8108 - val_loss: 0.9794 - val_acc: 0.7150\n",
      "Epoch 926/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.7988 - acc: 0.8077 - val_loss: 0.9540 - val_acc: 0.7660\n",
      "Epoch 927/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.7994 - acc: 0.8067 - val_loss: 0.9253 - val_acc: 0.7580\n",
      "Epoch 928/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.8008 - acc: 0.8062 - val_loss: 0.8928 - val_acc: 0.7890\n",
      "Epoch 929/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.7944 - acc: 0.8091 - val_loss: 0.8949 - val_acc: 0.7730\n",
      "Epoch 930/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.7929 - acc: 0.8122 - val_loss: 0.9084 - val_acc: 0.7740\n",
      "Epoch 931/1000\n",
      "9000/9000 [==============================] - 0s 38us/step - loss: 0.7922 - acc: 0.8117 - val_loss: 0.8904 - val_acc: 0.7690\n",
      "Epoch 932/1000\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 0.7941 - acc: 0.8124 - val_loss: 0.9969 - val_acc: 0.7450\n",
      "Epoch 933/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.7940 - acc: 0.8129 - val_loss: 0.9068 - val_acc: 0.7750\n",
      "Epoch 934/1000\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.7991 - acc: 0.8103 - val_loss: 0.9014 - val_acc: 0.7710\n",
      "Epoch 935/1000\n",
      "9000/9000 [==============================] - 0s 44us/step - loss: 0.7977 - acc: 0.8076 - val_loss: 0.9344 - val_acc: 0.7610\n",
      "Epoch 936/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.7932 - acc: 0.8127 - val_loss: 0.9632 - val_acc: 0.7470\n",
      "Epoch 937/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.7947 - acc: 0.8099 - val_loss: 0.8922 - val_acc: 0.7900\n",
      "Epoch 938/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.7961 - acc: 0.8084 - val_loss: 0.9115 - val_acc: 0.7790\n",
      "Epoch 939/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.7949 - acc: 0.8126 - val_loss: 0.8947 - val_acc: 0.7640\n",
      "Epoch 940/1000\n",
      "9000/9000 [==============================] - 0s 44us/step - loss: 0.7925 - acc: 0.8112 - val_loss: 0.9715 - val_acc: 0.7530\n",
      "Epoch 941/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.7952 - acc: 0.8098 - val_loss: 0.9184 - val_acc: 0.7830\n",
      "Epoch 942/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.7968 - acc: 0.8108 - val_loss: 0.9154 - val_acc: 0.7740\n",
      "Epoch 943/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.7918 - acc: 0.8133 - val_loss: 0.9116 - val_acc: 0.7590\n",
      "Epoch 944/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.7936 - acc: 0.8106 - val_loss: 0.9063 - val_acc: 0.7680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 945/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.7952 - acc: 0.8128 - val_loss: 0.9316 - val_acc: 0.7430\n",
      "Epoch 946/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.7936 - acc: 0.8122 - val_loss: 0.9401 - val_acc: 0.7660\n",
      "Epoch 947/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.7945 - acc: 0.8110 - val_loss: 0.9155 - val_acc: 0.7640\n",
      "Epoch 948/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.7902 - acc: 0.8118 - val_loss: 0.9159 - val_acc: 0.7730\n",
      "Epoch 949/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.7973 - acc: 0.8086 - val_loss: 1.0131 - val_acc: 0.7320\n",
      "Epoch 950/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.7975 - acc: 0.8094 - val_loss: 0.9065 - val_acc: 0.7710\n",
      "Epoch 951/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.7927 - acc: 0.8113 - val_loss: 0.9563 - val_acc: 0.7480\n",
      "Epoch 952/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.7958 - acc: 0.8093 - val_loss: 0.9351 - val_acc: 0.7420\n",
      "Epoch 953/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.7922 - acc: 0.8122 - val_loss: 0.9322 - val_acc: 0.7640\n",
      "Epoch 954/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.7910 - acc: 0.8114 - val_loss: 0.9096 - val_acc: 0.7750\n",
      "Epoch 955/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.7900 - acc: 0.8158 - val_loss: 0.9252 - val_acc: 0.7460\n",
      "Epoch 956/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.7936 - acc: 0.8117 - val_loss: 0.8892 - val_acc: 0.7920\n",
      "Epoch 957/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.7890 - acc: 0.8122 - val_loss: 0.8966 - val_acc: 0.7800\n",
      "Epoch 958/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.7948 - acc: 0.8124 - val_loss: 0.9303 - val_acc: 0.7620\n",
      "Epoch 959/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.7977 - acc: 0.8102 - val_loss: 0.9142 - val_acc: 0.7690\n",
      "Epoch 960/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.7967 - acc: 0.8090 - val_loss: 0.9092 - val_acc: 0.7690\n",
      "Epoch 961/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.7963 - acc: 0.8089 - val_loss: 0.9388 - val_acc: 0.7520\n",
      "Epoch 962/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.7938 - acc: 0.8078 - val_loss: 0.8972 - val_acc: 0.7720\n",
      "Epoch 963/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.7939 - acc: 0.8112 - val_loss: 0.9057 - val_acc: 0.7650\n",
      "Epoch 964/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.7944 - acc: 0.8104 - val_loss: 0.9192 - val_acc: 0.7680\n",
      "Epoch 965/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.7934 - acc: 0.8091 - val_loss: 0.8996 - val_acc: 0.7800\n",
      "Epoch 966/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.7926 - acc: 0.8124 - val_loss: 0.9146 - val_acc: 0.7690\n",
      "Epoch 967/1000\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 0.7981 - acc: 0.8139 - val_loss: 0.9213 - val_acc: 0.7590\n",
      "Epoch 968/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.7980 - acc: 0.8123 - val_loss: 0.9125 - val_acc: 0.7670\n",
      "Epoch 969/1000\n",
      "9000/9000 [==============================] - 0s 36us/step - loss: 0.7917 - acc: 0.8136 - val_loss: 1.1813 - val_acc: 0.6850\n",
      "Epoch 970/1000\n",
      "9000/9000 [==============================] - 0s 41us/step - loss: 0.8024 - acc: 0.8148 - val_loss: 0.9155 - val_acc: 0.7710\n",
      "Epoch 971/1000\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 0.7886 - acc: 0.8152 - val_loss: 1.0329 - val_acc: 0.7240\n",
      "Epoch 972/1000\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 0.7960 - acc: 0.8111 - val_loss: 0.9376 - val_acc: 0.7660\n",
      "Epoch 973/1000\n",
      "9000/9000 [==============================] - 0s 41us/step - loss: 0.7924 - acc: 0.8103 - val_loss: 0.9278 - val_acc: 0.7510\n",
      "Epoch 974/1000\n",
      "9000/9000 [==============================] - 0s 38us/step - loss: 0.7952 - acc: 0.8106 - val_loss: 0.8986 - val_acc: 0.7680\n",
      "Epoch 975/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.7892 - acc: 0.8151 - val_loss: 1.0196 - val_acc: 0.7260\n",
      "Epoch 976/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.7965 - acc: 0.8090 - val_loss: 1.1329 - val_acc: 0.6830\n",
      "Epoch 977/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.8034 - acc: 0.8067 - val_loss: 0.9162 - val_acc: 0.7640\n",
      "Epoch 978/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.7903 - acc: 0.8139 - val_loss: 1.0046 - val_acc: 0.7200\n",
      "Epoch 979/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.7973 - acc: 0.8107 - val_loss: 0.9155 - val_acc: 0.7710\n",
      "Epoch 980/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.7918 - acc: 0.8149 - val_loss: 0.8951 - val_acc: 0.7700\n",
      "Epoch 981/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.7913 - acc: 0.8150 - val_loss: 0.9517 - val_acc: 0.7500\n",
      "Epoch 982/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.7878 - acc: 0.8150 - val_loss: 0.8872 - val_acc: 0.7770\n",
      "Epoch 983/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.7904 - acc: 0.8142 - val_loss: 0.9215 - val_acc: 0.7670\n",
      "Epoch 984/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.7956 - acc: 0.8093 - val_loss: 0.9154 - val_acc: 0.7790\n",
      "Epoch 985/1000\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.7965 - acc: 0.8101 - val_loss: 0.9022 - val_acc: 0.7740\n",
      "Epoch 986/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.7891 - acc: 0.8149 - val_loss: 1.0316 - val_acc: 0.7320\n",
      "Epoch 987/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.7910 - acc: 0.8150 - val_loss: 0.9304 - val_acc: 0.7610\n",
      "Epoch 988/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.7899 - acc: 0.8114 - val_loss: 0.9263 - val_acc: 0.7580\n",
      "Epoch 989/1000\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.7936 - acc: 0.8103 - val_loss: 0.9489 - val_acc: 0.7360\n",
      "Epoch 990/1000\n",
      "9000/9000 [==============================] - 0s 39us/step - loss: 0.7879 - acc: 0.8148 - val_loss: 0.9130 - val_acc: 0.7700\n",
      "Epoch 991/1000\n",
      "9000/9000 [==============================] - 0s 46us/step - loss: 0.7904 - acc: 0.8150 - val_loss: 0.9420 - val_acc: 0.7620\n",
      "Epoch 992/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.7878 - acc: 0.8146 - val_loss: 0.9433 - val_acc: 0.7590\n",
      "Epoch 993/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.7881 - acc: 0.8162 - val_loss: 0.9652 - val_acc: 0.7460\n",
      "Epoch 994/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.7884 - acc: 0.8126 - val_loss: 0.9433 - val_acc: 0.7530\n",
      "Epoch 995/1000\n",
      "9000/9000 [==============================] - 0s 28us/step - loss: 0.7903 - acc: 0.8148 - val_loss: 0.9046 - val_acc: 0.7780\n",
      "Epoch 996/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.7884 - acc: 0.8177 - val_loss: 0.9155 - val_acc: 0.7640\n",
      "Epoch 997/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.7914 - acc: 0.8121 - val_loss: 0.9127 - val_acc: 0.7680\n",
      "Epoch 998/1000\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.7905 - acc: 0.8168 - val_loss: 0.9035 - val_acc: 0.7780\n",
      "Epoch 999/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.7891 - acc: 0.8120 - val_loss: 0.9127 - val_acc: 0.7790\n",
      "Epoch 1000/1000\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.7892 - acc: 0.8177 - val_loss: 0.8799 - val_acc: 0.7790\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=1000,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g,', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 0s 50us/step\n",
      "59998/59998 [==============================] - 3s 45us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.772322868347168, 0.8232222222222222]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8648217439655463, 0.783909463652762]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best result you've achieved so far, but you were training for quite a while! Next, experiment with dropout regularization to see if it offers any advantages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "9000/9000 [==============================] - 1s 80us/step - loss: 1.9786 - acc: 0.1590 - val_loss: 1.9371 - val_acc: 0.1770\n",
      "Epoch 2/200\n",
      "9000/9000 [==============================] - 0s 50us/step - loss: 1.9503 - acc: 0.1690 - val_loss: 1.9236 - val_acc: 0.2010\n",
      "Epoch 3/200\n",
      "9000/9000 [==============================] - 0s 52us/step - loss: 1.9337 - acc: 0.1836 - val_loss: 1.9151 - val_acc: 0.2110\n",
      "Epoch 4/200\n",
      "9000/9000 [==============================] - 0s 49us/step - loss: 1.9246 - acc: 0.1960 - val_loss: 1.9088 - val_acc: 0.2340\n",
      "Epoch 5/200\n",
      "9000/9000 [==============================] - 0s 50us/step - loss: 1.9143 - acc: 0.1978 - val_loss: 1.9003 - val_acc: 0.2400\n",
      "Epoch 6/200\n",
      "9000/9000 [==============================] - 1s 68us/step - loss: 1.9050 - acc: 0.2084 - val_loss: 1.8904 - val_acc: 0.2520\n",
      "Epoch 7/200\n",
      "9000/9000 [==============================] - 0s 53us/step - loss: 1.8992 - acc: 0.2102 - val_loss: 1.8797 - val_acc: 0.2620\n",
      "Epoch 8/200\n",
      "9000/9000 [==============================] - 0s 50us/step - loss: 1.8873 - acc: 0.2207 - val_loss: 1.8678 - val_acc: 0.2740\n",
      "Epoch 9/200\n",
      "9000/9000 [==============================] - 0s 48us/step - loss: 1.8741 - acc: 0.2299 - val_loss: 1.8523 - val_acc: 0.2850\n",
      "Epoch 10/200\n",
      "9000/9000 [==============================] - 0s 50us/step - loss: 1.8630 - acc: 0.2366 - val_loss: 1.8359 - val_acc: 0.3040\n",
      "Epoch 11/200\n",
      "9000/9000 [==============================] - 0s 47us/step - loss: 1.8508 - acc: 0.2464 - val_loss: 1.8155 - val_acc: 0.3130\n",
      "Epoch 12/200\n",
      "9000/9000 [==============================] - 1s 63us/step - loss: 1.8405 - acc: 0.2508 - val_loss: 1.7945 - val_acc: 0.3230\n",
      "Epoch 13/200\n",
      "9000/9000 [==============================] - 1s 57us/step - loss: 1.8205 - acc: 0.2648 - val_loss: 1.7714 - val_acc: 0.3480\n",
      "Epoch 14/200\n",
      "9000/9000 [==============================] - 1s 60us/step - loss: 1.8006 - acc: 0.2799 - val_loss: 1.7433 - val_acc: 0.3630\n",
      "Epoch 15/200\n",
      "9000/9000 [==============================] - 1s 60us/step - loss: 1.7814 - acc: 0.2928 - val_loss: 1.7186 - val_acc: 0.3810\n",
      "Epoch 16/200\n",
      "9000/9000 [==============================] - 0s 53us/step - loss: 1.7680 - acc: 0.2961 - val_loss: 1.6888 - val_acc: 0.3970\n",
      "Epoch 17/200\n",
      "9000/9000 [==============================] - 0s 48us/step - loss: 1.7363 - acc: 0.3176 - val_loss: 1.6583 - val_acc: 0.4130\n",
      "Epoch 18/200\n",
      "9000/9000 [==============================] - 0s 48us/step - loss: 1.7272 - acc: 0.3211 - val_loss: 1.6288 - val_acc: 0.4350\n",
      "Epoch 19/200\n",
      "9000/9000 [==============================] - 0s 51us/step - loss: 1.7019 - acc: 0.3362 - val_loss: 1.5947 - val_acc: 0.4530\n",
      "Epoch 20/200\n",
      "9000/9000 [==============================] - 0s 47us/step - loss: 1.6804 - acc: 0.3519 - val_loss: 1.5666 - val_acc: 0.4720\n",
      "Epoch 21/200\n",
      "9000/9000 [==============================] - 0s 50us/step - loss: 1.6556 - acc: 0.3580 - val_loss: 1.5340 - val_acc: 0.5040\n",
      "Epoch 22/200\n",
      "9000/9000 [==============================] - 0s 47us/step - loss: 1.6286 - acc: 0.3702 - val_loss: 1.5021 - val_acc: 0.5160\n",
      "Epoch 23/200\n",
      "9000/9000 [==============================] - 0s 48us/step - loss: 1.6097 - acc: 0.3818 - val_loss: 1.4720 - val_acc: 0.5310\n",
      "Epoch 24/200\n",
      "9000/9000 [==============================] - 0s 49us/step - loss: 1.5790 - acc: 0.3959 - val_loss: 1.4383 - val_acc: 0.5580\n",
      "Epoch 25/200\n",
      "9000/9000 [==============================] - 0s 50us/step - loss: 1.5643 - acc: 0.4090 - val_loss: 1.4129 - val_acc: 0.5860\n",
      "Epoch 26/200\n",
      "9000/9000 [==============================] - 0s 51us/step - loss: 1.5404 - acc: 0.4168 - val_loss: 1.3824 - val_acc: 0.5930\n",
      "Epoch 27/200\n",
      "9000/9000 [==============================] - 0s 49us/step - loss: 1.5199 - acc: 0.4227 - val_loss: 1.3577 - val_acc: 0.6050\n",
      "Epoch 28/200\n",
      "9000/9000 [==============================] - 0s 52us/step - loss: 1.4895 - acc: 0.4366 - val_loss: 1.3230 - val_acc: 0.6250\n",
      "Epoch 29/200\n",
      "9000/9000 [==============================] - 0s 48us/step - loss: 1.4755 - acc: 0.4422 - val_loss: 1.2954 - val_acc: 0.6280\n",
      "Epoch 30/200\n",
      "9000/9000 [==============================] - 0s 53us/step - loss: 1.4510 - acc: 0.4537 - val_loss: 1.2675 - val_acc: 0.6380\n",
      "Epoch 31/200\n",
      "9000/9000 [==============================] - 0s 48us/step - loss: 1.4418 - acc: 0.4598 - val_loss: 1.2492 - val_acc: 0.6370\n",
      "Epoch 32/200\n",
      "9000/9000 [==============================] - 0s 51us/step - loss: 1.4123 - acc: 0.4690 - val_loss: 1.2204 - val_acc: 0.6460\n",
      "Epoch 33/200\n",
      "9000/9000 [==============================] - 0s 48us/step - loss: 1.3944 - acc: 0.4720 - val_loss: 1.1967 - val_acc: 0.6530\n",
      "Epoch 34/200\n",
      "9000/9000 [==============================] - 0s 52us/step - loss: 1.3782 - acc: 0.4853 - val_loss: 1.1736 - val_acc: 0.6540\n",
      "Epoch 35/200\n",
      "9000/9000 [==============================] - 0s 48us/step - loss: 1.3599 - acc: 0.4912 - val_loss: 1.1527 - val_acc: 0.6580\n",
      "Epoch 36/200\n",
      "9000/9000 [==============================] - 0s 52us/step - loss: 1.3328 - acc: 0.5027 - val_loss: 1.1282 - val_acc: 0.6700\n",
      "Epoch 37/200\n",
      "9000/9000 [==============================] - 0s 49us/step - loss: 1.3315 - acc: 0.5070 - val_loss: 1.1119 - val_acc: 0.6680\n",
      "Epoch 38/200\n",
      "9000/9000 [==============================] - 0s 51us/step - loss: 1.3059 - acc: 0.5157 - val_loss: 1.0910 - val_acc: 0.6680\n",
      "Epoch 39/200\n",
      "9000/9000 [==============================] - 0s 48us/step - loss: 1.2864 - acc: 0.5204 - val_loss: 1.0693 - val_acc: 0.6900\n",
      "Epoch 40/200\n",
      "9000/9000 [==============================] - 0s 49us/step - loss: 1.2788 - acc: 0.5197 - val_loss: 1.0518 - val_acc: 0.6900\n",
      "Epoch 41/200\n",
      "9000/9000 [==============================] - 0s 48us/step - loss: 1.2655 - acc: 0.5301 - val_loss: 1.0365 - val_acc: 0.6920\n",
      "Epoch 42/200\n",
      "9000/9000 [==============================] - 0s 48us/step - loss: 1.2407 - acc: 0.5349 - val_loss: 1.0236 - val_acc: 0.6880\n",
      "Epoch 43/200\n",
      "9000/9000 [==============================] - 0s 49us/step - loss: 1.2345 - acc: 0.5366 - val_loss: 1.0058 - val_acc: 0.7030\n",
      "Epoch 44/200\n",
      "9000/9000 [==============================] - 0s 49us/step - loss: 1.2220 - acc: 0.5398 - val_loss: 0.9929 - val_acc: 0.7020\n",
      "Epoch 45/200\n",
      "9000/9000 [==============================] - 0s 51us/step - loss: 1.2081 - acc: 0.5481 - val_loss: 0.9789 - val_acc: 0.7010\n",
      "Epoch 46/200\n",
      "9000/9000 [==============================] - 0s 49us/step - loss: 1.1882 - acc: 0.5631 - val_loss: 0.9649 - val_acc: 0.7020\n",
      "Epoch 47/200\n",
      "9000/9000 [==============================] - 0s 51us/step - loss: 1.1698 - acc: 0.5623 - val_loss: 0.9475 - val_acc: 0.7130\n",
      "Epoch 48/200\n",
      "9000/9000 [==============================] - 0s 48us/step - loss: 1.1751 - acc: 0.5621 - val_loss: 0.9391 - val_acc: 0.7120\n",
      "Epoch 49/200\n",
      "9000/9000 [==============================] - 0s 46us/step - loss: 1.1620 - acc: 0.5681 - val_loss: 0.9271 - val_acc: 0.7180\n",
      "Epoch 50/200\n",
      "9000/9000 [==============================] - 0s 52us/step - loss: 1.1533 - acc: 0.5732 - val_loss: 0.9164 - val_acc: 0.7200\n",
      "Epoch 51/200\n",
      "9000/9000 [==============================] - 0s 54us/step - loss: 1.1400 - acc: 0.5810 - val_loss: 0.9079 - val_acc: 0.7180\n",
      "Epoch 52/200\n",
      "9000/9000 [==============================] - 1s 68us/step - loss: 1.1314 - acc: 0.5774 - val_loss: 0.8971 - val_acc: 0.7240\n",
      "Epoch 53/200\n",
      "9000/9000 [==============================] - 0s 53us/step - loss: 1.1156 - acc: 0.5831 - val_loss: 0.8863 - val_acc: 0.7230\n",
      "Epoch 54/200\n",
      "9000/9000 [==============================] - 0s 49us/step - loss: 1.1072 - acc: 0.5827 - val_loss: 0.8826 - val_acc: 0.7260\n",
      "Epoch 55/200\n",
      "9000/9000 [==============================] - 0s 51us/step - loss: 1.1048 - acc: 0.5892 - val_loss: 0.8694 - val_acc: 0.7280\n",
      "Epoch 56/200\n",
      "9000/9000 [==============================] - 0s 48us/step - loss: 1.0933 - acc: 0.5949 - val_loss: 0.8600 - val_acc: 0.7350\n",
      "Epoch 57/200\n",
      "9000/9000 [==============================] - 0s 52us/step - loss: 1.0661 - acc: 0.6004 - val_loss: 0.8536 - val_acc: 0.7300\n",
      "Epoch 58/200\n",
      "9000/9000 [==============================] - 1s 65us/step - loss: 1.0772 - acc: 0.5986 - val_loss: 0.8452 - val_acc: 0.7300\n",
      "Epoch 59/200\n",
      "9000/9000 [==============================] - 0s 54us/step - loss: 1.0727 - acc: 0.6046 - val_loss: 0.8408 - val_acc: 0.7260\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 1s 63us/step - loss: 1.0646 - acc: 0.6033 - val_loss: 0.8317 - val_acc: 0.7340\n",
      "Epoch 61/200\n",
      "9000/9000 [==============================] - 1s 60us/step - loss: 1.0499 - acc: 0.6071 - val_loss: 0.8262 - val_acc: 0.7310\n",
      "Epoch 62/200\n",
      "9000/9000 [==============================] - 1s 57us/step - loss: 1.0441 - acc: 0.6078 - val_loss: 0.8236 - val_acc: 0.7300\n",
      "Epoch 63/200\n",
      "9000/9000 [==============================] - 0s 49us/step - loss: 1.0351 - acc: 0.6137 - val_loss: 0.8150 - val_acc: 0.7330\n",
      "Epoch 64/200\n",
      "9000/9000 [==============================] - 0s 51us/step - loss: 1.0373 - acc: 0.6093 - val_loss: 0.8082 - val_acc: 0.7340\n",
      "Epoch 65/200\n",
      "9000/9000 [==============================] - 0s 47us/step - loss: 1.0338 - acc: 0.6212 - val_loss: 0.8031 - val_acc: 0.7350\n",
      "Epoch 66/200\n",
      "9000/9000 [==============================] - 0s 50us/step - loss: 1.0212 - acc: 0.6184 - val_loss: 0.7967 - val_acc: 0.7380\n",
      "Epoch 67/200\n",
      "9000/9000 [==============================] - 0s 50us/step - loss: 1.0212 - acc: 0.6181 - val_loss: 0.7929 - val_acc: 0.7420\n",
      "Epoch 68/200\n",
      "9000/9000 [==============================] - 0s 50us/step - loss: 1.0033 - acc: 0.6308 - val_loss: 0.7909 - val_acc: 0.7390\n",
      "Epoch 69/200\n",
      "9000/9000 [==============================] - 0s 49us/step - loss: 1.0081 - acc: 0.6237 - val_loss: 0.7836 - val_acc: 0.7400\n",
      "Epoch 70/200\n",
      "9000/9000 [==============================] - 0s 46us/step - loss: 0.9992 - acc: 0.6309 - val_loss: 0.7797 - val_acc: 0.7380\n",
      "Epoch 71/200\n",
      "9000/9000 [==============================] - 0s 50us/step - loss: 0.9864 - acc: 0.6333 - val_loss: 0.7724 - val_acc: 0.7400\n",
      "Epoch 72/200\n",
      "9000/9000 [==============================] - 0s 49us/step - loss: 0.9976 - acc: 0.6252 - val_loss: 0.7715 - val_acc: 0.7370\n",
      "Epoch 73/200\n",
      "9000/9000 [==============================] - 0s 52us/step - loss: 0.9867 - acc: 0.6369 - val_loss: 0.7646 - val_acc: 0.7450\n",
      "Epoch 74/200\n",
      "9000/9000 [==============================] - 0s 49us/step - loss: 0.9691 - acc: 0.6368 - val_loss: 0.7592 - val_acc: 0.7440\n",
      "Epoch 75/200\n",
      "9000/9000 [==============================] - 0s 51us/step - loss: 0.9648 - acc: 0.6423 - val_loss: 0.7575 - val_acc: 0.7420\n",
      "Epoch 76/200\n",
      "9000/9000 [==============================] - 0s 47us/step - loss: 0.9593 - acc: 0.6438 - val_loss: 0.7516 - val_acc: 0.7420\n",
      "Epoch 77/200\n",
      "9000/9000 [==============================] - 0s 53us/step - loss: 0.9615 - acc: 0.6410 - val_loss: 0.7480 - val_acc: 0.7430\n",
      "Epoch 78/200\n",
      "9000/9000 [==============================] - 0s 53us/step - loss: 0.9474 - acc: 0.6447 - val_loss: 0.7458 - val_acc: 0.7430\n",
      "Epoch 79/200\n",
      "9000/9000 [==============================] - 0s 50us/step - loss: 0.9578 - acc: 0.6447 - val_loss: 0.7455 - val_acc: 0.7430\n",
      "Epoch 80/200\n",
      "9000/9000 [==============================] - 0s 53us/step - loss: 0.9606 - acc: 0.6379 - val_loss: 0.7420 - val_acc: 0.7500\n",
      "Epoch 81/200\n",
      "9000/9000 [==============================] - 0s 47us/step - loss: 0.9307 - acc: 0.6502 - val_loss: 0.7346 - val_acc: 0.7520\n",
      "Epoch 82/200\n",
      "9000/9000 [==============================] - 0s 53us/step - loss: 0.9398 - acc: 0.6476 - val_loss: 0.7335 - val_acc: 0.7450\n",
      "Epoch 83/200\n",
      "9000/9000 [==============================] - 0s 46us/step - loss: 0.9271 - acc: 0.6509 - val_loss: 0.7288 - val_acc: 0.7470\n",
      "Epoch 84/200\n",
      "9000/9000 [==============================] - 0s 52us/step - loss: 0.9337 - acc: 0.6501 - val_loss: 0.7260 - val_acc: 0.7430\n",
      "Epoch 85/200\n",
      "9000/9000 [==============================] - 0s 49us/step - loss: 0.9308 - acc: 0.6497 - val_loss: 0.7264 - val_acc: 0.7440\n",
      "Epoch 86/200\n",
      "9000/9000 [==============================] - 0s 52us/step - loss: 0.9319 - acc: 0.6531 - val_loss: 0.7219 - val_acc: 0.7500\n",
      "Epoch 87/200\n",
      "9000/9000 [==============================] - 0s 48us/step - loss: 0.9265 - acc: 0.6591 - val_loss: 0.7204 - val_acc: 0.7450\n",
      "Epoch 88/200\n",
      "9000/9000 [==============================] - 0s 52us/step - loss: 0.9212 - acc: 0.6601 - val_loss: 0.7181 - val_acc: 0.7460\n",
      "Epoch 89/200\n",
      "9000/9000 [==============================] - 0s 51us/step - loss: 0.9154 - acc: 0.6526 - val_loss: 0.7149 - val_acc: 0.7430\n",
      "Epoch 90/200\n",
      "9000/9000 [==============================] - 0s 49us/step - loss: 0.9043 - acc: 0.6617 - val_loss: 0.7076 - val_acc: 0.7560\n",
      "Epoch 91/200\n",
      "9000/9000 [==============================] - 0s 51us/step - loss: 0.9024 - acc: 0.6557 - val_loss: 0.7097 - val_acc: 0.7400\n",
      "Epoch 92/200\n",
      "9000/9000 [==============================] - 0s 50us/step - loss: 0.9122 - acc: 0.6557 - val_loss: 0.7096 - val_acc: 0.7440\n",
      "Epoch 93/200\n",
      "9000/9000 [==============================] - 0s 51us/step - loss: 0.8883 - acc: 0.6716 - val_loss: 0.7026 - val_acc: 0.7540\n",
      "Epoch 94/200\n",
      "9000/9000 [==============================] - 0s 48us/step - loss: 0.8901 - acc: 0.6669 - val_loss: 0.7028 - val_acc: 0.7530\n",
      "Epoch 95/200\n",
      "9000/9000 [==============================] - 0s 47us/step - loss: 0.8955 - acc: 0.6656 - val_loss: 0.6997 - val_acc: 0.7530\n",
      "Epoch 96/200\n",
      "9000/9000 [==============================] - 1s 72us/step - loss: 0.8857 - acc: 0.6640 - val_loss: 0.6973 - val_acc: 0.7540\n",
      "Epoch 97/200\n",
      "9000/9000 [==============================] - 0s 52us/step - loss: 0.8819 - acc: 0.6690 - val_loss: 0.6940 - val_acc: 0.7590\n",
      "Epoch 98/200\n",
      "9000/9000 [==============================] - 0s 52us/step - loss: 0.8869 - acc: 0.6646 - val_loss: 0.6955 - val_acc: 0.7550\n",
      "Epoch 99/200\n",
      "9000/9000 [==============================] - 0s 52us/step - loss: 0.8792 - acc: 0.6691 - val_loss: 0.6913 - val_acc: 0.7610\n",
      "Epoch 100/200\n",
      "9000/9000 [==============================] - 0s 48us/step - loss: 0.8816 - acc: 0.6743 - val_loss: 0.6910 - val_acc: 0.7590\n",
      "Epoch 101/200\n",
      "9000/9000 [==============================] - 0s 52us/step - loss: 0.8645 - acc: 0.6757 - val_loss: 0.6867 - val_acc: 0.7530\n",
      "Epoch 102/200\n",
      "9000/9000 [==============================] - 1s 58us/step - loss: 0.8641 - acc: 0.6746 - val_loss: 0.6861 - val_acc: 0.7540\n",
      "Epoch 103/200\n",
      "9000/9000 [==============================] - 0s 55us/step - loss: 0.8594 - acc: 0.6796 - val_loss: 0.6829 - val_acc: 0.7470\n",
      "Epoch 104/200\n",
      "9000/9000 [==============================] - 1s 66us/step - loss: 0.8645 - acc: 0.6749 - val_loss: 0.6834 - val_acc: 0.7500\n",
      "Epoch 105/200\n",
      "9000/9000 [==============================] - 0s 54us/step - loss: 0.8730 - acc: 0.6759 - val_loss: 0.6863 - val_acc: 0.7440\n",
      "Epoch 106/200\n",
      "9000/9000 [==============================] - 1s 59us/step - loss: 0.8696 - acc: 0.6739 - val_loss: 0.6803 - val_acc: 0.7640\n",
      "Epoch 107/200\n",
      "9000/9000 [==============================] - 0s 45us/step - loss: 0.8543 - acc: 0.6784 - val_loss: 0.6765 - val_acc: 0.7560\n",
      "Epoch 108/200\n",
      "9000/9000 [==============================] - 0s 51us/step - loss: 0.8558 - acc: 0.6764 - val_loss: 0.6733 - val_acc: 0.7590\n",
      "Epoch 109/200\n",
      "9000/9000 [==============================] - 0s 47us/step - loss: 0.8385 - acc: 0.6824 - val_loss: 0.6732 - val_acc: 0.7560\n",
      "Epoch 110/200\n",
      "9000/9000 [==============================] - 0s 51us/step - loss: 0.8543 - acc: 0.6767 - val_loss: 0.6725 - val_acc: 0.7620\n",
      "Epoch 111/200\n",
      "9000/9000 [==============================] - 0s 49us/step - loss: 0.8385 - acc: 0.6906 - val_loss: 0.6709 - val_acc: 0.7560\n",
      "Epoch 112/200\n",
      "9000/9000 [==============================] - 0s 50us/step - loss: 0.8390 - acc: 0.6846 - val_loss: 0.6701 - val_acc: 0.7600\n",
      "Epoch 113/200\n",
      "9000/9000 [==============================] - 0s 49us/step - loss: 0.8384 - acc: 0.6850 - val_loss: 0.6661 - val_acc: 0.7630\n",
      "Epoch 114/200\n",
      "9000/9000 [==============================] - 0s 51us/step - loss: 0.8376 - acc: 0.6836 - val_loss: 0.6648 - val_acc: 0.7720\n",
      "Epoch 115/200\n",
      "9000/9000 [==============================] - 0s 48us/step - loss: 0.8336 - acc: 0.6833 - val_loss: 0.6662 - val_acc: 0.7560\n",
      "Epoch 116/200\n",
      "9000/9000 [==============================] - 0s 52us/step - loss: 0.8300 - acc: 0.6929 - val_loss: 0.6637 - val_acc: 0.7600\n",
      "Epoch 117/200\n",
      "9000/9000 [==============================] - 0s 54us/step - loss: 0.8330 - acc: 0.6881 - val_loss: 0.6602 - val_acc: 0.7600\n",
      "Epoch 118/200\n",
      "9000/9000 [==============================] - 0s 50us/step - loss: 0.8195 - acc: 0.6902 - val_loss: 0.6577 - val_acc: 0.7580\n",
      "Epoch 119/200\n",
      "9000/9000 [==============================] - 1s 56us/step - loss: 0.8203 - acc: 0.6972 - val_loss: 0.6572 - val_acc: 0.7660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "9000/9000 [==============================] - 0s 52us/step - loss: 0.8253 - acc: 0.6888 - val_loss: 0.6539 - val_acc: 0.7650\n",
      "Epoch 121/200\n",
      "9000/9000 [==============================] - 0s 53us/step - loss: 0.8137 - acc: 0.6900 - val_loss: 0.6549 - val_acc: 0.7580\n",
      "Epoch 122/200\n",
      "9000/9000 [==============================] - 0s 51us/step - loss: 0.8193 - acc: 0.6913 - val_loss: 0.6520 - val_acc: 0.7560\n",
      "Epoch 123/200\n",
      "9000/9000 [==============================] - 0s 55us/step - loss: 0.8277 - acc: 0.6858 - val_loss: 0.6539 - val_acc: 0.7600\n",
      "Epoch 124/200\n",
      "9000/9000 [==============================] - 0s 52us/step - loss: 0.8214 - acc: 0.6939 - val_loss: 0.6553 - val_acc: 0.7590\n",
      "Epoch 125/200\n",
      "9000/9000 [==============================] - 0s 53us/step - loss: 0.8056 - acc: 0.7011 - val_loss: 0.6503 - val_acc: 0.7620\n",
      "Epoch 126/200\n",
      "9000/9000 [==============================] - 0s 48us/step - loss: 0.7915 - acc: 0.6960 - val_loss: 0.6481 - val_acc: 0.7640\n",
      "Epoch 127/200\n",
      "9000/9000 [==============================] - 0s 51us/step - loss: 0.8058 - acc: 0.6962 - val_loss: 0.6462 - val_acc: 0.7670\n",
      "Epoch 128/200\n",
      "9000/9000 [==============================] - 0s 51us/step - loss: 0.8039 - acc: 0.6922 - val_loss: 0.6478 - val_acc: 0.7630\n",
      "Epoch 129/200\n",
      "9000/9000 [==============================] - 0s 52us/step - loss: 0.7986 - acc: 0.6987 - val_loss: 0.6470 - val_acc: 0.7710\n",
      "Epoch 130/200\n",
      "9000/9000 [==============================] - 0s 48us/step - loss: 0.7939 - acc: 0.6994 - val_loss: 0.6437 - val_acc: 0.7720\n",
      "Epoch 131/200\n",
      "9000/9000 [==============================] - 0s 52us/step - loss: 0.8107 - acc: 0.6979 - val_loss: 0.6453 - val_acc: 0.7670\n",
      "Epoch 132/200\n",
      "9000/9000 [==============================] - 0s 49us/step - loss: 0.7886 - acc: 0.7011 - val_loss: 0.6446 - val_acc: 0.7740\n",
      "Epoch 133/200\n",
      "9000/9000 [==============================] - 0s 50us/step - loss: 0.7932 - acc: 0.7014 - val_loss: 0.6417 - val_acc: 0.7730\n",
      "Epoch 134/200\n",
      "9000/9000 [==============================] - 0s 47us/step - loss: 0.7974 - acc: 0.7003 - val_loss: 0.6392 - val_acc: 0.7670\n",
      "Epoch 135/200\n",
      "9000/9000 [==============================] - 1s 63us/step - loss: 0.7808 - acc: 0.7084 - val_loss: 0.6364 - val_acc: 0.7640\n",
      "Epoch 136/200\n",
      "9000/9000 [==============================] - 1s 60us/step - loss: 0.7914 - acc: 0.7011 - val_loss: 0.6359 - val_acc: 0.7660\n",
      "Epoch 137/200\n",
      "9000/9000 [==============================] - 0s 51us/step - loss: 0.7901 - acc: 0.7041 - val_loss: 0.6353 - val_acc: 0.7690\n",
      "Epoch 138/200\n",
      "9000/9000 [==============================] - 0s 49us/step - loss: 0.7818 - acc: 0.7067 - val_loss: 0.6340 - val_acc: 0.7720\n",
      "Epoch 139/200\n",
      "9000/9000 [==============================] - 0s 46us/step - loss: 0.7853 - acc: 0.7030 - val_loss: 0.6343 - val_acc: 0.7630\n",
      "Epoch 140/200\n",
      "9000/9000 [==============================] - 0s 49us/step - loss: 0.7976 - acc: 0.7023 - val_loss: 0.6351 - val_acc: 0.7740\n",
      "Epoch 141/200\n",
      "9000/9000 [==============================] - 0s 48us/step - loss: 0.7636 - acc: 0.7114 - val_loss: 0.6305 - val_acc: 0.7730\n",
      "Epoch 142/200\n",
      "9000/9000 [==============================] - 1s 66us/step - loss: 0.7668 - acc: 0.7109 - val_loss: 0.6302 - val_acc: 0.7760\n",
      "Epoch 143/200\n",
      "9000/9000 [==============================] - 0s 55us/step - loss: 0.7790 - acc: 0.7058 - val_loss: 0.6308 - val_acc: 0.7760\n",
      "Epoch 144/200\n",
      "9000/9000 [==============================] - 1s 65us/step - loss: 0.7692 - acc: 0.7118 - val_loss: 0.6304 - val_acc: 0.7720\n",
      "Epoch 145/200\n",
      "9000/9000 [==============================] - 0s 54us/step - loss: 0.7748 - acc: 0.7070 - val_loss: 0.6287 - val_acc: 0.7770\n",
      "Epoch 146/200\n",
      "9000/9000 [==============================] - 1s 58us/step - loss: 0.7626 - acc: 0.7104 - val_loss: 0.6285 - val_acc: 0.7720\n",
      "Epoch 147/200\n",
      "9000/9000 [==============================] - 0s 48us/step - loss: 0.7599 - acc: 0.7130 - val_loss: 0.6248 - val_acc: 0.7720\n",
      "Epoch 148/200\n",
      "9000/9000 [==============================] - 0s 50us/step - loss: 0.7728 - acc: 0.7071 - val_loss: 0.6233 - val_acc: 0.7730\n",
      "Epoch 149/200\n",
      "9000/9000 [==============================] - 0s 48us/step - loss: 0.7550 - acc: 0.7151 - val_loss: 0.6243 - val_acc: 0.7640\n",
      "Epoch 150/200\n",
      "9000/9000 [==============================] - 0s 52us/step - loss: 0.7715 - acc: 0.7088 - val_loss: 0.6236 - val_acc: 0.7610\n",
      "Epoch 151/200\n",
      "9000/9000 [==============================] - 0s 53us/step - loss: 0.7434 - acc: 0.7160 - val_loss: 0.6217 - val_acc: 0.7740\n",
      "Epoch 152/200\n",
      "9000/9000 [==============================] - 0s 50us/step - loss: 0.7662 - acc: 0.7112 - val_loss: 0.6226 - val_acc: 0.7750\n",
      "Epoch 153/200\n",
      "9000/9000 [==============================] - 0s 53us/step - loss: 0.7508 - acc: 0.7148 - val_loss: 0.6182 - val_acc: 0.7790\n",
      "Epoch 154/200\n",
      "9000/9000 [==============================] - 0s 49us/step - loss: 0.7553 - acc: 0.7084 - val_loss: 0.6188 - val_acc: 0.7750\n",
      "Epoch 155/200\n",
      "9000/9000 [==============================] - 0s 52us/step - loss: 0.7554 - acc: 0.7148 - val_loss: 0.6183 - val_acc: 0.7800\n",
      "Epoch 156/200\n",
      "9000/9000 [==============================] - 0s 50us/step - loss: 0.7560 - acc: 0.7193 - val_loss: 0.6202 - val_acc: 0.7780\n",
      "Epoch 157/200\n",
      "9000/9000 [==============================] - 0s 48us/step - loss: 0.7514 - acc: 0.7138 - val_loss: 0.6186 - val_acc: 0.7730\n",
      "Epoch 158/200\n",
      "9000/9000 [==============================] - 0s 46us/step - loss: 0.7482 - acc: 0.7117 - val_loss: 0.6198 - val_acc: 0.7740\n",
      "Epoch 159/200\n",
      "9000/9000 [==============================] - 0s 50us/step - loss: 0.7455 - acc: 0.7183 - val_loss: 0.6189 - val_acc: 0.7720\n",
      "Epoch 160/200\n",
      "9000/9000 [==============================] - 0s 48us/step - loss: 0.7566 - acc: 0.7130 - val_loss: 0.6173 - val_acc: 0.7760\n",
      "Epoch 161/200\n",
      "9000/9000 [==============================] - 0s 48us/step - loss: 0.7469 - acc: 0.7196 - val_loss: 0.6160 - val_acc: 0.7780\n",
      "Epoch 162/200\n",
      "9000/9000 [==============================] - 0s 51us/step - loss: 0.7514 - acc: 0.7194 - val_loss: 0.6145 - val_acc: 0.7660\n",
      "Epoch 163/200\n",
      "9000/9000 [==============================] - 0s 47us/step - loss: 0.7445 - acc: 0.7126 - val_loss: 0.6144 - val_acc: 0.7690\n",
      "Epoch 164/200\n",
      "9000/9000 [==============================] - 0s 53us/step - loss: 0.7506 - acc: 0.7161 - val_loss: 0.6156 - val_acc: 0.7760\n",
      "Epoch 165/200\n",
      "9000/9000 [==============================] - 0s 47us/step - loss: 0.7486 - acc: 0.7144 - val_loss: 0.6130 - val_acc: 0.7790\n",
      "Epoch 166/200\n",
      "9000/9000 [==============================] - 0s 51us/step - loss: 0.7424 - acc: 0.7148 - val_loss: 0.6091 - val_acc: 0.7840\n",
      "Epoch 167/200\n",
      "9000/9000 [==============================] - 0s 48us/step - loss: 0.7410 - acc: 0.7200 - val_loss: 0.6093 - val_acc: 0.7820\n",
      "Epoch 168/200\n",
      "9000/9000 [==============================] - 0s 48us/step - loss: 0.7287 - acc: 0.7164 - val_loss: 0.6096 - val_acc: 0.7690\n",
      "Epoch 169/200\n",
      "9000/9000 [==============================] - 0s 51us/step - loss: 0.7301 - acc: 0.7224 - val_loss: 0.6109 - val_acc: 0.7780\n",
      "Epoch 170/200\n",
      "9000/9000 [==============================] - 0s 49us/step - loss: 0.7409 - acc: 0.7219 - val_loss: 0.6065 - val_acc: 0.7770\n",
      "Epoch 171/200\n",
      "9000/9000 [==============================] - 0s 49us/step - loss: 0.7331 - acc: 0.7222 - val_loss: 0.6078 - val_acc: 0.7810\n",
      "Epoch 172/200\n",
      "9000/9000 [==============================] - 0s 48us/step - loss: 0.7288 - acc: 0.7190 - val_loss: 0.6056 - val_acc: 0.7740\n",
      "Epoch 173/200\n",
      "9000/9000 [==============================] - 0s 50us/step - loss: 0.7233 - acc: 0.7234 - val_loss: 0.6073 - val_acc: 0.7690\n",
      "Epoch 174/200\n",
      "9000/9000 [==============================] - 0s 49us/step - loss: 0.7215 - acc: 0.7276 - val_loss: 0.6038 - val_acc: 0.7770\n",
      "Epoch 175/200\n",
      "9000/9000 [==============================] - 0s 48us/step - loss: 0.7344 - acc: 0.7217 - val_loss: 0.6062 - val_acc: 0.7750\n",
      "Epoch 176/200\n",
      "9000/9000 [==============================] - 0s 52us/step - loss: 0.7281 - acc: 0.7174 - val_loss: 0.6063 - val_acc: 0.7780\n",
      "Epoch 177/200\n",
      "9000/9000 [==============================] - 0s 52us/step - loss: 0.7214 - acc: 0.7262 - val_loss: 0.6041 - val_acc: 0.7810\n",
      "Epoch 178/200\n",
      "9000/9000 [==============================] - 1s 58us/step - loss: 0.7135 - acc: 0.7306 - val_loss: 0.6027 - val_acc: 0.7790\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 1s 65us/step - loss: 0.7214 - acc: 0.7259 - val_loss: 0.6061 - val_acc: 0.7720\n",
      "Epoch 180/200\n",
      "9000/9000 [==============================] - 0s 49us/step - loss: 0.7159 - acc: 0.7256 - val_loss: 0.6042 - val_acc: 0.7790\n",
      "Epoch 181/200\n",
      "9000/9000 [==============================] - 0s 54us/step - loss: 0.7148 - acc: 0.7234 - val_loss: 0.6024 - val_acc: 0.7780\n",
      "Epoch 182/200\n",
      "9000/9000 [==============================] - 0s 51us/step - loss: 0.7155 - acc: 0.7272 - val_loss: 0.6034 - val_acc: 0.7760\n",
      "Epoch 183/200\n",
      "9000/9000 [==============================] - 0s 49us/step - loss: 0.7219 - acc: 0.7206 - val_loss: 0.6005 - val_acc: 0.7830\n",
      "Epoch 184/200\n",
      "9000/9000 [==============================] - 0s 54us/step - loss: 0.7103 - acc: 0.7302 - val_loss: 0.5979 - val_acc: 0.7870\n",
      "Epoch 185/200\n",
      "9000/9000 [==============================] - 1s 59us/step - loss: 0.7035 - acc: 0.7282 - val_loss: 0.6005 - val_acc: 0.7810\n",
      "Epoch 186/200\n",
      "9000/9000 [==============================] - 1s 62us/step - loss: 0.7023 - acc: 0.7320 - val_loss: 0.6030 - val_acc: 0.7740\n",
      "Epoch 187/200\n",
      "9000/9000 [==============================] - 0s 54us/step - loss: 0.7149 - acc: 0.7259 - val_loss: 0.6016 - val_acc: 0.7790\n",
      "Epoch 188/200\n",
      "9000/9000 [==============================] - 1s 62us/step - loss: 0.7003 - acc: 0.7260 - val_loss: 0.5990 - val_acc: 0.7800\n",
      "Epoch 189/200\n",
      "9000/9000 [==============================] - 0s 52us/step - loss: 0.6911 - acc: 0.7299 - val_loss: 0.5969 - val_acc: 0.7810\n",
      "Epoch 190/200\n",
      "9000/9000 [==============================] - 0s 49us/step - loss: 0.6981 - acc: 0.7260 - val_loss: 0.5974 - val_acc: 0.7840\n",
      "Epoch 191/200\n",
      "9000/9000 [==============================] - 0s 52us/step - loss: 0.6966 - acc: 0.7320 - val_loss: 0.5963 - val_acc: 0.7820\n",
      "Epoch 192/200\n",
      "9000/9000 [==============================] - 0s 52us/step - loss: 0.7058 - acc: 0.7354 - val_loss: 0.5981 - val_acc: 0.7720\n",
      "Epoch 193/200\n",
      "9000/9000 [==============================] - 0s 50us/step - loss: 0.6936 - acc: 0.7400 - val_loss: 0.5978 - val_acc: 0.7720\n",
      "Epoch 194/200\n",
      "9000/9000 [==============================] - 0s 52us/step - loss: 0.6856 - acc: 0.7378 - val_loss: 0.5940 - val_acc: 0.7790\n",
      "Epoch 195/200\n",
      "9000/9000 [==============================] - 0s 48us/step - loss: 0.6965 - acc: 0.7331 - val_loss: 0.5949 - val_acc: 0.7790\n",
      "Epoch 196/200\n",
      "9000/9000 [==============================] - 0s 50us/step - loss: 0.6967 - acc: 0.7351 - val_loss: 0.5973 - val_acc: 0.7750\n",
      "Epoch 197/200\n",
      "9000/9000 [==============================] - 0s 48us/step - loss: 0.6951 - acc: 0.7334 - val_loss: 0.5939 - val_acc: 0.7720\n",
      "Epoch 198/200\n",
      "9000/9000 [==============================] - 0s 47us/step - loss: 0.7123 - acc: 0.7238 - val_loss: 0.5942 - val_acc: 0.7770\n",
      "Epoch 199/200\n",
      "9000/9000 [==============================] - 0s 51us/step - loss: 0.6812 - acc: 0.7404 - val_loss: 0.5941 - val_acc: 0.7770\n",
      "Epoch 200/200\n",
      "9000/9000 [==============================] - 0s 47us/step - loss: 0.6951 - acc: 0.7301 - val_loss: 0.5919 - val_acc: 0.7770\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dropout(0.3, input_shape=(2000,)))\n",
    "model.add(layers.Dense(50, activation='relu')) #2 hidden layers\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dropout_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=200,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 1s 56us/step\n",
      "59998/59998 [==============================] - 3s 48us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.42711972114774915, 0.8368888888888889]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5695126592971876, 0.7808093603020759]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! the variance did become higher again compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, one of the solutions to high variance was just getting more data. You actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple your data set, and see what happens. Note that you are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "\n",
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 1.9174 - acc: 0.1980 - val_loss: 1.8795 - val_acc: 0.2573\n",
      "Epoch 2/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 1.8363 - acc: 0.2845 - val_loss: 1.7788 - val_acc: 0.3580\n",
      "Epoch 3/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 1.7026 - acc: 0.4082 - val_loss: 1.6033 - val_acc: 0.4733\n",
      "Epoch 4/120\n",
      "33000/33000 [==============================] - 1s 27us/step - loss: 1.5035 - acc: 0.5190 - val_loss: 1.3824 - val_acc: 0.5660\n",
      "Epoch 5/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 1.2945 - acc: 0.5973 - val_loss: 1.1860 - val_acc: 0.6413\n",
      "Epoch 6/120\n",
      "33000/33000 [==============================] - 1s 45us/step - loss: 1.1195 - acc: 0.6494 - val_loss: 1.0358 - val_acc: 0.6750\n",
      "Epoch 7/120\n",
      "33000/33000 [==============================] - 2s 55us/step - loss: 0.9885 - acc: 0.6785 - val_loss: 0.9264 - val_acc: 0.6970\n",
      "Epoch 8/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.8952 - acc: 0.7008 - val_loss: 0.8522 - val_acc: 0.7127\n",
      "Epoch 9/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.8284 - acc: 0.7164 - val_loss: 0.7965 - val_acc: 0.7293\n",
      "Epoch 10/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.7786 - acc: 0.7287 - val_loss: 0.7532 - val_acc: 0.7387\n",
      "Epoch 11/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.7402 - acc: 0.7387 - val_loss: 0.7227 - val_acc: 0.7457\n",
      "Epoch 12/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.7097 - acc: 0.7481 - val_loss: 0.6960 - val_acc: 0.7517\n",
      "Epoch 13/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.6843 - acc: 0.7553 - val_loss: 0.6771 - val_acc: 0.7583\n",
      "Epoch 14/120\n",
      "33000/33000 [==============================] - 1s 27us/step - loss: 0.6634 - acc: 0.7621 - val_loss: 0.6585 - val_acc: 0.7597\n",
      "Epoch 15/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.6456 - acc: 0.7668 - val_loss: 0.6436 - val_acc: 0.7637\n",
      "Epoch 16/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.6298 - acc: 0.7717 - val_loss: 0.6320 - val_acc: 0.7683\n",
      "Epoch 17/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.6163 - acc: 0.7768 - val_loss: 0.6213 - val_acc: 0.7710\n",
      "Epoch 18/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.6040 - acc: 0.7803 - val_loss: 0.6119 - val_acc: 0.7730\n",
      "Epoch 19/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.5928 - acc: 0.7838 - val_loss: 0.6030 - val_acc: 0.7747\n",
      "Epoch 20/120\n",
      "33000/33000 [==============================] - 1s 27us/step - loss: 0.5831 - acc: 0.7882 - val_loss: 0.5960 - val_acc: 0.7780\n",
      "Epoch 21/120\n",
      "33000/33000 [==============================] - 1s 27us/step - loss: 0.5737 - acc: 0.7911 - val_loss: 0.5902 - val_acc: 0.7797\n",
      "Epoch 22/120\n",
      "33000/33000 [==============================] - 1s 27us/step - loss: 0.5651 - acc: 0.7945 - val_loss: 0.5842 - val_acc: 0.7843\n",
      "Epoch 23/120\n",
      "33000/33000 [==============================] - 1s 27us/step - loss: 0.5575 - acc: 0.7977 - val_loss: 0.5787 - val_acc: 0.7843\n",
      "Epoch 24/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.5498 - acc: 0.7994 - val_loss: 0.5736 - val_acc: 0.7883\n",
      "Epoch 25/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.5431 - acc: 0.8022 - val_loss: 0.5697 - val_acc: 0.7890\n",
      "Epoch 26/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.5365 - acc: 0.8047 - val_loss: 0.5675 - val_acc: 0.7950\n",
      "Epoch 27/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.5300 - acc: 0.8075 - val_loss: 0.5605 - val_acc: 0.7950\n",
      "Epoch 28/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.5243 - acc: 0.8107 - val_loss: 0.5575 - val_acc: 0.7953\n",
      "Epoch 29/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.5188 - acc: 0.8112 - val_loss: 0.5539 - val_acc: 0.7960\n",
      "Epoch 30/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.5134 - acc: 0.8131 - val_loss: 0.5503 - val_acc: 0.7997\n",
      "Epoch 31/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.5085 - acc: 0.8151 - val_loss: 0.5480 - val_acc: 0.8003\n",
      "Epoch 32/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.5034 - acc: 0.8166 - val_loss: 0.5475 - val_acc: 0.8020\n",
      "Epoch 33/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.4988 - acc: 0.8193 - val_loss: 0.5436 - val_acc: 0.8017\n",
      "Epoch 34/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.4940 - acc: 0.8213 - val_loss: 0.5404 - val_acc: 0.8023\n",
      "Epoch 35/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.4894 - acc: 0.8225 - val_loss: 0.5407 - val_acc: 0.8063\n",
      "Epoch 36/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4856 - acc: 0.8232 - val_loss: 0.5360 - val_acc: 0.8073\n",
      "Epoch 37/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.4814 - acc: 0.8256 - val_loss: 0.5357 - val_acc: 0.8063\n",
      "Epoch 38/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.4774 - acc: 0.8265 - val_loss: 0.5338 - val_acc: 0.8087\n",
      "Epoch 39/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.4735 - acc: 0.8275 - val_loss: 0.5308 - val_acc: 0.8090\n",
      "Epoch 40/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4696 - acc: 0.8302 - val_loss: 0.5283 - val_acc: 0.8103\n",
      "Epoch 41/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4660 - acc: 0.8316 - val_loss: 0.5286 - val_acc: 0.8103\n",
      "Epoch 42/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.4626 - acc: 0.8317 - val_loss: 0.5256 - val_acc: 0.8110\n",
      "Epoch 43/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.4591 - acc: 0.8344 - val_loss: 0.5252 - val_acc: 0.8133\n",
      "Epoch 44/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.4558 - acc: 0.8349 - val_loss: 0.5228 - val_acc: 0.8113\n",
      "Epoch 45/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.4524 - acc: 0.8377 - val_loss: 0.5221 - val_acc: 0.8107\n",
      "Epoch 46/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.4493 - acc: 0.8379 - val_loss: 0.5205 - val_acc: 0.8107\n",
      "Epoch 47/120\n",
      "33000/33000 [==============================] - 1s 27us/step - loss: 0.4463 - acc: 0.8388 - val_loss: 0.5222 - val_acc: 0.8133\n",
      "Epoch 48/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4431 - acc: 0.8407 - val_loss: 0.5207 - val_acc: 0.8130\n",
      "Epoch 49/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4400 - acc: 0.8418 - val_loss: 0.5182 - val_acc: 0.8133\n",
      "Epoch 50/120\n",
      "33000/33000 [==============================] - 1s 27us/step - loss: 0.4372 - acc: 0.8433 - val_loss: 0.5177 - val_acc: 0.8093\n",
      "Epoch 51/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.4344 - acc: 0.8437 - val_loss: 0.5173 - val_acc: 0.8140\n",
      "Epoch 52/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.4317 - acc: 0.8449 - val_loss: 0.5156 - val_acc: 0.8143\n",
      "Epoch 53/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.4289 - acc: 0.8461 - val_loss: 0.5176 - val_acc: 0.8193\n",
      "Epoch 54/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4261 - acc: 0.8466 - val_loss: 0.5147 - val_acc: 0.8187\n",
      "Epoch 55/120\n",
      "33000/33000 [==============================] - 1s 27us/step - loss: 0.4238 - acc: 0.8475 - val_loss: 0.5146 - val_acc: 0.8200\n",
      "Epoch 56/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.4207 - acc: 0.8490 - val_loss: 0.5154 - val_acc: 0.8173\n",
      "Epoch 57/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.4185 - acc: 0.8499 - val_loss: 0.5179 - val_acc: 0.8150\n",
      "Epoch 58/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.4162 - acc: 0.8508 - val_loss: 0.5149 - val_acc: 0.8157\n",
      "Epoch 59/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.4134 - acc: 0.8525 - val_loss: 0.5164 - val_acc: 0.8167\n",
      "Epoch 60/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.4110 - acc: 0.8533 - val_loss: 0.5140 - val_acc: 0.8187\n",
      "Epoch 61/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.4087 - acc: 0.8537 - val_loss: 0.5134 - val_acc: 0.8143\n",
      "Epoch 62/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.4064 - acc: 0.8551 - val_loss: 0.5144 - val_acc: 0.8150\n",
      "Epoch 63/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.4044 - acc: 0.8552 - val_loss: 0.5136 - val_acc: 0.8167\n",
      "Epoch 64/120\n",
      "33000/33000 [==============================] - 1s 44us/step - loss: 0.4018 - acc: 0.8561 - val_loss: 0.5130 - val_acc: 0.8183\n",
      "Epoch 65/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.3997 - acc: 0.8572 - val_loss: 0.5114 - val_acc: 0.8163\n",
      "Epoch 66/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3973 - acc: 0.8585 - val_loss: 0.5141 - val_acc: 0.8203\n",
      "Epoch 67/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3956 - acc: 0.8591 - val_loss: 0.5118 - val_acc: 0.8167\n",
      "Epoch 68/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.3933 - acc: 0.8594 - val_loss: 0.5117 - val_acc: 0.8153\n",
      "Epoch 69/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3909 - acc: 0.8599 - val_loss: 0.5112 - val_acc: 0.8160\n",
      "Epoch 70/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3890 - acc: 0.8620 - val_loss: 0.5111 - val_acc: 0.8153\n",
      "Epoch 71/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.3874 - acc: 0.8619 - val_loss: 0.5126 - val_acc: 0.8183\n",
      "Epoch 72/120\n",
      "33000/33000 [==============================] - 2s 53us/step - loss: 0.3853 - acc: 0.8625 - val_loss: 0.5123 - val_acc: 0.8200\n",
      "Epoch 73/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.3834 - acc: 0.8631 - val_loss: 0.5111 - val_acc: 0.8190\n",
      "Epoch 74/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3816 - acc: 0.8642 - val_loss: 0.5128 - val_acc: 0.8157\n",
      "Epoch 75/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3794 - acc: 0.8641 - val_loss: 0.5130 - val_acc: 0.8183\n",
      "Epoch 76/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3776 - acc: 0.8659 - val_loss: 0.5144 - val_acc: 0.8153\n",
      "Epoch 77/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3757 - acc: 0.8650 - val_loss: 0.5133 - val_acc: 0.8157\n",
      "Epoch 78/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.3742 - acc: 0.8669 - val_loss: 0.5129 - val_acc: 0.8150\n",
      "Epoch 79/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3721 - acc: 0.8688 - val_loss: 0.5143 - val_acc: 0.8183\n",
      "Epoch 80/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.3704 - acc: 0.8688 - val_loss: 0.5135 - val_acc: 0.8167\n",
      "Epoch 81/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.3691 - acc: 0.8690 - val_loss: 0.5164 - val_acc: 0.8203\n",
      "Epoch 82/120\n",
      "33000/33000 [==============================] - 2s 53us/step - loss: 0.3673 - acc: 0.8702 - val_loss: 0.5171 - val_acc: 0.8197\n",
      "Epoch 83/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.3650 - acc: 0.8717 - val_loss: 0.5162 - val_acc: 0.8170\n",
      "Epoch 84/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.3637 - acc: 0.8705 - val_loss: 0.5176 - val_acc: 0.8200\n",
      "Epoch 85/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.3621 - acc: 0.8722 - val_loss: 0.5158 - val_acc: 0.8160\n",
      "Epoch 86/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.3604 - acc: 0.8733 - val_loss: 0.5157 - val_acc: 0.8183\n",
      "Epoch 87/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3588 - acc: 0.8733 - val_loss: 0.5184 - val_acc: 0.8167\n",
      "Epoch 88/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3570 - acc: 0.8735 - val_loss: 0.5166 - val_acc: 0.8143\n",
      "Epoch 89/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.3555 - acc: 0.8742 - val_loss: 0.5197 - val_acc: 0.8183\n",
      "Epoch 90/120\n",
      "33000/33000 [==============================] - 1s 27us/step - loss: 0.3538 - acc: 0.8761 - val_loss: 0.5176 - val_acc: 0.8183\n",
      "Epoch 91/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3522 - acc: 0.8754 - val_loss: 0.5167 - val_acc: 0.8177\n",
      "Epoch 92/120\n",
      "33000/33000 [==============================] - 1s 27us/step - loss: 0.3507 - acc: 0.8768 - val_loss: 0.5216 - val_acc: 0.8170\n",
      "Epoch 93/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.3491 - acc: 0.8781 - val_loss: 0.5169 - val_acc: 0.8167\n",
      "Epoch 94/120\n",
      "33000/33000 [==============================] - 1s 26us/step - loss: 0.3480 - acc: 0.8779 - val_loss: 0.5223 - val_acc: 0.8190\n",
      "Epoch 95/120\n",
      "33000/33000 [==============================] - 1s 27us/step - loss: 0.3465 - acc: 0.8781 - val_loss: 0.5212 - val_acc: 0.8147\n",
      "Epoch 96/120\n",
      "33000/33000 [==============================] - 1s 26us/step - loss: 0.3444 - acc: 0.8785 - val_loss: 0.5239 - val_acc: 0.8187\n",
      "Epoch 97/120\n",
      "33000/33000 [==============================] - 1s 27us/step - loss: 0.3432 - acc: 0.8807 - val_loss: 0.5240 - val_acc: 0.8173\n",
      "Epoch 98/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.3421 - acc: 0.8799 - val_loss: 0.5221 - val_acc: 0.8177\n",
      "Epoch 99/120\n",
      "33000/33000 [==============================] - 1s 27us/step - loss: 0.3402 - acc: 0.8813 - val_loss: 0.5208 - val_acc: 0.8180\n",
      "Epoch 100/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.3390 - acc: 0.8813 - val_loss: 0.5259 - val_acc: 0.8183\n",
      "Epoch 101/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.3377 - acc: 0.8810 - val_loss: 0.5230 - val_acc: 0.8173\n",
      "Epoch 102/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3363 - acc: 0.8831 - val_loss: 0.5224 - val_acc: 0.8183\n",
      "Epoch 103/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3349 - acc: 0.8833 - val_loss: 0.5258 - val_acc: 0.8157\n",
      "Epoch 104/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.3333 - acc: 0.8839 - val_loss: 0.5267 - val_acc: 0.8163\n",
      "Epoch 105/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.3319 - acc: 0.8836 - val_loss: 0.5269 - val_acc: 0.8157\n",
      "Epoch 106/120\n",
      "33000/33000 [==============================] - 1s 27us/step - loss: 0.3306 - acc: 0.8840 - val_loss: 0.5308 - val_acc: 0.8153\n",
      "Epoch 107/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.3296 - acc: 0.8852 - val_loss: 0.5280 - val_acc: 0.8170\n",
      "Epoch 108/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3280 - acc: 0.8848 - val_loss: 0.5283 - val_acc: 0.8163\n",
      "Epoch 109/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3266 - acc: 0.8849 - val_loss: 0.5323 - val_acc: 0.8147\n",
      "Epoch 110/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.3255 - acc: 0.8862 - val_loss: 0.5284 - val_acc: 0.8147\n",
      "Epoch 111/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3241 - acc: 0.8870 - val_loss: 0.5297 - val_acc: 0.8143\n",
      "Epoch 112/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3229 - acc: 0.8876 - val_loss: 0.5313 - val_acc: 0.8150\n",
      "Epoch 113/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3216 - acc: 0.8870 - val_loss: 0.5342 - val_acc: 0.8173\n",
      "Epoch 114/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3201 - acc: 0.8886 - val_loss: 0.5361 - val_acc: 0.8170\n",
      "Epoch 115/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3191 - acc: 0.8895 - val_loss: 0.5333 - val_acc: 0.8143\n",
      "Epoch 116/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3175 - acc: 0.8893 - val_loss: 0.5342 - val_acc: 0.8163\n",
      "Epoch 117/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3164 - acc: 0.8897 - val_loss: 0.5332 - val_acc: 0.8173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3152 - acc: 0.8904 - val_loss: 0.5346 - val_acc: 0.8160\n",
      "Epoch 119/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3142 - acc: 0.8905 - val_loss: 0.5378 - val_acc: 0.8177\n",
      "Epoch 120/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.3127 - acc: 0.8910 - val_loss: 0.5359 - val_acc: 0.8170\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 2s 47us/step\n",
      "4000/4000 [==============================] - 0s 46us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3084263940290971, 0.8941515151515151]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5681627358198166, 0.80525]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, you were able to get a fairly similar validation accuracy of 89.67 (compared to 88.55 in obtained in the first model in this lab). your test set accuracy went up from 75.8 to a staggering 80.225% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lesson, you not only built an initial deep-learning model, you then used a validation set to tune your model using various types of regularization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
